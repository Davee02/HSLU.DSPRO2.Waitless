{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85188db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e2a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_timestamp(df):\n",
    "    \"\"\"Extract temporal components from timestamp and mark closing hours\"\"\"\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['weekday'] = df['datetime'].dt.weekday  # Monday=0, Sunday=6\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['minute'] = df['datetime'].dt.minute\n",
    "\n",
    "    # Create operating hours mask (True if within operating hours, False otherwise)\n",
    "    operating_hours_mask = (df['hour'] >= 9) & (df['hour'] < 21)\n",
    "    \n",
    "    # Handle the 'closed' column with explicit logic to avoid bitwise operations\n",
    "    if 'closed' in df.columns:\n",
    "        # First, ensure closed column is numeric\n",
    "        if df['closed'].dtype == object:\n",
    "            df['closed'] = df['closed'].map({'True': 1, 'False': 0, True: 1, False: 0}).fillna(0)\n",
    "        elif df['closed'].dtype == bool:\n",
    "            df['closed'] = df['closed'].astype(int)\n",
    "        \n",
    "        # Now for each row outside operating hours, set closed to 1\n",
    "        # This preserves existing closed=1 values during operating hours\n",
    "        df.loc[~operating_hours_mask, 'closed'] = 1\n",
    "    else:\n",
    "        # If closed column doesn't exist, create it\n",
    "        df['closed'] = 0\n",
    "        df.loc[~operating_hours_mask, 'closed'] = 1\n",
    "\n",
    "    df['is_weekend'] = df['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df['part_of_day'] = df['hour'].apply(lambda x: \n",
    "                                        'morning' if 6 <= x < 12 else\n",
    "                                        'afternoon' if 12 <= x < 17 else\n",
    "                                        'evening' if 17 <= x < 20 else\n",
    "                                        'night')\n",
    "    \n",
    "    df['season'] = df['month'].apply(lambda x:\n",
    "                                    'winter' if x in [12, 1, 2] else\n",
    "                                    'spring' if x in [3, 4, 5] else\n",
    "                                    'summer' if x in [6, 7, 8] else\n",
    "                                    'fall')\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "    df['weekday_sin'] = np.sin(2 * np.pi * df['weekday']/7)\n",
    "    df['weekday_cos'] = np.cos(2 * np.pi * df['weekday']/7)\n",
    "    df['minute_sin'] = np.sin(2 * np.pi * df['minute']/60)\n",
    "    df['minute_cos'] = np.cos(2 * np.pi * df['minute']/60)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_boolean_features(df):\n",
    "    \"\"\"Convert boolean features to integers\"\"\"\n",
    "    bool_cols = ['closed', 'is_german_holiday', 'is_swiss_holiday', 'is_french_holiday']\n",
    "    \n",
    "    for col in bool_cols:\n",
    "        if col in df.columns:\n",
    "            if df[col].dtype == bool:\n",
    "                df[col] = df[col].astype(int)\n",
    "            elif df[col].dtype == object:\n",
    "                df[col] = df[col].map({'True': 1, 'False': 0})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ca3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_theme_park_data_memory_efficient(df, output_file='processed_data.parquet', batch_size=100000, temp_dir='temp_efficient'):\n",
    "    \"\"\"\n",
    "    Memory-efficient implementation that processes the entire dataset for scaling/encoding\n",
    "    but operates in batches to maintain memory efficiency.\n",
    "    Now scales numerical features per ride_name separately.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "    from collections import defaultdict\n",
    "\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "    \n",
    "    for f in os.listdir(temp_dir):\n",
    "        if f.endswith('.parquet'):\n",
    "            os.remove(os.path.join(temp_dir, f))\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    print(f\"Total rows to process: {total_rows}\")\n",
    "    \n",
    "    print(\"Phase 1: Calculating statistics for encoding and scaling...\")\n",
    "    \n",
    "    cat_cols = ['ride_name', 'part_of_day', 'season', 'year']\n",
    "    num_cols = ['temperature', 'rain', 'wind']\n",
    "    num_cols = [col for col in num_cols if col in df.columns]\n",
    "    \n",
    "    ride_stats = defaultdict(lambda: {\n",
    "        'count': 0,\n",
    "        'mean': np.zeros(len(num_cols)),\n",
    "        'var': np.zeros(len(num_cols))\n",
    "    })\n",
    "    \n",
    "    cat_values = {col: set() for col in cat_cols}\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch = df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        batch = decompose_timestamp(batch)\n",
    "        batch = process_boolean_features(batch)\n",
    "        \n",
    "        for col in cat_cols:\n",
    "            unique_vals = batch[col].dropna().astype(str).unique()\n",
    "            cat_values[col].update(unique_vals)\n",
    "        \n",
    "        if num_cols:\n",
    "            for ride in batch['ride_name'].unique():\n",
    "                ride_batch = batch[batch['ride_name'] == ride]\n",
    "                \n",
    "                if len(ride_batch) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                ride_batch_count = len(ride_batch)\n",
    "                ride_batch_mean = ride_batch[num_cols].mean().values\n",
    "                ride_batch_var = ride_batch[num_cols].var().values\n",
    "                \n",
    "                # Update running statistics using Welford's algorithm\n",
    "                if ride_stats[ride]['count'] == 0:\n",
    "                    ride_stats[ride]['mean'] = ride_batch_mean\n",
    "                    ride_stats[ride]['var'] = ride_batch_var\n",
    "                    ride_stats[ride]['count'] = ride_batch_count\n",
    "                else:\n",
    "                    delta = ride_batch_mean - ride_stats[ride]['mean']\n",
    "                    mean_new = ride_stats[ride]['mean'] + delta * (ride_batch_count / (ride_stats[ride]['count'] + ride_batch_count))\n",
    "                    delta2 = ride_batch_mean - mean_new\n",
    "                    ride_stats[ride]['var'] = (ride_stats[ride]['var'] * ride_stats[ride]['count'] + \n",
    "                                             ride_batch_var * ride_batch_count + \n",
    "                                             delta * delta2 * ride_stats[ride]['count'] * ride_batch_count / \n",
    "                                             (ride_stats[ride]['count'] + ride_batch_count)) / (ride_stats[ride]['count'] + ride_batch_count)\n",
    "                    ride_stats[ride]['mean'] = mean_new\n",
    "                    ride_stats[ride]['count'] += ride_batch_count\n",
    "        \n",
    "        progress = (end_idx / total_rows) * 100\n",
    "        print(f\"Statistics collection progress: {progress:.2f}%\")\n",
    "        \n",
    "        # Release memory\n",
    "        del batch\n",
    "    \n",
    "    # Create scalers for each ride\n",
    "    ride_scalers = {}\n",
    "    for ride, stats in ride_stats.items():\n",
    "        scaler = StandardScaler()\n",
    "        scaler.mean_ = stats['mean']\n",
    "        scaler.scale_ = np.sqrt(stats['var'])\n",
    "        scaler.var_ = stats['var']\n",
    "        scaler.n_features_in_ = len(num_cols)\n",
    "        scaler.n_samples_seen_ = stats['count']\n",
    "        scaler.feature_names_in_ = np.array(num_cols)\n",
    "        ride_scalers[ride] = scaler\n",
    "    \n",
    "\n",
    "    categories = []\n",
    "    cat_indices = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        sorted_cats = sorted(list(cat_values[col]))\n",
    "        categories.append(np.array(sorted_cats))\n",
    "        n_cats = len(sorted_cats)\n",
    "        cat_indices.append((start_idx, start_idx + n_cats))\n",
    "        start_idx += n_cats\n",
    "    \n",
    "    encoder = OneHotEncoder(\n",
    "        sparse_output=False,\n",
    "        handle_unknown='ignore',\n",
    "        categories=categories\n",
    "    )\n",
    "    \n",
    "    dummy_data = pd.DataFrame([[categories[i][0] for i in range(len(cat_cols))]], columns=cat_cols)\n",
    "    encoder.fit(dummy_data)\n",
    "    \n",
    "    print(\"Statistics calculated. Starting data transformation...\")\n",
    "    \n",
    "    batch_files = []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch = df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        batch_num = (start_idx // batch_size) + 1\n",
    "        print(f\"Processing batch {batch_num}: rows {start_idx} to {end_idx}\")\n",
    "        \n",
    "        batch = decompose_timestamp(batch)\n",
    "        \n",
    "        batch = batch.drop(columns=['month', 'day', 'hour', 'minute'], errors='ignore')\n",
    "        batch = process_boolean_features(batch)\n",
    "        \n",
    "        try:\n",
    "            for col in cat_cols:\n",
    "                batch[col] = batch[col].astype(str)\n",
    "                \n",
    "            encoded_cats = encoder.transform(batch[cat_cols])\n",
    "            encoded_df = pd.DataFrame(\n",
    "                encoded_cats,\n",
    "                columns=encoder.get_feature_names_out(cat_cols),\n",
    "                index=batch.index\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during encoding: {e}\")\n",
    "            print(f\"Unique values: {[batch[col].unique()[:5] for col in cat_cols]}\")\n",
    "            raise\n",
    "\n",
    "        # Scale numerical features per ride_name\n",
    "        if num_cols:\n",
    "            for ride in batch['ride_name'].unique():\n",
    "                ride_mask = batch['ride_name'] == ride\n",
    "                if ride_mask.any() and ride in ride_scalers:\n",
    "                    # copy the numerical column to suffic \"_unscaled\" for reference\n",
    "                    for col in num_cols:\n",
    "                        batch.loc[ride_mask, col + '_unscaled'] = batch.loc[ride_mask, col]\n",
    "\n",
    "                    # scale the numerical columns\n",
    "                    batch.loc[ride_mask, num_cols] = ride_scalers[ride].transform(batch.loc[ride_mask, num_cols])\n",
    "\n",
    "        batch = pd.concat([batch.drop(cat_cols, axis=1), encoded_df], axis=1)\n",
    "\n",
    "        cols_to_drop = ['datetime']\n",
    "        batch = batch.drop(columns=[col for col in cols_to_drop if col in batch.columns])\n",
    "        \n",
    "        temp_file = os.path.join(temp_dir, f\"batch_{batch_num}.parquet\")\n",
    "        batch.to_parquet(temp_file, index=False)\n",
    "        batch_files.append(temp_file)\n",
    "        \n",
    "        del batch\n",
    "        del encoded_df\n",
    "        \n",
    "        progress = (end_idx / total_rows) * 100\n",
    "        print(f\"Transformation progress: {progress:.2f}%\")\n",
    "    \n",
    "    print(f\"All batches processed. Creating final output file...\")\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "    \n",
    "    concat_batch_size = 5 \n",
    "    for i in range(0, len(batch_files), concat_batch_size):\n",
    "        batch_group = batch_files[i:i+concat_batch_size]\n",
    "        print(f\"Combining batch files {i+1} to {min(i+concat_batch_size, len(batch_files))}\")\n",
    "        \n",
    "        group_dfs = [pd.read_parquet(file) for file in batch_group]\n",
    "        combined_df = pd.concat(group_dfs, ignore_index=True)\n",
    "        \n",
    "        mode = 'w' if i == 0 else 'a'\n",
    "        combined_df.to_parquet(output_file, index=False, engine='fastparquet', append=(mode=='a'))\n",
    "        \n",
    "        # Clean up\n",
    "        for df_obj in group_dfs:\n",
    "            del df_obj\n",
    "        del combined_df\n",
    "    \n",
    "    print(f\"All data combined and saved to {output_file}\")\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    for file in batch_files:\n",
    "        os.remove(file)\n",
    "    \n",
    "    print(\"Temporary files removed\")\n",
    "    \n",
    "    transformers = {'encoder': encoder, 'scalers': ride_scalers}\n",
    "    return transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eede7841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ride_name', 'timestamp', 'wait_time', 'closed', 'is_german_holiday',\n",
      "       'is_swiss_holiday', 'is_french_holiday', 'date', 'datetime',\n",
      "       'time_bucket', 'day_of_week', 'temperature', 'rain'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_input_dir = \"../data/processed\"\n",
    "input_file = os.path.join(data_input_dir, \"ep\", \"bucket_cleaned_wait_times_with_nonoperating.parquet\")\n",
    "ep_df = pd.read_parquet(input_file)\n",
    "print(ep_df.columns.unique())\n",
    "ep_df.drop(columns=['feature_attraction_type', 'feature_category', 'feature_max_height', 'feature_track_length', 'feature_max_speed', 'feature_g_force',\n",
    "       'feature_min_age', 'feature_min_height', 'feature_capacity_per_hour', 'date'], errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c0eea42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique hours: 24\n",
      "Unique hours: [np.int32(0), np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6), np.int32(7), np.int32(8), np.int32(9), np.int32(10), np.int32(11), np.int32(12), np.int32(13), np.int32(14), np.int32(15), np.int32(16), np.int32(17), np.int32(18), np.int32(19), np.int32(20), np.int32(21), np.int32(22), np.int32(23)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not pd.api.types.is_datetime64_any_dtype(ep_df['timestamp']):\n",
    "    ep_df['timestamp'] = pd.to_datetime(ep_df['timestamp'])\n",
    "\n",
    "# Extract hours and check unique values\n",
    "hours = ep_df['timestamp'].dt.hour\n",
    "unique_hours = hours.unique()\n",
    "\n",
    "print(f\"Number of unique hours: {len(unique_hours)}\")\n",
    "print(f\"Unique hours: {sorted(unique_hours)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0bec018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.int32(2017), np.int32(2018), np.int32(2019), np.int32(2020), np.int32(2021), np.int32(2022), np.int32(2023), np.int32(2024)}\n"
     ]
    }
   ],
   "source": [
    "years = ep_df[\"timestamp\"].unique().year\n",
    "print(set(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b551ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data')\n",
    "output_path = data_dir / 'processed' / 'ep' / 'final_cleaned_processed_wait_times.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "745535a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows to process: 7834739\n",
      "Phase 1: Calculating statistics for encoding and scaling...\n",
      "Statistics collection progress: 25.53%\n",
      "Statistics collection progress: 51.05%\n",
      "Statistics collection progress: 76.58%\n",
      "Statistics collection progress: 100.00%\n",
      "Statistics calculated. Starting data transformation...\n",
      "Processing batch 1: rows 0 to 2000000\n",
      "Transformation progress: 25.53%\n",
      "Processing batch 2: rows 2000000 to 4000000\n",
      "Transformation progress: 51.05%\n",
      "Processing batch 3: rows 4000000 to 6000000\n",
      "Transformation progress: 76.58%\n",
      "Processing batch 4: rows 6000000 to 7834739\n",
      "Transformation progress: 100.00%\n",
      "All batches processed. Creating final output file...\n",
      "Combining batch files 1 to 4\n",
      "All data combined and saved to ../data/processed/ep/final_cleaned_processed_wait_times.parquet\n",
      "Temporary files removed\n"
     ]
    }
   ],
   "source": [
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "transformers = preprocess_theme_park_data_memory_efficient(ep_df, output_path, batch_size=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de81c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 264701 rows for ride_name_alpine express enzian to ../data/processed/ep/rides/alpine_express_enzian.parquet\n",
      "Saved 254543 rows for ride_name_arena of football  be part of it to ../data/processed/ep/rides/arena_of_football__be_part_of_it.parquet\n",
      "Saved 267554 rows for ride_name_arthur to ../data/processed/ep/rides/arthur.parquet\n",
      "Saved 257348 rows for ride_name_atlantica supersplash to ../data/processed/ep/rides/atlantica_supersplash.parquet\n",
      "Saved 267263 rows for ride_name_atlantis adventure to ../data/processed/ep/rides/atlantis_adventure.parquet\n",
      "Saved 267395 rows for ride_name_baaa express to ../data/processed/ep/rides/baaa_express.parquet\n",
      "Saved 267104 rows for ride_name_blue fire megacoaster to ../data/processed/ep/rides/blue_fire_megacoaster.parquet\n",
      "Saved 252583 rows for ride_name_castello dei medici to ../data/processed/ep/rides/castello_dei_medici.parquet\n",
      "Saved 267395 rows for ride_name_dancing dingie to ../data/processed/ep/rides/dancing_dingie.parquet\n",
      "Saved 267096 rows for ride_name_euromir to ../data/processed/ep/rides/euromir.parquet\n",
      "Saved 239418 rows for ride_name_eurosat  cancan coaster to ../data/processed/ep/rides/eurosat__cancan_coaster.parquet\n",
      "Saved 267555 rows for ride_name_eurotower to ../data/processed/ep/rides/eurotower.parquet\n",
      "Saved 257186 rows for ride_name_fjordrafting to ../data/processed/ep/rides/fjordrafting.parquet\n",
      "Saved 239197 rows for ride_name_jim button  journey through morrowland to ../data/processed/ep/rides/jim_button__journey_through_morrowland.parquet\n",
      "Saved 266935 rows for ride_name_josefinas magical imperial journey to ../data/processed/ep/rides/josefinas_magical_imperial_journey.parquet\n",
      "Saved 267400 rows for ride_name_kolumbusjolle to ../data/processed/ep/rides/kolumbusjolle.parquet\n",
      "Saved 239418 rows for ride_name_madame freudenreich curiosits to ../data/processed/ep/rides/madame_freudenreich_curiosits.parquet\n",
      "Saved 267551 rows for ride_name_matterhornblitz to ../data/processed/ep/rides/matterhornblitz.parquet\n",
      "Saved 267245 rows for ride_name_old mac donalds tractor fun to ../data/processed/ep/rides/old_mac_donalds_tractor_fun.parquet\n",
      "Saved 267553 rows for ride_name_pegasus to ../data/processed/ep/rides/pegasus.parquet\n",
      "Saved 267089 rows for ride_name_poppy towers to ../data/processed/ep/rides/poppy_towers.parquet\n",
      "Saved 257620 rows for ride_name_poseidon to ../data/processed/ep/rides/poseidon.parquet\n",
      "Saved 266348 rows for ride_name_silver star to ../data/processed/ep/rides/silver_star.parquet\n",
      "Saved 267254 rows for ride_name_swiss bob run to ../data/processed/ep/rides/swiss_bob_run.parquet\n",
      "Saved 255219 rows for ride_name_tirol log flume to ../data/processed/ep/rides/tirol_log_flume.parquet\n",
      "Saved 235572 rows for ride_name_vienna wave swing  glckspilz to ../data/processed/ep/rides/vienna_wave_swing__glckspilz.parquet\n",
      "Saved 266332 rows for ride_name_vindjammer to ../data/processed/ep/rides/vindjammer.parquet\n",
      "Saved 235884 rows for ride_name_voletarium to ../data/processed/ep/rides/voletarium.parquet\n",
      "Saved 267705 rows for ride_name_volo da vinci to ../data/processed/ep/rides/volo_da_vinci.parquet\n",
      "Saved 35567 rows for ride_name_voltron nevera powered by rimac to ../data/processed/ep/rides/voltron_nevera_powered_by_rimac.parquet\n",
      "Saved 267709 rows for ride_name_whale adventures  northern lights to ../data/processed/ep/rides/whale_adventures__northern_lights.parquet\n"
     ]
    }
   ],
   "source": [
    "with_nonoperating = pd.read_parquet(output_path)\n",
    "\n",
    "output_dir = data_dir / 'processed' / 'ep' / 'rides'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "ride_cols = [col for col in with_nonoperating.columns if col.startswith(\"ride_name_\")]\n",
    "\n",
    "for col in ride_cols:\n",
    "    ride_df = with_nonoperating[with_nonoperating[col] == 1].copy()\n",
    "\n",
    "    ride_name = col.replace(\"ride_name_\", \"\").replace(\" \", \"_\").lower()\n",
    "    filename = f\"{ride_name}.parquet\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    ride_df.to_parquet(filepath, index=False)\n",
    "    print(f\"Saved {len(ride_df)} rows for {col} to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3733f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile(\"../data/processed/ep/rides/poseidon.parquet\")\n",
    "all_columns = parquet_file.schema.names\n",
    "\n",
    "columns_to_read = [col for col in all_columns \n",
    "                   if not (col.startswith(\"feature_attraction_type\") or col.startswith(\"feature_category\") or col.startswith(\"feature\"))]\n",
    "\n",
    "table = pq.read_table(\"../data/processed/ep/rides/poseidon.parquet\")\n",
    "ep_df_preview = table.slice(0, 1000000).to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a22e287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ep_df_preview.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdd727de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'wait_time', 'closed', 'is_german_holiday',\n",
       "       'is_swiss_holiday', 'is_french_holiday', 'time_bucket', 'day_of_week',\n",
       "       'temperature', 'rain', 'weekday', 'is_weekend', 'month_sin',\n",
       "       'month_cos', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',\n",
       "       'minute_sin', 'minute_cos', 'temperature_unscaled', 'rain_unscaled',\n",
       "       'ride_name_alpine express enzian',\n",
       "       'ride_name_arena of football  be part of it', 'ride_name_arthur',\n",
       "       'ride_name_atlantica supersplash', 'ride_name_atlantis adventure',\n",
       "       'ride_name_baaa express', 'ride_name_blue fire megacoaster',\n",
       "       'ride_name_castello dei medici', 'ride_name_dancing dingie',\n",
       "       'ride_name_euromir', 'ride_name_eurosat  cancan coaster',\n",
       "       'ride_name_eurotower', 'ride_name_fjordrafting',\n",
       "       'ride_name_jim button  journey through morrowland',\n",
       "       'ride_name_josefinas magical imperial journey',\n",
       "       'ride_name_kolumbusjolle', 'ride_name_madame freudenreich curiosits',\n",
       "       'ride_name_matterhornblitz', 'ride_name_old mac donalds tractor fun',\n",
       "       'ride_name_pegasus', 'ride_name_poppy towers', 'ride_name_poseidon',\n",
       "       'ride_name_silver star', 'ride_name_swiss bob run',\n",
       "       'ride_name_tirol log flume', 'ride_name_vienna wave swing  glckspilz',\n",
       "       'ride_name_vindjammer', 'ride_name_voletarium',\n",
       "       'ride_name_volo da vinci', 'ride_name_voltron nevera powered by rimac',\n",
       "       'ride_name_whale adventures  northern lights', 'part_of_day_afternoon',\n",
       "       'part_of_day_evening', 'part_of_day_morning', 'part_of_day_night',\n",
       "       'season_fall', 'season_spring', 'season_summer', 'season_winter',\n",
       "       'year_2017', 'year_2018', 'year_2019', 'year_2020', 'year_2021',\n",
       "       'year_2022', 'year_2023', 'year_2024'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_df_preview.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e641fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parquet_file = pq.ParquetFile(\"../data/processed/ep/rides/poseidon.parquet\")\n",
    "all_columns = parquet_file.schema.names\n",
    "\n",
    "columns_to_read = [col for col in all_columns \n",
    "                   if not (col.startswith(\"ride_name\") or col.startswith(\"season\")or col.startswith(\"part\"))]\n",
    "\n",
    "table = pq.read_table(\"../data/processed/ep/rides/poseidon.parquet\", columns=columns_to_read)\n",
    "ep_df_column_analyze = table.slice(0, 1000000).to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c0d8bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(-1.0), np.float64(-0.8660254037844386), np.float64(-0.8660254037844384), np.float64(-0.5000000000000004), np.float64(-0.4999999999999997), np.float64(-2.4492935982947064e-16), np.float64(1.2246467991473532e-16), np.float64(0.49999999999999994), np.float64(0.8660254037844387)]\n",
      "[np.float64(-1.0), np.float64(-0.8660254037844388), np.float64(-0.8660254037844387), np.float64(-0.5000000000000004), np.float64(-0.4999999999999998), np.float64(-1.8369701987210297e-16), np.float64(0.5000000000000001), np.float64(0.8660254037844384), np.float64(1.0)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(ep_df_column_analyze[\"month_sin\"].unique()))\n",
    "print(sorted(ep_df_column_analyze[\"month_cos\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8e6c014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'wait_time', 'closed', 'is_german_holiday',\n",
       "       'is_swiss_holiday', 'is_french_holiday', 'time_bucket', 'day_of_week',\n",
       "       'temperature', 'rain', 'weekday', 'is_weekend', 'month_sin',\n",
       "       'month_cos', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',\n",
       "       'minute_sin', 'minute_cos', 'temperature_unscaled', 'rain_unscaled',\n",
       "       'year_2017', 'year_2018', 'year_2019', 'year_2020', 'year_2021',\n",
       "       'year_2022', 'year_2023', 'year_2024'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_df_column_analyze.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faaf5a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ep_df_column_analyze[\"hour_cos\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb45cb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>wait_time</th>\n",
       "      <th>closed</th>\n",
       "      <th>is_german_holiday</th>\n",
       "      <th>is_swiss_holiday</th>\n",
       "      <th>is_french_holiday</th>\n",
       "      <th>time_bucket</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rain</th>\n",
       "      <th>...</th>\n",
       "      <th>temperature_unscaled</th>\n",
       "      <th>rain_unscaled</th>\n",
       "      <th>year_2017</th>\n",
       "      <th>year_2018</th>\n",
       "      <th>year_2019</th>\n",
       "      <th>year_2020</th>\n",
       "      <th>year_2021</th>\n",
       "      <th>year_2022</th>\n",
       "      <th>year_2023</th>\n",
       "      <th>year_2024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2017-05-23 09:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.596628</td>\n",
       "      <td>-0.207325</td>\n",
       "      <td>...</td>\n",
       "      <td>17.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2017-05-23 10:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 10:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.017454</td>\n",
       "      <td>-0.207325</td>\n",
       "      <td>...</td>\n",
       "      <td>19.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2017-05-23 11:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 11:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.387782</td>\n",
       "      <td>-0.207325</td>\n",
       "      <td>...</td>\n",
       "      <td>22.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2017-05-23 12:00:00</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 12:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.623445</td>\n",
       "      <td>-0.207325</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2017-05-23 13:00:00</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 13:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.825441</td>\n",
       "      <td>-0.207325</td>\n",
       "      <td>...</td>\n",
       "      <td>24.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257580</th>\n",
       "      <td>2024-12-30 13:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-30 13:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.113495</td>\n",
       "      <td>-0.207325</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257581</th>\n",
       "      <td>2024-12-30 14:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-30 14:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.046163</td>\n",
       "      <td>-0.207325</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257582</th>\n",
       "      <td>2024-12-30 15:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-30 15:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.231326</td>\n",
       "      <td>-0.207325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257583</th>\n",
       "      <td>2024-12-30 16:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-30 16:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.231326</td>\n",
       "      <td>-0.207325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257584</th>\n",
       "      <td>2024-12-30 17:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-30 17:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.281825</td>\n",
       "      <td>-0.207325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14468 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 timestamp  wait_time  closed  is_german_holiday  \\\n",
       "108    2017-05-23 09:00:00        0.0       0                  0   \n",
       "109    2017-05-23 10:00:00        0.0       0                  0   \n",
       "110    2017-05-23 11:00:00       10.0       0                  0   \n",
       "111    2017-05-23 12:00:00       25.0       0                  0   \n",
       "112    2017-05-23 13:00:00       25.0       0                  0   \n",
       "...                    ...        ...     ...                ...   \n",
       "257580 2024-12-30 13:00:00        0.0       0                  0   \n",
       "257581 2024-12-30 14:00:00        0.0       0                  0   \n",
       "257582 2024-12-30 15:00:00        0.0       0                  0   \n",
       "257583 2024-12-30 16:00:00        0.0       0                  0   \n",
       "257584 2024-12-30 17:00:00        0.0       0                  0   \n",
       "\n",
       "        is_swiss_holiday  is_french_holiday         time_bucket  day_of_week  \\\n",
       "108                    0                  0 2017-05-23 09:00:00            1   \n",
       "109                    0                  0 2017-05-23 10:00:00            1   \n",
       "110                    0                  0 2017-05-23 11:00:00            1   \n",
       "111                    0                  0 2017-05-23 12:00:00            1   \n",
       "112                    0                  0 2017-05-23 13:00:00            1   \n",
       "...                  ...                ...                 ...          ...   \n",
       "257580                 0                  0 2024-12-30 13:00:00            0   \n",
       "257581                 0                  0 2024-12-30 14:00:00            0   \n",
       "257582                 0                  0 2024-12-30 15:00:00            0   \n",
       "257583                 0                  0 2024-12-30 16:00:00            0   \n",
       "257584                 0                  0 2024-12-30 17:00:00            0   \n",
       "\n",
       "        temperature      rain  ...  temperature_unscaled  rain_unscaled  \\\n",
       "108        0.596628 -0.207325  ...                  17.4            0.0   \n",
       "109        1.017454 -0.207325  ...                  19.9            0.0   \n",
       "110        1.387782 -0.207325  ...                  22.1            0.0   \n",
       "111        1.623445 -0.207325  ...                  23.5            0.0   \n",
       "112        1.825441 -0.207325  ...                  24.7            0.0   \n",
       "...             ...       ...  ...                   ...            ...   \n",
       "257580    -2.113495 -0.207325  ...                   1.3            0.0   \n",
       "257581    -2.046163 -0.207325  ...                   1.7            0.0   \n",
       "257582    -2.231326 -0.207325  ...                   0.6            0.0   \n",
       "257583    -2.231326 -0.207325  ...                   0.6            0.0   \n",
       "257584    -2.281825 -0.207325  ...                   0.3            0.0   \n",
       "\n",
       "        year_2017  year_2018  year_2019  year_2020  year_2021  year_2022  \\\n",
       "108           1.0        0.0        0.0        0.0        0.0        0.0   \n",
       "109           1.0        0.0        0.0        0.0        0.0        0.0   \n",
       "110           1.0        0.0        0.0        0.0        0.0        0.0   \n",
       "111           1.0        0.0        0.0        0.0        0.0        0.0   \n",
       "112           1.0        0.0        0.0        0.0        0.0        0.0   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "257580        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "257581        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "257582        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "257583        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "257584        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "        year_2023  year_2024  \n",
       "108           0.0        0.0  \n",
       "109           0.0        0.0  \n",
       "110           0.0        0.0  \n",
       "111           0.0        0.0  \n",
       "112           0.0        0.0  \n",
       "...           ...        ...  \n",
       "257580        0.0        1.0  \n",
       "257581        0.0        1.0  \n",
       "257582        0.0        1.0  \n",
       "257583        0.0        1.0  \n",
       "257584        0.0        1.0  \n",
       "\n",
       "[14468 rows x 30 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_df_column_analyze[ep_df_column_analyze[\"closed\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40ddd7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data...\n",
      "\n",
      "==== Basic Information ====\n",
      "Data shape: (7834739, 69)\n",
      "\n",
      "==== Check 1: Missing Values ====\n",
      "✓ No missing values found\n",
      "\n",
      "==== Check 2: Feature Ranges ====\n",
      "✓ month_sin is within expected range [-1, 1]\n",
      "✓ month_cos is within expected range [-1, 1]\n",
      "✓ hour_sin is within expected range [-1, 1]\n",
      "✓ hour_cos is within expected range [-1, 1]\n",
      "✓ weekday_sin is within expected range [-1, 1]\n",
      "✓ weekday_cos is within expected range [-1, 1]\n",
      "✓ minute_sin is within expected range [-1, 1]\n",
      "✓ minute_cos is within expected range [-1, 1]\n",
      "✓ ride_name_alpine express enzian contains only 0 and 1 as expected\n",
      "✓ ride_name_arena of football  be part of it contains only 0 and 1 as expected\n",
      "✓ ride_name_arthur contains only 0 and 1 as expected\n",
      "✓ ride_name_atlantica supersplash contains only 0 and 1 as expected\n",
      "✓ ride_name_atlantis adventure contains only 0 and 1 as expected\n",
      "✓ ride_name_baaa express contains only 0 and 1 as expected\n",
      "✓ ride_name_blue fire megacoaster contains only 0 and 1 as expected\n",
      "✓ ride_name_castello dei medici contains only 0 and 1 as expected\n",
      "✓ ride_name_dancing dingie contains only 0 and 1 as expected\n",
      "✓ ride_name_euromir contains only 0 and 1 as expected\n",
      "✓ ride_name_eurosat  cancan coaster contains only 0 and 1 as expected\n",
      "✓ ride_name_eurotower contains only 0 and 1 as expected\n",
      "✓ ride_name_fjordrafting contains only 0 and 1 as expected\n",
      "✓ ride_name_jim button  journey through morrowland contains only 0 and 1 as expected\n",
      "✓ ride_name_josefinas magical imperial journey contains only 0 and 1 as expected\n",
      "✓ ride_name_kolumbusjolle contains only 0 and 1 as expected\n",
      "✓ ride_name_madame freudenreich curiosits contains only 0 and 1 as expected\n",
      "✓ ride_name_matterhornblitz contains only 0 and 1 as expected\n",
      "✓ ride_name_old mac donalds tractor fun contains only 0 and 1 as expected\n",
      "✓ ride_name_pegasus contains only 0 and 1 as expected\n",
      "✓ ride_name_poppy towers contains only 0 and 1 as expected\n",
      "✓ ride_name_poseidon contains only 0 and 1 as expected\n",
      "✓ ride_name_silver star contains only 0 and 1 as expected\n",
      "✓ ride_name_swiss bob run contains only 0 and 1 as expected\n",
      "✓ ride_name_tirol log flume contains only 0 and 1 as expected\n",
      "✓ ride_name_vienna wave swing  glckspilz contains only 0 and 1 as expected\n",
      "✓ ride_name_vindjammer contains only 0 and 1 as expected\n",
      "✓ ride_name_voletarium contains only 0 and 1 as expected\n",
      "✓ ride_name_volo da vinci contains only 0 and 1 as expected\n",
      "✓ ride_name_voltron nevera powered by rimac contains only 0 and 1 as expected\n",
      "✓ ride_name_whale adventures  northern lights contains only 0 and 1 as expected\n",
      "✓ part_of_day_afternoon contains only 0 and 1 as expected\n",
      "✓ part_of_day_evening contains only 0 and 1 as expected\n",
      "✓ part_of_day_morning contains only 0 and 1 as expected\n",
      "✓ part_of_day_night contains only 0 and 1 as expected\n",
      "✓ season_fall contains only 0 and 1 as expected\n",
      "✓ season_spring contains only 0 and 1 as expected\n",
      "✓ season_summer contains only 0 and 1 as expected\n",
      "✓ season_winter contains only 0 and 1 as expected\n",
      "✓ closed contains only 0 and 1 as expected\n",
      "✓ is_german_holiday contains only 0 and 1 as expected\n",
      "✓ is_swiss_holiday contains only 0 and 1 as expected\n",
      "✓ is_french_holiday contains only 0 and 1 as expected\n",
      "✓ is_weekend contains only 0 and 1 as expected\n",
      "\n",
      "==== Check 3: Consistency Checks ====\n",
      "✓ 'weekday' and 'is_weekend' are consistent\n",
      "✓ Each observation has exactly one ride assigned\n",
      "✓ Each observation has exactly one part_of_day assigned\n",
      "✓ Each observation has exactly one season assigned\n",
      "\n",
      "==== Check 4: Scaled Numerical Features ====\n",
      "✓ temperature appears properly scaled (mean ≈ 0, std ≈ 1)\n",
      "✓ rain appears properly scaled (mean ≈ 0, std ≈ 1)\n",
      "\n",
      "==== Check 5: Cyclical Feature Correlations ====\n",
      "Warning: Correlation between month_sin and month_cos is -0.3087, expected near 0\n",
      "Warning: Correlation between hour_sin and hour_cos is -0.4406, expected near 0\n",
      "✓ weekday_sin and weekday_cos have low correlation as expected\n",
      "✓ minute_sin and minute_cos have low correlation as expected\n",
      "\n",
      "==== Check 6: Wait Time Distribution ====\n",
      "Wait time min: 0.0, mean: 0.59, max: 90.0\n",
      "Warning: 79142 extreme wait time outliers found\n",
      "Outlier values: [np.float64(25.0), np.float64(30.0), np.float64(35.0), np.float64(40.0), np.float64(45.0), np.float64(50.0), np.float64(55.0), np.float64(60.0), np.float64(65.0), np.float64(70.0), np.float64(75.0), np.float64(80.0), np.float64(85.0), np.float64(90.0)]\n",
      "\n",
      "==== Check 7: Closed Rides and Wait Times ====\n",
      "✓ All closed rides have wait time 0 or NaN as expected\n",
      "\n",
      "==== Check 8: Inspection of Cyclical Features ====\n",
      "Cyclical encodings should form circular patterns when sin/cos components are plotted against each other\n",
      "✓ month cyclical encoding maintains unit circle property\n",
      "✓ hour cyclical encoding maintains unit circle property\n",
      "✓ weekday cyclical encoding maintains unit circle property\n",
      "\n",
      "==== Summary ====\n",
      "Sanity check complete. Review the warnings above if any.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>wait_time</th>\n",
       "      <th>closed</th>\n",
       "      <th>is_german_holiday</th>\n",
       "      <th>is_swiss_holiday</th>\n",
       "      <th>is_french_holiday</th>\n",
       "      <th>time_bucket</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rain</th>\n",
       "      <th>...</th>\n",
       "      <th>season_summer</th>\n",
       "      <th>season_winter</th>\n",
       "      <th>year_2017</th>\n",
       "      <th>year_2018</th>\n",
       "      <th>year_2019</th>\n",
       "      <th>year_2020</th>\n",
       "      <th>year_2021</th>\n",
       "      <th>year_2022</th>\n",
       "      <th>year_2023</th>\n",
       "      <th>year_2024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-23 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.113445</td>\n",
       "      <td>-0.208719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-23 00:05:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 00:05:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.104696</td>\n",
       "      <td>-0.208719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-23 00:10:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 00:10:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.096957</td>\n",
       "      <td>-0.208719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-23 00:15:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 00:15:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090111</td>\n",
       "      <td>-0.208719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-23 00:20:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 00:20:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084040</td>\n",
       "      <td>-0.208719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7834734</th>\n",
       "      <td>2024-12-31 22:40:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-31 22:40:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.426214</td>\n",
       "      <td>-0.209151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7834735</th>\n",
       "      <td>2024-12-31 22:45:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-31 22:45:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.430681</td>\n",
       "      <td>-0.209151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7834736</th>\n",
       "      <td>2024-12-31 22:50:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-31 22:50:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.434503</td>\n",
       "      <td>-0.209151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7834737</th>\n",
       "      <td>2024-12-31 22:55:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-31 22:55:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.437611</td>\n",
       "      <td>-0.209151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7834738</th>\n",
       "      <td>2024-12-31 23:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-31 23:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.439931</td>\n",
       "      <td>-0.209151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7834739 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  wait_time  closed  is_german_holiday  \\\n",
       "0       2017-05-23 00:00:00        0.0       1                  0   \n",
       "1       2017-05-23 00:05:00        0.0       1                  0   \n",
       "2       2017-05-23 00:10:00        0.0       1                  0   \n",
       "3       2017-05-23 00:15:00        0.0       1                  0   \n",
       "4       2017-05-23 00:20:00        0.0       1                  0   \n",
       "...                     ...        ...     ...                ...   \n",
       "7834734 2024-12-31 22:40:00        0.0       1                  0   \n",
       "7834735 2024-12-31 22:45:00        0.0       1                  0   \n",
       "7834736 2024-12-31 22:50:00        0.0       1                  0   \n",
       "7834737 2024-12-31 22:55:00        0.0       1                  0   \n",
       "7834738 2024-12-31 23:00:00        0.0       1                  0   \n",
       "\n",
       "         is_swiss_holiday  is_french_holiday         time_bucket  day_of_week  \\\n",
       "0                       0                  0 2017-05-23 00:00:00            1   \n",
       "1                       0                  0 2017-05-23 00:05:00            1   \n",
       "2                       0                  0 2017-05-23 00:10:00            1   \n",
       "3                       0                  0 2017-05-23 00:15:00            1   \n",
       "4                       0                  0 2017-05-23 00:20:00            1   \n",
       "...                   ...                ...                 ...          ...   \n",
       "7834734                 0                  0 2024-12-31 22:40:00            1   \n",
       "7834735                 0                  0 2024-12-31 22:45:00            1   \n",
       "7834736                 0                  0 2024-12-31 22:50:00            1   \n",
       "7834737                 0                  0 2024-12-31 22:55:00            1   \n",
       "7834738                 0                  0 2024-12-31 23:00:00            1   \n",
       "\n",
       "         temperature      rain  ...  season_summer  season_winter  year_2017  \\\n",
       "0           0.113445 -0.208719  ...            0.0            0.0        1.0   \n",
       "1           0.104696 -0.208719  ...            0.0            0.0        1.0   \n",
       "2           0.096957 -0.208719  ...            0.0            0.0        1.0   \n",
       "3           0.090111 -0.208719  ...            0.0            0.0        1.0   \n",
       "4           0.084040 -0.208719  ...            0.0            0.0        1.0   \n",
       "...              ...       ...  ...            ...            ...        ...   \n",
       "7834734    -2.426214 -0.209151  ...            0.0            1.0        0.0   \n",
       "7834735    -2.430681 -0.209151  ...            0.0            1.0        0.0   \n",
       "7834736    -2.434503 -0.209151  ...            0.0            1.0        0.0   \n",
       "7834737    -2.437611 -0.209151  ...            0.0            1.0        0.0   \n",
       "7834738    -2.439931 -0.209151  ...            0.0            1.0        0.0   \n",
       "\n",
       "         year_2018  year_2019  year_2020  year_2021  year_2022  year_2023  \\\n",
       "0              0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "1              0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "2              0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "3              0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "4              0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "7834734        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "7834735        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "7834736        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "7834737        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "7834738        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "         year_2024  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  \n",
       "...            ...  \n",
       "7834734        1.0  \n",
       "7834735        1.0  \n",
       "7834736        1.0  \n",
       "7834737        1.0  \n",
       "7834738        1.0  \n",
       "\n",
       "[7834739 rows x 69 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sanity_check(data_path):\n",
    "    \"\"\"\n",
    "    Perform sanity checks on the processed theme park data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path : str\n",
    "        Path to the processed parquet file\n",
    "    \"\"\"\n",
    "    print(\"Loading processed data...\")\n",
    "    df = pd.read_parquet(data_path)\n",
    "    \n",
    "    print(f\"\\n==== Basic Information ====\")\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    \n",
    "    # Check 1: Missing values\n",
    "    print(\"\\n==== Check 1: Missing Values ====\")\n",
    "    missing = df.isna().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"Warning: Missing values found!\")\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(\"✓ No missing values found\")\n",
    "    \n",
    "    # Check 2: Feature ranges\n",
    "    print(\"\\n==== Check 2: Feature Ranges ====\")\n",
    "    \n",
    "    # Cyclical features should be between -1 and 1\n",
    "    cyclical_cols = [col for col in df.columns if col.endswith('_sin') or col.endswith('_cos')]\n",
    "    for col in cyclical_cols:\n",
    "        min_val, max_val = df[col].min(), df[col].max()\n",
    "        if min_val < -1.1 or max_val > 1.1:  # allow small floating point errors\n",
    "            print(f\"Warning: {col} range is [{min_val:.2f}, {max_val:.2f}], expected [-1, 1]\")\n",
    "        else:\n",
    "            print(f\"✓ {col} is within expected range [-1, 1]\")\n",
    "    \n",
    "    # One-hot encoded features should be 0 or 1\n",
    "    one_hot_cols = [\n",
    "        col for col in df.columns if \n",
    "        col.startswith('ride_name_') or \n",
    "        col.startswith('part_of_day_') or\n",
    "        col.startswith('season_')\n",
    "    ]\n",
    "    \n",
    "    for col in one_hot_cols:\n",
    "        unique_vals = df[col].unique()\n",
    "        if not np.all(np.isin(unique_vals, [0, 1])):\n",
    "            print(f\"Warning: {col} contains values other than 0 and 1: {unique_vals}\")\n",
    "        else:\n",
    "            print(f\"✓ {col} contains only 0 and 1 as expected\")\n",
    "    \n",
    "    # Boolean features should be 0 or 1\n",
    "    bool_cols = ['closed', 'is_german_holiday', 'is_swiss_holiday', 'is_french_holiday', 'is_weekend']\n",
    "    bool_cols = [col for col in bool_cols if col in df.columns]\n",
    "    \n",
    "    for col in bool_cols:\n",
    "        unique_vals = df[col].unique()\n",
    "        if not np.all(np.isin(unique_vals, [0, 1])):\n",
    "            print(f\"Warning: {col} contains values other than 0 and 1: {unique_vals}\")\n",
    "        else:\n",
    "            print(f\"✓ {col} contains only 0 and 1 as expected\")\n",
    "    \n",
    "    # Check 3: Consistency checks\n",
    "    print(\"\\n==== Check 3: Consistency Checks ====\")\n",
    "    \n",
    "    # Weekday features should be consistent with is_weekend\n",
    "    if 'weekday' in df.columns and 'is_weekend' in df.columns:\n",
    "        weekend_mask = df['weekday'] >= 5\n",
    "        is_weekend_mask = df['is_weekend'] == 1\n",
    "        \n",
    "        if (weekend_mask != is_weekend_mask).sum() > 0:\n",
    "            print(f\"Warning: 'weekday' and 'is_weekend' are inconsistent in {(weekend_mask != is_weekend_mask).sum()} rows\")\n",
    "        else:\n",
    "            print(\"✓ 'weekday' and 'is_weekend' are consistent\")\n",
    "    \n",
    "    # Ride names should sum to 1 for each row (one ride per observation)\n",
    "    ride_cols = [col for col in df.columns if col.startswith('ride_name_')]\n",
    "    ride_sums = df[ride_cols].sum(axis=1)\n",
    "    \n",
    "    if not np.all(ride_sums == 1):\n",
    "        print(f\"Warning: Some rows have {(ride_sums != 1).sum()} ride assignments that don't sum to 1\")\n",
    "        print(f\"Min: {ride_sums.min()}, Max: {ride_sums.max()}\")\n",
    "    else:\n",
    "        print(\"✓ Each observation has exactly one ride assigned\")\n",
    "    \n",
    "    # Part of day should sum to 1 for each row\n",
    "    part_of_day_cols = [col for col in df.columns if col.startswith('part_of_day_')]\n",
    "    part_of_day_sums = df[part_of_day_cols].sum(axis=1)\n",
    "    \n",
    "    if not np.all(part_of_day_sums == 1):\n",
    "        print(f\"Warning: Some rows have part_of_day assignments that don't sum to 1\")\n",
    "        print(f\"Min: {part_of_day_sums.min()}, Max: {part_of_day_sums.max()}\")\n",
    "    else:\n",
    "        print(\"✓ Each observation has exactly one part_of_day assigned\")\n",
    "    \n",
    "    # Season should sum to 1 for each row\n",
    "    season_cols = [col for col in df.columns if col.startswith('season_')]\n",
    "    season_sums = df[season_cols].sum(axis=1)\n",
    "    \n",
    "    if not np.all(season_sums == 1):\n",
    "        print(f\"Warning: Some rows have season assignments that don't sum to 1\")\n",
    "        print(f\"Min: {season_sums.min()}, Max: {season_sums.max()}\")\n",
    "    else:\n",
    "        print(\"✓ Each observation has exactly one season assigned\")\n",
    "    \n",
    "    # Check 4: Scaled numerical features\n",
    "    print(\"\\n==== Check 4: Scaled Numerical Features ====\")\n",
    "    num_cols = ['temperature', 'rain', 'wind', 'year']\n",
    "    num_cols = [col for col in num_cols if col in df.columns]\n",
    "    \n",
    "    for col in num_cols:\n",
    "        mean, std = df[col].mean(), df[col].std()\n",
    "        if abs(mean) > 0.1 or abs(std - 1) > 0.1:\n",
    "            print(f\"Warning: {col} may not be properly scaled. Mean: {mean:.4f}, Std: {std:.4f}\")\n",
    "        else:\n",
    "            print(f\"✓ {col} appears properly scaled (mean ≈ 0, std ≈ 1)\")\n",
    "    \n",
    "    # Check 5: Correlations between cyclical features\n",
    "    print(\"\\n==== Check 5: Cyclical Feature Correlations ====\")\n",
    "    for base in ['month', 'hour', 'weekday', 'minute']:\n",
    "        sin_col = f'{base}_sin'\n",
    "        cos_col = f'{base}_cos'\n",
    "        \n",
    "        if sin_col in df.columns and cos_col in df.columns:\n",
    "            corr = df[sin_col].corr(df[cos_col])\n",
    "            if abs(corr) > 0.1:\n",
    "                print(f\"Warning: Correlation between {sin_col} and {cos_col} is {corr:.4f}, expected near 0\")\n",
    "            else:\n",
    "                print(f\"✓ {sin_col} and {cos_col} have low correlation as expected\")\n",
    "    \n",
    "    # Check 6: Wait time distribution\n",
    "    if 'wait_time' in df.columns:\n",
    "        print(\"\\n==== Check 6: Wait Time Distribution ====\")\n",
    "        wait_time = df['wait_time']\n",
    "        print(f\"Wait time min: {wait_time.min()}, mean: {wait_time.mean():.2f}, max: {wait_time.max()}\")\n",
    "        \n",
    "        if wait_time.min() < 0:\n",
    "            print(f\"Warning: Negative wait times found: {wait_time[wait_time < 0].count()} values\")\n",
    "        \n",
    "        # Check for extreme outliers (> 5 std from mean)\n",
    "        mean, std = wait_time.mean(), wait_time.std()\n",
    "        outliers = wait_time[(wait_time > mean + 5*std) | (wait_time < mean - 5*std)]\n",
    "        if len(outliers) > 0:\n",
    "            print(f\"Warning: {len(outliers)} extreme wait time outliers found\")\n",
    "            print(f\"Outlier values: {sorted(outliers.unique())}\")\n",
    "        else:\n",
    "            print(\"✓ No extreme outliers in wait times\")\n",
    "    \n",
    "    # Check 7: Closed rides should have wait time 0 or NaN\n",
    "    if 'closed' in df.columns and 'wait_time' in df.columns:\n",
    "        print(\"\\n==== Check 7: Closed Rides and Wait Times ====\")\n",
    "        closed_rides = df[df['closed'] == 1]\n",
    "        if len(closed_rides) > 0:\n",
    "            invalid_waits = closed_rides[(closed_rides['wait_time'] > 0) & (~closed_rides['wait_time'].isna())]\n",
    "            if len(invalid_waits) > 0:\n",
    "                print(f\"Warning: {len(invalid_waits)} closed rides have wait times > 0\")\n",
    "                print(f\"Example: {invalid_waits[['wait_time']].head()}\")\n",
    "            else:\n",
    "                print(\"✓ All closed rides have wait time 0 or NaN as expected\")\n",
    "        else:\n",
    "            print(\"No closed rides in the dataset\")\n",
    "    \n",
    "    # Check 8: inspection of cyclical features\n",
    "    print(\"\\n==== Check 8: Inspection of Cyclical Features ====\")\n",
    "\n",
    "    cyclical_pairs = []\n",
    "    for base in ['month', 'hour', 'weekday']:\n",
    "        if f'{base}_sin' in df.columns and f'{base}_cos' in df.columns:\n",
    "            cyclical_pairs.append((base, f'{base}_sin', f'{base}_cos'))\n",
    "    \n",
    "    print(f\"Cyclical encodings should form circular patterns when sin/cos components are plotted against each other\")\n",
    "    for base, sin_col, cos_col in cyclical_pairs:\n",
    "        circle_check = np.sqrt(df[sin_col]**2 + df[cos_col]**2)\n",
    "        if (abs(circle_check - 1) > 0.1).any():\n",
    "            print(f\"Warning: {base} cyclical encoding doesn't maintain unit circle (sin²+cos²=1)\")\n",
    "            print(f\"Min: {circle_check.min():.4f}, Max: {circle_check.max():.4f}\")\n",
    "        else:\n",
    "            print(f\"✓ {base} cyclical encoding maintains unit circle property\")\n",
    "\n",
    "    print(\"\\n==== Summary ====\")\n",
    "    print(\"Sanity check complete. Review the warnings above if any.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "sanity_check(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af549b5",
   "metadata": {},
   "source": [
    "### Explanation for Warning\n",
    "- Warning: Correlation between month_sin and month_cos is -0.2893, expected near 0\n",
    "    - A perfect -1.0 correlation between sin and cos components typically happens when the values are concentrated at specific points (like only 0, 15, 30, 45 minutes). This is the case in the bucket variant\n",
    "- Warning: Correlation between hour_sin and hour_cos is -0.6495, expected near 0\n",
    "    - This happens because most data points are from 10 AM to 6 PM, this creates a correlation\n",
    "- Warning: Correlation between minute_sin and minute_cos is -1.0000, expected near 0\n",
    "    - Seasonality in the data - the park has have more data points from certain months. Also we dropped 3 months"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-real",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

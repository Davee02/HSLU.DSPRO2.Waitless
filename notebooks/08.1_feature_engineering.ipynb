{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85188db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e2a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_timestamp(df):\n",
    "    \"\"\"Extract temporal components from timestamp\"\"\"\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['weekday'] = df['datetime'].dt.weekday  # Monday=0, Sunday=6\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['minute'] = df['datetime'].dt.minute\n",
    "\n",
    "    df['is_weekend'] = df['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df['part_of_day'] = df['hour'].apply(lambda x: \n",
    "                                        'morning' if 6 <= x < 12 else\n",
    "                                        'afternoon' if 12 <= x < 17 else\n",
    "                                        'evening' if 17 <= x < 20 else\n",
    "                                        'night')\n",
    "    \n",
    "    df['season'] = df['month'].apply(lambda x:\n",
    "                                    'winter' if x in [12, 1, 2] else\n",
    "                                    'spring' if x in [3, 4, 5] else\n",
    "                                    'summer' if x in [6, 7, 8] else\n",
    "                                    'fall')\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "    df['weekday_sin'] = np.sin(2 * np.pi * df['weekday']/7)\n",
    "    df['weekday_cos'] = np.cos(2 * np.pi * df['weekday']/7)\n",
    "    df['minute_sin'] = np.sin(2 * np.pi * df['minute']/60)\n",
    "    df['minute_cos'] = np.cos(2 * np.pi * df['minute']/60)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_boolean_features(df):\n",
    "    \"\"\"Convert boolean features to integers\"\"\"\n",
    "    bool_cols = ['closed', 'is_german_holiday', 'is_swiss_holiday', 'is_french_holiday']\n",
    "    \n",
    "    for col in bool_cols:\n",
    "        if col in df.columns:\n",
    "            if df[col].dtype == bool:\n",
    "                df[col] = df[col].astype(int)\n",
    "            elif df[col].dtype == object:\n",
    "                df[col] = df[col].map({'True': 1, 'False': 0})\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_theme_park_data_memory_efficient(df, output_file='processed_data.parquet', batch_size=100000, temp_dir='temp_efficient'):\n",
    "    \"\"\"\n",
    "    Memory-efficient implementation that processes the entire dataset for scaling/encoding\n",
    "    but operates in batches to maintain memory efficiency.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "    \n",
    "\n",
    "    for f in os.listdir(temp_dir):\n",
    "        if f.endswith('.parquet'):\n",
    "            os.remove(os.path.join(temp_dir, f))\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    print(f\"Total rows to process: {total_rows}\")\n",
    "    \n",
    "    print(\"Phase 1: Calculating statistics for encoding and scaling...\")\n",
    "    \n",
    "    cat_cols = ['ride_name', 'part_of_day', 'season', 'year']\n",
    "    num_cols = ['temperature', 'rain', 'wind']\n",
    "    num_cols = [col for col in num_cols if col in df.columns]\n",
    "    \n",
    "    count_all = 0\n",
    "    mean_all = np.zeros(len(num_cols))\n",
    "    var_all = np.zeros(len(num_cols))\n",
    "    \n",
    "    cat_values = {col: set() for col in cat_cols}\n",
    "    \n",
    "    # First pass: decompose timestamps and collect statistics\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch = df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Process timestamps to get categorical features\n",
    "        batch = decompose_timestamp(batch)\n",
    "        batch = process_boolean_features(batch)\n",
    "        \n",
    "        # Collect unique values for categorical columns\n",
    "        for col in cat_cols:\n",
    "            unique_vals = batch[col].dropna().astype(str).unique()\n",
    "            cat_values[col].update(unique_vals)\n",
    "        \n",
    "        # Calculate statistics for numerical columns (for online StandardScaler)\n",
    "        if num_cols:\n",
    "            batch_count = len(batch)\n",
    "            batch_mean = batch[num_cols].mean().values\n",
    "            batch_var = batch[num_cols].var().values\n",
    "            \n",
    "            # Update running statistics using Welford's algorithm for stable variance calculation\n",
    "            if count_all == 0:\n",
    "                mean_all = batch_mean\n",
    "                var_all = batch_var\n",
    "                count_all = batch_count\n",
    "            else:\n",
    "                delta = batch_mean - mean_all\n",
    "                mean_all_new = mean_all + delta * (batch_count / (count_all + batch_count))\n",
    "                delta2 = batch_mean - mean_all_new\n",
    "                var_all = (var_all * count_all + batch_var * batch_count + \n",
    "                          delta * delta2 * count_all * batch_count / (count_all + batch_count)) / (count_all + batch_count)\n",
    "                mean_all = mean_all_new\n",
    "                count_all += batch_count\n",
    "        \n",
    "        progress = (end_idx / total_rows) * 100\n",
    "        print(f\"Statistics collection progress: {progress:.2f}%\")\n",
    "        \n",
    "        # Release memory\n",
    "        del batch\n",
    "    \n",
    "    # Create and fit scaler with calculated statistics\n",
    "    scaler = StandardScaler()\n",
    "    if num_cols:\n",
    "        scaler.mean_ = mean_all\n",
    "        scaler.scale_ = np.sqrt(var_all)\n",
    "        scaler.var_ = var_all\n",
    "        scaler.n_features_in_ = len(num_cols)\n",
    "        scaler.n_samples_seen_ = count_all\n",
    "        scaler.feature_names_in_ = np.array(num_cols)\n",
    "    \n",
    "    # Create encoder with predefined categories\n",
    "    categories = []\n",
    "    cat_indices = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        sorted_cats = sorted(list(cat_values[col]))\n",
    "        categories.append(np.array(sorted_cats))\n",
    "        n_cats = len(sorted_cats)\n",
    "        cat_indices.append((start_idx, start_idx + n_cats))\n",
    "        start_idx += n_cats\n",
    "    \n",
    "    encoder = OneHotEncoder(\n",
    "        sparse_output=False,\n",
    "        handle_unknown='ignore',\n",
    "        categories=categories\n",
    "    )\n",
    "    \n",
    "    dummy_data = pd.DataFrame([[categories[i][0] for i in range(len(cat_cols))]], columns=cat_cols)\n",
    "    encoder.fit(dummy_data)\n",
    "    \n",
    "    print(\"Statistics calculated. Starting data transformation...\")\n",
    "    \n",
    "\n",
    "    batch_files = []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch = df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        batch_num = (start_idx // batch_size) + 1\n",
    "        print(f\"Processing batch {batch_num}: rows {start_idx} to {end_idx}\")\n",
    "        \n",
    "        batch = decompose_timestamp(batch)\n",
    "        \n",
    "        batch = batch.drop(columns=['month', 'day', 'hour', 'minute'], errors='ignore')\n",
    "        batch = process_boolean_features(batch)\n",
    "        \n",
    "        try:\n",
    "            for col in cat_cols:\n",
    "                batch[col] = batch[col].astype(str)\n",
    "                \n",
    "            encoded_cats = encoder.transform(batch[cat_cols])\n",
    "            encoded_df = pd.DataFrame(\n",
    "                encoded_cats,\n",
    "                columns=encoder.get_feature_names_out(cat_cols),\n",
    "                index=batch.index\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during encoding: {e}\")\n",
    "            print(f\"Unique values: {[batch[col].unique()[:5] for col in cat_cols]}\")\n",
    "            raise\n",
    "\n",
    "        if num_cols:\n",
    "            batch[num_cols] = scaler.transform(batch[num_cols])\n",
    "\n",
    "        batch = pd.concat([batch.drop(cat_cols, axis=1), encoded_df], axis=1)\n",
    "\n",
    "        cols_to_drop = ['timestamp', 'datetime']\n",
    "        batch = batch.drop(columns=[col for col in cols_to_drop if col in batch.columns])\n",
    "        \n",
    "        temp_file = os.path.join(temp_dir, f\"batch_{batch_num}.parquet\")\n",
    "        batch.to_parquet(temp_file, index=False)\n",
    "        batch_files.append(temp_file)\n",
    "        \n",
    "        del batch\n",
    "        del encoded_df\n",
    "        \n",
    "        progress = (end_idx / total_rows) * 100\n",
    "        print(f\"Transformation progress: {progress:.2f}%\")\n",
    "    \n",
    "    print(f\"All batches processed. Creating final output file...\")\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "    \n",
    "    concat_batch_size = 5 \n",
    "    for i in range(0, len(batch_files), concat_batch_size):\n",
    "        batch_group = batch_files[i:i+concat_batch_size]\n",
    "        print(f\"Combining batch files {i+1} to {min(i+concat_batch_size, len(batch_files))}\")\n",
    "        \n",
    "        group_dfs = [pd.read_parquet(file) for file in batch_group]\n",
    "        combined_df = pd.concat(group_dfs, ignore_index=True)\n",
    "        \n",
    "        mode = 'w' if i == 0 else 'a'\n",
    "        combined_df.to_parquet(output_file, index=False, engine='fastparquet', append=(mode=='a'))\n",
    "        \n",
    "        # Clean up\n",
    "        for df_obj in group_dfs:\n",
    "            del df_obj\n",
    "        del combined_df\n",
    "    \n",
    "    print(f\"All data combined and saved to {output_file}\")\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    for file in batch_files:\n",
    "        os.remove(file)\n",
    "    \n",
    "    print(\"Temporary files removed\")\n",
    "    \n",
    "    transformers = {'encoder': encoder, 'scaler': scaler}\n",
    "    return transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eede7841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ride_name', 'timestamp', 'wait_time', 'closed', 'temperature', 'rain',\n",
      "       'wind', 'is_german_holiday', 'is_swiss_holiday', 'is_french_holiday',\n",
      "       'feature_attraction_type', 'feature_category', 'feature_max_height',\n",
      "       'feature_track_length', 'feature_max_speed', 'feature_g_force',\n",
      "       'feature_min_age', 'feature_min_height', 'feature_capacity_per_hour',\n",
      "       'date', 'datetime', 'time_bucket', 'day_of_week'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_input_dir = \"../data/processed\"\n",
    "input_file = os.path.join(data_input_dir, \"ep\", \"bucket_cleaned_wait_times.parquet\")\n",
    "ep_df = pd.read_parquet(input_file)\n",
    "print(ep_df.columns.unique())\n",
    "ep_df.drop(columns=['feature_attraction_type', 'feature_category', 'feature_max_height', 'feature_track_length', 'feature_max_speed', 'feature_g_force',\n",
    "       'feature_min_age', 'feature_min_height', 'feature_capacity_per_hour', 'date'], errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0bec018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.int32(2017), np.int32(2018), np.int32(2019), np.int32(2020), np.int32(2021), np.int32(2022), np.int32(2023), np.int32(2024)}\n"
     ]
    }
   ],
   "source": [
    "years = ep_df[\"timestamp\"].unique().year\n",
    "print(set(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b551ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows to process: 904296\n",
      "Phase 1: Calculating statistics for encoding and scaling...\n",
      "Statistics collection progress: 100.00%\n",
      "Statistics calculated. Starting data transformation...\n",
      "Processing batch 1: rows 0 to 904296\n",
      "Transformation progress: 100.00%\n",
      "All batches processed. Creating final output file...\n",
      "Combining batch files 1 to 1\n",
      "All data combined and saved to ../data/processed/ep/final_cleaned_processed_wait_times.parquet\n",
      "Temporary files removed\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path('../data')\n",
    "output_path = data_dir / 'processed' / 'ep' / 'final_cleaned_processed_wait_times.parquet'\n",
    "\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "transformers = preprocess_theme_park_data_memory_efficient(ep_df, output_path, batch_size=1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3733f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = pq.ParquetFile(output_path)\n",
    "all_columns = parquet_file.schema.names\n",
    "\n",
    "columns_to_read = [col for col in all_columns \n",
    "                   if not (col.startswith(\"feature_attraction_type\") or col.startswith(\"feature_category\") or col.startswith(\"feature\"))]\n",
    "\n",
    "table = pq.read_table(output_path)\n",
    "ep_df_preview = table.slice(0, 1000000).to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a22e287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ep_df_preview.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdd727de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['wait_time', 'closed', 'temperature', 'rain', 'wind',\n",
       "       'is_german_holiday', 'is_swiss_holiday', 'is_french_holiday',\n",
       "       'time_bucket', 'day_of_week', 'weekday', 'is_weekend', 'month_sin',\n",
       "       'month_cos', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',\n",
       "       'minute_sin', 'minute_cos', 'ride_name_alpine express enzian',\n",
       "       'ride_name_arena of football  be part of it', 'ride_name_arthur',\n",
       "       'ride_name_atlantica supersplash', 'ride_name_atlantis adventure',\n",
       "       'ride_name_baaa express', 'ride_name_blue fire megacoaster',\n",
       "       'ride_name_castello dei medici', 'ride_name_dancing dingie',\n",
       "       'ride_name_euromir', 'ride_name_eurosat  cancan coaster',\n",
       "       'ride_name_eurotower', 'ride_name_fjordrafting',\n",
       "       'ride_name_jim button  journey through morrowland',\n",
       "       'ride_name_josefinas magical imperial journey',\n",
       "       'ride_name_kolumbusjolle', 'ride_name_madame freudenreich curiosits',\n",
       "       'ride_name_matterhornblitz', 'ride_name_old mac donalds tractor fun',\n",
       "       'ride_name_pegasus', 'ride_name_poppy towers', 'ride_name_poseidon',\n",
       "       'ride_name_silver star', 'ride_name_swiss bob run',\n",
       "       'ride_name_tirol log flume', 'ride_name_vienna wave swing  glckspilz',\n",
       "       'ride_name_vindjammer', 'ride_name_voletarium',\n",
       "       'ride_name_volo da vinci', 'ride_name_voltron nevera powered by rimac',\n",
       "       'ride_name_whale adventures  northern lights', 'part_of_day_afternoon',\n",
       "       'part_of_day_evening', 'part_of_day_morning', 'part_of_day_night',\n",
       "       'season_fall', 'season_spring', 'season_summer', 'season_winter',\n",
       "       'year_2017', 'year_2018', 'year_2019', 'year_2020', 'year_2021',\n",
       "       'year_2022', 'year_2023', 'year_2024'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_df_preview.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = pq.ParquetFile(output_path)\n",
    "all_columns = parquet_file.schema.names\n",
    "\n",
    "columns_to_read = [col for col in all_columns \n",
    "                   if not (col.startswith(\"ride_name\") or col.startswith(\"season\")or col.startswith(\"part\"))]\n",
    "\n",
    "table = pq.read_table(output_path, columns=columns_to_read)\n",
    "ep_df_column_analyze = table.slice(0, 1000000).to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc820c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c0d8bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(-1.0), np.float64(-0.8660254037844386), np.float64(-0.8660254037844384), np.float64(-0.5000000000000004), np.float64(-0.4999999999999997), np.float64(-2.4492935982947064e-16), np.float64(1.2246467991473532e-16), np.float64(0.49999999999999994), np.float64(0.8660254037844387)]\n",
      "[np.float64(-1.0), np.float64(-0.8660254037844388), np.float64(-0.8660254037844387), np.float64(-0.5000000000000004), np.float64(-0.4999999999999998), np.float64(-1.8369701987210297e-16), np.float64(0.5000000000000001), np.float64(0.8660254037844384), np.float64(1.0)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(ep_df_column_analyze[\"month_sin\"].unique()))\n",
    "print(sorted(ep_df_column_analyze[\"month_cos\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8e6c014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['wait_time', 'closed', 'temperature', 'rain', 'wind',\n",
       "       'is_german_holiday', 'is_swiss_holiday', 'is_french_holiday',\n",
       "       'time_bucket', 'day_of_week', 'weekday', 'is_weekend', 'month_sin',\n",
       "       'month_cos', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',\n",
       "       'minute_sin', 'minute_cos', 'year_2017', 'year_2018', 'year_2019',\n",
       "       'year_2020', 'year_2021', 'year_2022', 'year_2023', 'year_2024'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_df_column_analyze.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40ddd7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Basic Information ====\n",
      "Data shape: (904296, 67)\n",
      "\n",
      "==== Check 1: Missing Values ====\n",
      "Warning: Missing values found!\n",
      "temperature      4823\n",
      "rain           146719\n",
      "wind            12948\n",
      "dtype: int64\n",
      "\n",
      "==== Check 2: Feature Ranges ====\n",
      "✓ month_sin is within expected range [-1, 1]\n",
      "✓ month_cos is within expected range [-1, 1]\n",
      "✓ hour_sin is within expected range [-1, 1]\n",
      "✓ hour_cos is within expected range [-1, 1]\n",
      "✓ weekday_sin is within expected range [-1, 1]\n",
      "✓ weekday_cos is within expected range [-1, 1]\n",
      "✓ minute_sin is within expected range [-1, 1]\n",
      "✓ minute_cos is within expected range [-1, 1]\n",
      "✓ ride_name_alpine express enzian contains only 0 and 1 as expected\n",
      "✓ ride_name_arena of football  be part of it contains only 0 and 1 as expected\n",
      "✓ ride_name_arthur contains only 0 and 1 as expected\n",
      "✓ ride_name_atlantica supersplash contains only 0 and 1 as expected\n",
      "✓ ride_name_atlantis adventure contains only 0 and 1 as expected\n",
      "✓ ride_name_baaa express contains only 0 and 1 as expected\n",
      "✓ ride_name_blue fire megacoaster contains only 0 and 1 as expected\n",
      "✓ ride_name_castello dei medici contains only 0 and 1 as expected\n",
      "✓ ride_name_dancing dingie contains only 0 and 1 as expected\n",
      "✓ ride_name_euromir contains only 0 and 1 as expected\n",
      "✓ ride_name_eurosat  cancan coaster contains only 0 and 1 as expected\n",
      "✓ ride_name_eurotower contains only 0 and 1 as expected\n",
      "✓ ride_name_fjordrafting contains only 0 and 1 as expected\n",
      "✓ ride_name_jim button  journey through morrowland contains only 0 and 1 as expected\n",
      "✓ ride_name_josefinas magical imperial journey contains only 0 and 1 as expected\n",
      "✓ ride_name_kolumbusjolle contains only 0 and 1 as expected\n",
      "✓ ride_name_madame freudenreich curiosits contains only 0 and 1 as expected\n",
      "✓ ride_name_matterhornblitz contains only 0 and 1 as expected\n",
      "✓ ride_name_old mac donalds tractor fun contains only 0 and 1 as expected\n",
      "✓ ride_name_pegasus contains only 0 and 1 as expected\n",
      "✓ ride_name_poppy towers contains only 0 and 1 as expected\n",
      "✓ ride_name_poseidon contains only 0 and 1 as expected\n",
      "✓ ride_name_silver star contains only 0 and 1 as expected\n",
      "✓ ride_name_swiss bob run contains only 0 and 1 as expected\n",
      "✓ ride_name_tirol log flume contains only 0 and 1 as expected\n",
      "✓ ride_name_vienna wave swing  glckspilz contains only 0 and 1 as expected\n",
      "✓ ride_name_vindjammer contains only 0 and 1 as expected\n",
      "✓ ride_name_voletarium contains only 0 and 1 as expected\n",
      "✓ ride_name_volo da vinci contains only 0 and 1 as expected\n",
      "✓ ride_name_voltron nevera powered by rimac contains only 0 and 1 as expected\n",
      "✓ ride_name_whale adventures  northern lights contains only 0 and 1 as expected\n",
      "✓ part_of_day_afternoon contains only 0 and 1 as expected\n",
      "✓ part_of_day_evening contains only 0 and 1 as expected\n",
      "✓ part_of_day_morning contains only 0 and 1 as expected\n",
      "✓ part_of_day_night contains only 0 and 1 as expected\n",
      "✓ season_fall contains only 0 and 1 as expected\n",
      "✓ season_spring contains only 0 and 1 as expected\n",
      "✓ season_summer contains only 0 and 1 as expected\n",
      "✓ season_winter contains only 0 and 1 as expected\n",
      "✓ closed contains only 0 and 1 as expected\n",
      "✓ is_german_holiday contains only 0 and 1 as expected\n",
      "✓ is_swiss_holiday contains only 0 and 1 as expected\n",
      "✓ is_french_holiday contains only 0 and 1 as expected\n",
      "✓ is_weekend contains only 0 and 1 as expected\n",
      "\n",
      "==== Check 3: Consistency Checks ====\n",
      "✓ 'weekday' and 'is_weekend' are consistent\n",
      "✓ Each observation has exactly one ride assigned\n",
      "✓ Each observation has exactly one part_of_day assigned\n",
      "✓ Each observation has exactly one season assigned\n",
      "\n",
      "==== Check 4: Scaled Numerical Features ====\n",
      "✓ temperature appears properly scaled (mean ≈ 0, std ≈ 1)\n",
      "✓ rain appears properly scaled (mean ≈ 0, std ≈ 1)\n",
      "✓ wind appears properly scaled (mean ≈ 0, std ≈ 1)\n",
      "\n",
      "==== Check 5: Cyclical Feature Correlations ====\n",
      "Warning: Correlation between month_sin and month_cos is -0.3301, expected near 0\n",
      "Warning: Correlation between hour_sin and hour_cos is -0.5761, expected near 0\n",
      "✓ weekday_sin and weekday_cos have low correlation as expected\n",
      "Warning: Correlation between minute_sin and minute_cos is -1.0000, expected near 0\n",
      "\n",
      "==== Check 6: Wait Time Distribution ====\n",
      "Wait time min: 0.0, mean: 10.79, max: 90.0\n",
      "Warning: 878 extreme wait time outliers found\n",
      "Outlier values: [np.float64(80.0), np.float64(85.0), np.float64(90.0)]\n",
      "\n",
      "==== Check 7: Closed Rides and Wait Times ====\n",
      "✓ All closed rides have wait time 0 or NaN as expected\n",
      "\n",
      "==== Check 8: Inspection of Cyclical Features ====\n",
      "Cyclical encodings should form circular patterns when sin/cos components are plotted against each other\n",
      "✓ month cyclical encoding maintains unit circle property\n",
      "✓ hour cyclical encoding maintains unit circle property\n",
      "✓ weekday cyclical encoding maintains unit circle property\n",
      "\n",
      "==== Summary ====\n",
      "Sanity check complete. Review the warnings above if any.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wait_time</th>\n",
       "      <th>closed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rain</th>\n",
       "      <th>wind</th>\n",
       "      <th>is_german_holiday</th>\n",
       "      <th>is_swiss_holiday</th>\n",
       "      <th>is_french_holiday</th>\n",
       "      <th>time_bucket</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>season_summer</th>\n",
       "      <th>season_winter</th>\n",
       "      <th>year_2017</th>\n",
       "      <th>year_2018</th>\n",
       "      <th>year_2019</th>\n",
       "      <th>year_2020</th>\n",
       "      <th>year_2021</th>\n",
       "      <th>year_2022</th>\n",
       "      <th>year_2023</th>\n",
       "      <th>year_2024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043878</td>\n",
       "      <td>-0.283247</td>\n",
       "      <td>-0.987521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043878</td>\n",
       "      <td>-0.283247</td>\n",
       "      <td>-0.987521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043878</td>\n",
       "      <td>-0.283247</td>\n",
       "      <td>-0.987521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043878</td>\n",
       "      <td>-0.283247</td>\n",
       "      <td>-0.987521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043878</td>\n",
       "      <td>-0.283247</td>\n",
       "      <td>-0.987521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-23 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904291</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.523810</td>\n",
       "      <td>-0.283247</td>\n",
       "      <td>-0.696543</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-31 17:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904292</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.523810</td>\n",
       "      <td>-0.283247</td>\n",
       "      <td>-0.696543</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-31 17:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904293</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.523810</td>\n",
       "      <td>-0.283247</td>\n",
       "      <td>-0.696543</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-31 17:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904294</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.523810</td>\n",
       "      <td>-0.283247</td>\n",
       "      <td>-0.696543</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-31 17:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904295</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.523810</td>\n",
       "      <td>-0.283247</td>\n",
       "      <td>-0.696543</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-31 17:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>904296 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        wait_time  closed  temperature      rain      wind  is_german_holiday  \\\n",
       "0             0.0       0     0.043878 -0.283247 -0.987521                  0   \n",
       "1             0.0       0     0.043878 -0.283247 -0.987521                  0   \n",
       "2             0.0       0     0.043878 -0.283247 -0.987521                  0   \n",
       "3             0.0       0     0.043878 -0.283247 -0.987521                  0   \n",
       "4             0.0       0     0.043878 -0.283247 -0.987521                  0   \n",
       "...           ...     ...          ...       ...       ...                ...   \n",
       "904291       20.0       0    -2.523810 -0.283247 -0.696543                  0   \n",
       "904292       20.0       0    -2.523810 -0.283247 -0.696543                  0   \n",
       "904293        5.0       0    -2.523810 -0.283247 -0.696543                  0   \n",
       "904294        0.0       0    -2.523810 -0.283247 -0.696543                  0   \n",
       "904295        5.0       0    -2.523810 -0.283247 -0.696543                  0   \n",
       "\n",
       "        is_swiss_holiday  is_french_holiday         time_bucket  day_of_week  \\\n",
       "0                      0                  0 2017-05-23 09:00:00            1   \n",
       "1                      0                  0 2017-05-23 09:00:00            1   \n",
       "2                      0                  0 2017-05-23 09:00:00            1   \n",
       "3                      0                  0 2017-05-23 09:00:00            1   \n",
       "4                      0                  0 2017-05-23 09:00:00            1   \n",
       "...                  ...                ...                 ...          ...   \n",
       "904291                 0                  0 2024-12-31 17:30:00            1   \n",
       "904292                 0                  0 2024-12-31 17:30:00            1   \n",
       "904293                 0                  0 2024-12-31 17:30:00            1   \n",
       "904294                 0                  0 2024-12-31 17:30:00            1   \n",
       "904295                 0                  0 2024-12-31 17:30:00            1   \n",
       "\n",
       "        ...  season_summer  season_winter  year_2017  year_2018  year_2019  \\\n",
       "0       ...            0.0            0.0        1.0        0.0        0.0   \n",
       "1       ...            0.0            0.0        1.0        0.0        0.0   \n",
       "2       ...            0.0            0.0        1.0        0.0        0.0   \n",
       "3       ...            0.0            0.0        1.0        0.0        0.0   \n",
       "4       ...            0.0            0.0        1.0        0.0        0.0   \n",
       "...     ...            ...            ...        ...        ...        ...   \n",
       "904291  ...            0.0            1.0        0.0        0.0        0.0   \n",
       "904292  ...            0.0            1.0        0.0        0.0        0.0   \n",
       "904293  ...            0.0            1.0        0.0        0.0        0.0   \n",
       "904294  ...            0.0            1.0        0.0        0.0        0.0   \n",
       "904295  ...            0.0            1.0        0.0        0.0        0.0   \n",
       "\n",
       "        year_2020  year_2021  year_2022  year_2023  year_2024  \n",
       "0             0.0        0.0        0.0        0.0        0.0  \n",
       "1             0.0        0.0        0.0        0.0        0.0  \n",
       "2             0.0        0.0        0.0        0.0        0.0  \n",
       "3             0.0        0.0        0.0        0.0        0.0  \n",
       "4             0.0        0.0        0.0        0.0        0.0  \n",
       "...           ...        ...        ...        ...        ...  \n",
       "904291        0.0        0.0        0.0        0.0        1.0  \n",
       "904292        0.0        0.0        0.0        0.0        1.0  \n",
       "904293        0.0        0.0        0.0        0.0        1.0  \n",
       "904294        0.0        0.0        0.0        0.0        1.0  \n",
       "904295        0.0        0.0        0.0        0.0        1.0  \n",
       "\n",
       "[904296 rows x 67 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sanity_check(data_path):\n",
    "    \"\"\"\n",
    "    Perform sanity checks on the processed theme park data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path : str\n",
    "        Path to the processed parquet file\n",
    "    \"\"\"\n",
    "    print(\"Loading processed data...\")\n",
    "    df = pd.read_parquet(data_path)\n",
    "    \n",
    "    print(f\"\\n==== Basic Information ====\")\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    \n",
    "    # Check 1: Missing values\n",
    "    print(\"\\n==== Check 1: Missing Values ====\")\n",
    "    missing = df.isna().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"Warning: Missing values found!\")\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(\"✓ No missing values found\")\n",
    "    \n",
    "    # Check 2: Feature ranges\n",
    "    print(\"\\n==== Check 2: Feature Ranges ====\")\n",
    "    \n",
    "    # Cyclical features should be between -1 and 1\n",
    "    cyclical_cols = [col for col in df.columns if col.endswith('_sin') or col.endswith('_cos')]\n",
    "    for col in cyclical_cols:\n",
    "        min_val, max_val = df[col].min(), df[col].max()\n",
    "        if min_val < -1.1 or max_val > 1.1:  # allow small floating point errors\n",
    "            print(f\"Warning: {col} range is [{min_val:.2f}, {max_val:.2f}], expected [-1, 1]\")\n",
    "        else:\n",
    "            print(f\"✓ {col} is within expected range [-1, 1]\")\n",
    "    \n",
    "    # One-hot encoded features should be 0 or 1\n",
    "    one_hot_cols = [\n",
    "        col for col in df.columns if \n",
    "        col.startswith('ride_name_') or \n",
    "        col.startswith('part_of_day_') or\n",
    "        col.startswith('season_')\n",
    "    ]\n",
    "    \n",
    "    for col in one_hot_cols:\n",
    "        unique_vals = df[col].unique()\n",
    "        if not np.all(np.isin(unique_vals, [0, 1])):\n",
    "            print(f\"Warning: {col} contains values other than 0 and 1: {unique_vals}\")\n",
    "        else:\n",
    "            print(f\"✓ {col} contains only 0 and 1 as expected\")\n",
    "    \n",
    "    # Boolean features should be 0 or 1\n",
    "    bool_cols = ['closed', 'is_german_holiday', 'is_swiss_holiday', 'is_french_holiday', 'is_weekend']\n",
    "    bool_cols = [col for col in bool_cols if col in df.columns]\n",
    "    \n",
    "    for col in bool_cols:\n",
    "        unique_vals = df[col].unique()\n",
    "        if not np.all(np.isin(unique_vals, [0, 1])):\n",
    "            print(f\"Warning: {col} contains values other than 0 and 1: {unique_vals}\")\n",
    "        else:\n",
    "            print(f\"✓ {col} contains only 0 and 1 as expected\")\n",
    "    \n",
    "    # Check 3: Consistency checks\n",
    "    print(\"\\n==== Check 3: Consistency Checks ====\")\n",
    "    \n",
    "    # Weekday features should be consistent with is_weekend\n",
    "    if 'weekday' in df.columns and 'is_weekend' in df.columns:\n",
    "        weekend_mask = df['weekday'] >= 5\n",
    "        is_weekend_mask = df['is_weekend'] == 1\n",
    "        \n",
    "        if (weekend_mask != is_weekend_mask).sum() > 0:\n",
    "            print(f\"Warning: 'weekday' and 'is_weekend' are inconsistent in {(weekend_mask != is_weekend_mask).sum()} rows\")\n",
    "        else:\n",
    "            print(\"✓ 'weekday' and 'is_weekend' are consistent\")\n",
    "    \n",
    "    # Ride names should sum to 1 for each row (one ride per observation)\n",
    "    ride_cols = [col for col in df.columns if col.startswith('ride_name_')]\n",
    "    ride_sums = df[ride_cols].sum(axis=1)\n",
    "    \n",
    "    if not np.all(ride_sums == 1):\n",
    "        print(f\"Warning: Some rows have {(ride_sums != 1).sum()} ride assignments that don't sum to 1\")\n",
    "        print(f\"Min: {ride_sums.min()}, Max: {ride_sums.max()}\")\n",
    "    else:\n",
    "        print(\"✓ Each observation has exactly one ride assigned\")\n",
    "    \n",
    "    # Part of day should sum to 1 for each row\n",
    "    part_of_day_cols = [col for col in df.columns if col.startswith('part_of_day_')]\n",
    "    part_of_day_sums = df[part_of_day_cols].sum(axis=1)\n",
    "    \n",
    "    if not np.all(part_of_day_sums == 1):\n",
    "        print(f\"Warning: Some rows have part_of_day assignments that don't sum to 1\")\n",
    "        print(f\"Min: {part_of_day_sums.min()}, Max: {part_of_day_sums.max()}\")\n",
    "    else:\n",
    "        print(\"✓ Each observation has exactly one part_of_day assigned\")\n",
    "    \n",
    "    # Season should sum to 1 for each row\n",
    "    season_cols = [col for col in df.columns if col.startswith('season_')]\n",
    "    season_sums = df[season_cols].sum(axis=1)\n",
    "    \n",
    "    if not np.all(season_sums == 1):\n",
    "        print(f\"Warning: Some rows have season assignments that don't sum to 1\")\n",
    "        print(f\"Min: {season_sums.min()}, Max: {season_sums.max()}\")\n",
    "    else:\n",
    "        print(\"✓ Each observation has exactly one season assigned\")\n",
    "    \n",
    "    # Check 4: Scaled numerical features\n",
    "    print(\"\\n==== Check 4: Scaled Numerical Features ====\")\n",
    "    num_cols = ['temperature', 'rain', 'wind', 'year']\n",
    "    num_cols = [col for col in num_cols if col in df.columns]\n",
    "    \n",
    "    for col in num_cols:\n",
    "        mean, std = df[col].mean(), df[col].std()\n",
    "        if abs(mean) > 0.1 or abs(std - 1) > 0.1:\n",
    "            print(f\"Warning: {col} may not be properly scaled. Mean: {mean:.4f}, Std: {std:.4f}\")\n",
    "        else:\n",
    "            print(f\"✓ {col} appears properly scaled (mean ≈ 0, std ≈ 1)\")\n",
    "    \n",
    "    # Check 5: Correlations between cyclical features\n",
    "    print(\"\\n==== Check 5: Cyclical Feature Correlations ====\")\n",
    "    for base in ['month', 'hour', 'weekday', 'minute']:\n",
    "        sin_col = f'{base}_sin'\n",
    "        cos_col = f'{base}_cos'\n",
    "        \n",
    "        if sin_col in df.columns and cos_col in df.columns:\n",
    "            corr = df[sin_col].corr(df[cos_col])\n",
    "            if abs(corr) > 0.1:\n",
    "                print(f\"Warning: Correlation between {sin_col} and {cos_col} is {corr:.4f}, expected near 0\")\n",
    "            else:\n",
    "                print(f\"✓ {sin_col} and {cos_col} have low correlation as expected\")\n",
    "    \n",
    "    # Check 6: Wait time distribution\n",
    "    if 'wait_time' in df.columns:\n",
    "        print(\"\\n==== Check 6: Wait Time Distribution ====\")\n",
    "        wait_time = df['wait_time']\n",
    "        print(f\"Wait time min: {wait_time.min()}, mean: {wait_time.mean():.2f}, max: {wait_time.max()}\")\n",
    "        \n",
    "        if wait_time.min() < 0:\n",
    "            print(f\"Warning: Negative wait times found: {wait_time[wait_time < 0].count()} values\")\n",
    "        \n",
    "        # Check for extreme outliers (> 5 std from mean)\n",
    "        mean, std = wait_time.mean(), wait_time.std()\n",
    "        outliers = wait_time[(wait_time > mean + 5*std) | (wait_time < mean - 5*std)]\n",
    "        if len(outliers) > 0:\n",
    "            print(f\"Warning: {len(outliers)} extreme wait time outliers found\")\n",
    "            print(f\"Outlier values: {sorted(outliers.unique())}\")\n",
    "        else:\n",
    "            print(\"✓ No extreme outliers in wait times\")\n",
    "    \n",
    "    # Check 7: Closed rides should have wait time 0 or NaN\n",
    "    if 'closed' in df.columns and 'wait_time' in df.columns:\n",
    "        print(\"\\n==== Check 7: Closed Rides and Wait Times ====\")\n",
    "        closed_rides = df[df['closed'] == 1]\n",
    "        if len(closed_rides) > 0:\n",
    "            invalid_waits = closed_rides[(closed_rides['wait_time'] > 0) & (~closed_rides['wait_time'].isna())]\n",
    "            if len(invalid_waits) > 0:\n",
    "                print(f\"Warning: {len(invalid_waits)} closed rides have wait times > 0\")\n",
    "                print(f\"Example: {invalid_waits[['wait_time']].head()}\")\n",
    "            else:\n",
    "                print(\"✓ All closed rides have wait time 0 or NaN as expected\")\n",
    "        else:\n",
    "            print(\"No closed rides in the dataset\")\n",
    "    \n",
    "    # Check 8: inspection of cyclical features\n",
    "    print(\"\\n==== Check 8: Inspection of Cyclical Features ====\")\n",
    "\n",
    "    cyclical_pairs = []\n",
    "    for base in ['month', 'hour', 'weekday']:\n",
    "        if f'{base}_sin' in df.columns and f'{base}_cos' in df.columns:\n",
    "            cyclical_pairs.append((base, f'{base}_sin', f'{base}_cos'))\n",
    "    \n",
    "    print(f\"Cyclical encodings should form circular patterns when sin/cos components are plotted against each other\")\n",
    "    for base, sin_col, cos_col in cyclical_pairs:\n",
    "        circle_check = np.sqrt(df[sin_col]**2 + df[cos_col]**2)\n",
    "        if (abs(circle_check - 1) > 0.1).any():\n",
    "            print(f\"Warning: {base} cyclical encoding doesn't maintain unit circle (sin²+cos²=1)\")\n",
    "            print(f\"Min: {circle_check.min():.4f}, Max: {circle_check.max():.4f}\")\n",
    "        else:\n",
    "            print(f\"✓ {base} cyclical encoding maintains unit circle property\")\n",
    "\n",
    "    print(\"\\n==== Summary ====\")\n",
    "    print(\"Sanity check complete. Review the warnings above if any.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "sanity_check(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af549b5",
   "metadata": {},
   "source": [
    "### Explanation for Warning\n",
    "- Warning: Correlation between month_sin and month_cos is -0.2893, expected near 0\n",
    "    - A perfect -1.0 correlation between sin and cos components typically happens when the values are concentrated at specific points (like only 0, 15, 30, 45 minutes). This is the case in the bucket variant\n",
    "- Warning: Correlation between hour_sin and hour_cos is -0.6495, expected near 0\n",
    "    - This happens because most data points are from 10 AM to 6 PM, this creates a correlation\n",
    "- Warning: Correlation between minute_sin and minute_cos is -1.0000, expected near 0\n",
    "    - Seasonality in the data - the park has have more data points from certain months. Also we dropped 3 months"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3fba5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspro2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

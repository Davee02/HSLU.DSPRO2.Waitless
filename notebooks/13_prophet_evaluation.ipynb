{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7559d194",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from prophet import Prophet\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Define consistent colors for plots\n",
    "TRAIN_COLOR = 'steelblue'\n",
    "TRAIN_FILL_COLOR = 'steelblue'\n",
    "TRAIN_FILL_ALPHA = 0.3\n",
    "VAL_COLOR = 'coral'\n",
    "VAL_FILL_COLOR = 'coral'\n",
    "VAL_FILL_ALPHA = 0.3\n",
    "TEST_COLOR = 'forestgreen'\n",
    "TEST_FILL_COLOR = 'forestgreen'\n",
    "TEST_FILL_ALPHA = 0.3\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7734c079",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a489690f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = pd.read_parquet(file_path)\n",
    "    return data\n",
    "\n",
    "def check_for_missing_values(data):\n",
    "    missing_values = data.isnull().sum()\n",
    "    if missing_values.any():\n",
    "        print(\"Missing values found in the dataset:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "    else:\n",
    "        print(\"No missing values found in the dataset.\")\n",
    "    return missing_values\n",
    "\n",
    "def split_data(data, train_years, val_year, test_year):\n",
    "    data['time_bucket'] = pd.to_datetime(data['time_bucket'])\n",
    "    \n",
    "    train_data = data[data['time_bucket'].dt.year.isin(train_years)]\n",
    "    val_data = data[data['time_bucket'].dt.year == val_year]\n",
    "    test_data = data[data['time_bucket'].dt.year == test_year]\n",
    "    \n",
    "    print(f\"Train data size: {len(train_data)}\")\n",
    "    print(f\"Validation data size: {len(val_data)}\")\n",
    "    print(f\"Test data size: {len(test_data)}\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def filter_ride_data(data, ride_name):\n",
    "    return data[data[f'ride_name_{ride_name}'] == True].copy()\n",
    "\n",
    "def get_all_rides(data):\n",
    "    ride_columns = [col for col in data.columns if col.startswith('ride_name_')]\n",
    "    return [col.replace('ride_name_', '') for col in ride_columns]\n",
    "\n",
    "def filter_to_operating_hours(ride_data):\n",
    "    # Determine operating hours from data where wait times > 0\n",
    "    operating_hours = ride_data[ride_data[\"wait_time\"] > 0].groupby(\n",
    "        ride_data[\"timestamp\"].dt.date\n",
    "    )[\"timestamp\"].agg(['min', 'max']).reset_index()\n",
    "    \n",
    "    # Extract opening and closing hours\n",
    "    operating_hours['opening_hour'] = pd.to_datetime(operating_hours['min']).dt.hour\n",
    "    operating_hours['closing_hour'] = pd.to_datetime(operating_hours['max']).dt.hour\n",
    "    \n",
    "    # Set reasonable boundaries for operating hours\n",
    "    operating_hours['opening_hour'] = operating_hours['opening_hour'].clip(lower=9, upper=11)\n",
    "    operating_hours['closing_hour'] = operating_hours['closing_hour'].clip(lower=17, upper=21)\n",
    "    \n",
    "    # Create date-to-hours mapping\n",
    "    date_to_hours = {}\n",
    "    for _, row in operating_hours.iterrows():\n",
    "        date_to_hours[row['timestamp']] = (row['opening_hour'], row['closing_hour'])\n",
    "    \n",
    "    # Filter data to operating hours only\n",
    "    def is_operating_hour(timestamp):\n",
    "        date = timestamp.date()\n",
    "        if date not in date_to_hours:\n",
    "            # Return 0 for dates not in mapping (likely closed days)\n",
    "            return 0\n",
    "        \n",
    "        open_hour, close_hour = date_to_hours[date]\n",
    "        hour = timestamp.hour\n",
    "        return 1 if open_hour <= hour < close_hour else 0\n",
    "    \n",
    "    ride_data['operating_hour'] = ride_data['timestamp'].apply(is_operating_hour)\n",
    "    ride_data = ride_data[ride_data['operating_hour'] == 1]\n",
    "    ride_data = ride_data.drop(columns=[\"operating_hour\"])\n",
    "    \n",
    "    return ride_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f98928",
   "metadata": {},
   "source": [
    "## Model Loading and Prediction Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c11403a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BaseTimeSeriesModel:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.forecast = None\n",
    "        self.holidays = None\n",
    "        \n",
    "    def prepare_prophet_dataframe(self, data, include_y=True):\n",
    "        prophet_df = data[['timestamp', 'wait_time', 'temperature_unscaled', 'rain_unscaled', 'is_weekend']].copy()\n",
    "        prophet_df = prophet_df.rename(columns={'timestamp': 'ds', 'wait_time': 'y', 'temperature_unscaled': 'temperature', 'rain_unscaled': 'rain'})\n",
    "\n",
    "        if not include_y:\n",
    "            prophet_df = prophet_df.drop([\"y\"], axis=1) \n",
    "        \n",
    "        # Add additional features\n",
    "        prophet_df['temp_squared'] = prophet_df['temperature'] ** 2\n",
    "        prophet_df['high_temp'] = (prophet_df['temperature'] > 25).astype(int)\n",
    "        prophet_df['any_rain'] = (prophet_df['rain'] > 0).astype(int)\n",
    "        prophet_df['temp_weekend'] = prophet_df['temperature'] * prophet_df['is_weekend']\n",
    "        prophet_df['rain_weekend'] = prophet_df['rain'] * prophet_df['is_weekend']\n",
    "        \n",
    "        return prophet_df\n",
    "    \n",
    "    def predict(self, future_df):\n",
    "        # Add required columns for prediction if they're not already present\n",
    "        future_df = future_df.copy()\n",
    "        \n",
    "        # Add COVID regressors if they're not already present\n",
    "        if 'between_covid_lockdowns' not in future_df.columns:\n",
    "            future_df['between_covid_lockdowns'] = 0\n",
    "            covid_period = (future_df['ds'] >= '2020-05-20') & (future_df['ds'] <= '2020-11-01')\n",
    "            future_df.loc[covid_period, 'between_covid_lockdowns'] = 1\n",
    "            \n",
    "        if 'covid_recovery' not in future_df.columns:\n",
    "            future_df['covid_recovery'] = 0\n",
    "            recovery_period = (future_df['ds'] >= '2021-05-21') & (future_df['ds'] <= '2021-08-31')\n",
    "            future_df.loc[recovery_period, 'covid_recovery'] = 1\n",
    "        \n",
    "        # Make predictions\n",
    "        self.forecast = self.model.predict(future_df)\n",
    "        \n",
    "        # Apply post-processing\n",
    "        self.forecast = self.post_process_forecast(self.forecast)\n",
    "        \n",
    "        return self.forecast\n",
    "    \n",
    "    def post_process_forecast(self, forecast):\n",
    "        forecast = forecast.copy()\n",
    "        \n",
    "        # Correct negative predictions\n",
    "        negative_mask = forecast['yhat'] < 0\n",
    "        forecast.loc[negative_mask, 'yhat'] = 0\n",
    "        forecast.loc[negative_mask, 'yhat_lower'] = 0\n",
    "        forecast.loc[negative_mask, 'yhat_upper'] = 0\n",
    "        \n",
    "        return forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba28d8",
   "metadata": {},
   "source": [
    "## Model Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b1c4b97",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(ride_df, actual_values, predictions, title=\"\"):\n",
    "    # Calculate metrics\n",
    "    mae = np.mean(np.abs(predictions - actual_values))\n",
    "    rmse = np.sqrt(np.mean(np.square(predictions - actual_values)))\n",
    "    \n",
    "    # For sMAPE, avoid division by zero\n",
    "    epsilon = 1e-8\n",
    "    abs_pct_errors = np.abs(predictions - actual_values) / (np.abs(predictions) + np.abs(actual_values) + epsilon)\n",
    "    # Only include points where actual values are non-zero\n",
    "    non_zero_mask = (actual_values > 0) & (predictions > 0)\n",
    "    smape = np.mean(abs_pct_errors[non_zero_mask]) * 100\n",
    "\n",
    "    # Calculate additional metrics for test evaluation\n",
    "    mse = np.mean(np.square(predictions - actual_values))\n",
    "    \n",
    "    # R-squared\n",
    "    ss_res = np.sum((actual_values - predictions) ** 2)\n",
    "    ss_tot = np.sum((actual_values - np.mean(actual_values)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\n{title} MAE: {mae:.2f} minutes\")\n",
    "    print(f\"{title} RMSE: {rmse:.2f} minutes\")\n",
    "    print(f\"{title} MSE: {mse:.2f}\")\n",
    "    print(f\"{title} sMAPE: {smape:.2f}%\")\n",
    "    \n",
    "    # Create a DataFrame with results for time-based analysis\n",
    "    results_df = pd.DataFrame({\n",
    "        'time_bucket': ride_df['time_bucket'].values,\n",
    "        'actual': actual_values,\n",
    "        'predicted': predictions,\n",
    "    })\n",
    "    \n",
    "    # Add time components\n",
    "    results_df['hour'] = results_df['time_bucket'].dt.hour\n",
    "    results_df['day_of_week'] = results_df['time_bucket'].dt.dayofweek\n",
    "    results_df['month'] = results_df['time_bucket'].dt.month\n",
    "    results_df['date'] = results_df['time_bucket'].dt.date\n",
    "    \n",
    "    # Calculate errors\n",
    "    results_df['error'] = results_df['predicted'] - results_df['actual']\n",
    "    results_df['abs_error'] = np.abs(results_df['error'])\n",
    "    results_df['pct_error'] = abs_pct_errors * 100\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Actual vs Predicted scatter plot\n",
    "    axes[0,0].scatter(actual_values, predictions, alpha=0.5, color=TEST_COLOR)\n",
    "    max_val = max(np.max(actual_values), np.max(predictions))\n",
    "    axes[0,0].plot([0, max_val], [0, max_val], 'k--')\n",
    "    axes[0,0].set_xlabel('Actual Wait Time (minutes)')\n",
    "    axes[0,0].set_ylabel('Predicted Wait Time (minutes)')\n",
    "    axes[0,0].set_title(f'{title} - Actual vs Predicted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error distribution\n",
    "    axes[0,1].hist(results_df['error'], bins=30, alpha=0.7, color=TEST_COLOR, edgecolor='black')\n",
    "    axes[0,1].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0,1].set_xlabel('Prediction Error (Predicted - Actual)')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].set_title(f'{title} - Error Distribution')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hourly analysis\n",
    "    hourly_errors = results_df.groupby('hour')['abs_error'].mean()\n",
    "    hourly_errors.plot(kind='bar', ax=axes[1,0], color=TEST_COLOR, alpha=0.7)\n",
    "    axes[1,0].set_xlabel('Hour of Day')\n",
    "    axes[1,0].set_ylabel('Mean Absolute Error (minutes)')\n",
    "    axes[1,0].set_title(f'{title} - Error Analysis by Hour of Day')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Time series plot (sample of data)\n",
    "    if len(results_df) > 1000:\n",
    "        sample_df = results_df.sample(n=1000).sort_values('time_bucket')\n",
    "    else:\n",
    "        sample_df = results_df.sort_values('time_bucket')\n",
    "    \n",
    "    axes[1,1].plot(sample_df['time_bucket'], sample_df['actual'], \n",
    "                   label='Actual', alpha=0.7, color='blue')\n",
    "    axes[1,1].plot(sample_df['time_bucket'], sample_df['predicted'], \n",
    "                   label='Predicted', alpha=0.7, color=TEST_COLOR)\n",
    "    axes[1,1].set_xlabel('Date')\n",
    "    axes[1,1].set_ylabel('Wait Time (minutes)')\n",
    "    axes[1,1].set_title(f'{title} - Time Series (Sample)')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "        \"mse\": mse,\n",
    "        \"smape\": smape,\n",
    "        \"r2\": r2\n",
    "    }\n",
    "    \n",
    "    return metrics, results_df, fig\n",
    "\n",
    "def evaluate_confidence_intervals(actual_values, lower_bounds, upper_bounds, predictions):\n",
    "    # Coverage rate (percentage of actual values within confidence intervals)\n",
    "    within_ci = (actual_values >= lower_bounds) & (actual_values <= upper_bounds)\n",
    "    coverage_rate = np.mean(within_ci) * 100\n",
    "    \n",
    "    # Average interval width\n",
    "    avg_interval_width = np.mean(upper_bounds - lower_bounds)\n",
    "    \n",
    "    # Interval score (lower is better)\n",
    "    alpha = 0.05  # For 95% confidence intervals\n",
    "    interval_score = np.mean(\n",
    "        (upper_bounds - lower_bounds) + \n",
    "        (2/alpha) * np.maximum(0, lower_bounds - actual_values) +\n",
    "        (2/alpha) * np.maximum(0, actual_values - upper_bounds)\n",
    "    )\n",
    "    \n",
    "    print(f\"Confidence Interval Evaluation:\")\n",
    "    print(f\"Coverage Rate: {coverage_rate:.1f}% (target: 95%)\")\n",
    "    print(f\"Average Interval Width: {avg_interval_width:.2f} minutes\")\n",
    "    print(f\"Interval Score: {interval_score:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        \"coverage_rate\": coverage_rate,\n",
    "        \"avg_interval_width\": avg_interval_width,\n",
    "        \"interval_score\": interval_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3778ec",
   "metadata": {},
   "source": [
    "## Model Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24f47364",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_model(ride_name, output_dir=\"models\"):\n",
    "    # Create ride-specific directory path\n",
    "    ride_dir = os.path.join(output_dir, ride_name.replace(\" \", \"_\"))\n",
    "    \n",
    "    # Check if models exist\n",
    "    if not os.path.exists(ride_dir):\n",
    "        return None, None\n",
    "    \n",
    "    # Load Prophet model\n",
    "    try:\n",
    "        with open(os.path.join(ride_dir, \"prophet_model.pkl\"), \"rb\") as f:\n",
    "            prophet_model_obj = pickle.load(f)\n",
    "    except:\n",
    "        print(f\"Could not load model for {ride_name}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Initialize BaseTimeSeriesModel and set the loaded model\n",
    "    prophet_ts = BaseTimeSeriesModel()\n",
    "    prophet_ts.model = prophet_model_obj\n",
    "    \n",
    "    # Load holidays if they exist\n",
    "    holidays_path = os.path.join(ride_dir, \"holidays.csv\")\n",
    "    if os.path.exists(holidays_path):\n",
    "        prophet_ts.holidays = pd.read_csv(holidays_path)\n",
    "        prophet_ts.holidays['ds'] = pd.to_datetime(prophet_ts.holidays['ds'])\n",
    "    \n",
    "    # Load metrics\n",
    "    with open(os.path.join(ride_dir, \"metrics.json\"), \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    return prophet_ts, metrics\n",
    "\n",
    "def get_processed_rides(output_dir=\"models\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        return []\n",
    "    \n",
    "    # Get all subdirectories in the output directory that contain a model file\n",
    "    processed_rides = []\n",
    "    for d in os.listdir(output_dir):\n",
    "        dir_path = os.path.join(output_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            model_path = os.path.join(dir_path, \"prophet_model.pkl\")\n",
    "            if os.path.exists(model_path):\n",
    "                processed_rides.append(d)\n",
    "    \n",
    "    # Convert directory names back to ride names\n",
    "    processed_rides = [ride.replace(\"_\", \" \") for ride in processed_rides]\n",
    "    \n",
    "    return processed_rides\n",
    "\n",
    "def save_test_results(ride_name, test_metrics, ci_metrics, results_df, fig, output_dir=\"models\"):\n",
    "    # Create ride-specific directory\n",
    "    ride_dir = os.path.join(output_dir, ride_name.replace(\" \", \"_\"))\n",
    "    \n",
    "    # Load existing metrics and add test results\n",
    "    metrics_path = os.path.join(ride_dir, \"metrics.json\")\n",
    "    with open(metrics_path, \"r\") as f:\n",
    "        all_metrics = json.load(f)\n",
    "    \n",
    "    # Add test metrics\n",
    "    all_metrics[\"test\"] = test_metrics\n",
    "    all_metrics[\"confidence_intervals\"] = ci_metrics\n",
    "    all_metrics[\"test_timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Save updated metrics\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(all_metrics, f, indent=4)\n",
    "    \n",
    "    # Save test results\n",
    "    results_dir = os.path.join(ride_dir, \"results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    results_df.to_csv(os.path.join(results_dir, \"test_results.csv\"), index=False)\n",
    "    \n",
    "    # Save test evaluation figure\n",
    "    fig_dir = os.path.join(ride_dir, \"figures\")\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "    fig.savefig(os.path.join(fig_dir, \"test_evaluation.png\"), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"Test results saved for {ride_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cd91d",
   "metadata": {},
   "source": [
    "## Test Set Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3c5c8bb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_single_ride_on_test(ride_name, test_data, model_dir=\"models\"):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Evaluating ride: {ride_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load the saved model\n",
    "    prophet_ts, existing_metrics = load_model(ride_name, model_dir)\n",
    "    \n",
    "    if prophet_ts is None:\n",
    "        print(f\"No saved model found for {ride_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Filter test data for the current ride\n",
    "    ride_test_data = filter_ride_data(test_data, ride_name)\n",
    "    \n",
    "    print(f\"Test data size: {len(ride_test_data)}\")\n",
    "    \n",
    "    # Skip if not enough test data\n",
    "    if len(ride_test_data) < 10:\n",
    "        print(f\"Skipping {ride_name} due to insufficient test data\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare test data for prediction\n",
    "    future_test = prophet_ts.prepare_prophet_dataframe(ride_test_data, include_y=False)\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions on test set...\")\n",
    "    test_forecast = prophet_ts.predict(future_test)\n",
    "    \n",
    "    # Merge predictions with original data\n",
    "    ride_test_data_with_forecast = pd.merge(\n",
    "        ride_test_data,\n",
    "        test_forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']],\n",
    "        left_on='time_bucket', \n",
    "        right_on='ds',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Remove any rows with missing predictions\n",
    "    ride_test_data_with_forecast = ride_test_data_with_forecast.dropna(subset=['yhat'])\n",
    "    \n",
    "    if len(ride_test_data_with_forecast) == 0:\n",
    "        print(f\"No valid predictions generated for {ride_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_actuals = ride_test_data_with_forecast['wait_time'].values\n",
    "    test_predictions = ride_test_data_with_forecast['yhat'].values\n",
    "    test_lower = ride_test_data_with_forecast['yhat_lower'].values\n",
    "    test_upper = ride_test_data_with_forecast['yhat_upper'].values\n",
    "    \n",
    "    # Evaluate point predictions\n",
    "    test_metrics, test_results_df, fig_evaluation = evaluate_model(\n",
    "        ride_test_data_with_forecast, test_actuals, test_predictions, \n",
    "        title=f\"{ride_name} - Test Set\"\n",
    "    )\n",
    "    \n",
    "    # Evaluate confidence intervals\n",
    "    ci_metrics = evaluate_confidence_intervals(\n",
    "        test_actuals, test_lower, test_upper, test_predictions\n",
    "    )\n",
    "    \n",
    "    # Add residual analysis\n",
    "    test_results_df['residual'] = test_results_df['actual'] - test_results_df['predicted']\n",
    "    test_results_df['abs_residual'] = np.abs(test_results_df['residual'])\n",
    "    \n",
    "    # Save results\n",
    "    save_test_results(ride_name, test_metrics, ci_metrics, test_results_df, \n",
    "                     fig_evaluation, model_dir)\n",
    "    \n",
    "    # Combine all metrics\n",
    "    combined_metrics = {\n",
    "        \"test\": test_metrics,\n",
    "        \"confidence_intervals\": ci_metrics,\n",
    "        \"validation\": existing_metrics.get(\"validation\", {}),\n",
    "        \"data_counts\": {\n",
    "            **existing_metrics.get(\"data_counts\", {}),\n",
    "            \"test\": len(ride_test_data_with_forecast)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    plt.close(fig_evaluation)  # Close the figure to save memory\n",
    "    \n",
    "    return combined_metrics\n",
    "\n",
    "def evaluate_all_rides_on_test(test_data, model_dir=\"models\"):\n",
    "    # Get all rides that have saved models\n",
    "    processed_rides = get_processed_rides(model_dir)\n",
    "    print(f\"Found {len(processed_rides)} rides with saved models\")\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    all_test_results = {}\n",
    "    \n",
    "    # Process each ride\n",
    "    for i, ride_name in enumerate(processed_rides):\n",
    "        print(f\"\\nEvaluating ride {i+1}/{len(processed_rides)}: {ride_name}\")\n",
    "        \n",
    "        ride_metrics = evaluate_single_ride_on_test(ride_name, test_data, model_dir)\n",
    "        \n",
    "        if ride_metrics:\n",
    "            all_test_results[ride_name] = ride_metrics\n",
    "        \n",
    "        # Print progress\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"\\nCompleted {i + 1}/{len(processed_rides)} rides\")\n",
    "    \n",
    "    return all_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd40c9",
   "metadata": {},
   "source": [
    "## Test Results Analysis and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de9cea78",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_test_summary_report(all_results, output_dir=\"models\"):\n",
    "    # Create lists to store summary data\n",
    "    rides = []\n",
    "    val_mae = []\n",
    "    val_rmse = []\n",
    "    val_smape = []\n",
    "    test_mae = []\n",
    "    test_rmse = []\n",
    "    test_smape = []\n",
    "    test_r2 = []\n",
    "    coverage_rates = []\n",
    "    interval_widths = []\n",
    "    data_counts = []\n",
    "    \n",
    "    # Extract data from results\n",
    "    for ride_name, metrics in all_results.items():\n",
    "        if not metrics:\n",
    "            continue\n",
    "            \n",
    "        rides.append(ride_name)\n",
    "        \n",
    "        # Validation metrics\n",
    "        val_metrics = metrics.get(\"validation\", {})\n",
    "        val_mae.append(val_metrics.get(\"mae\", float('nan')))\n",
    "        val_rmse.append(val_metrics.get(\"rmse\", float('nan')))\n",
    "        val_smape.append(val_metrics.get(\"smape\", float('nan')))\n",
    "        \n",
    "        # Test metrics\n",
    "        test_metrics = metrics.get(\"test\", {})\n",
    "        test_mae.append(test_metrics.get(\"mae\", float('nan')))\n",
    "        test_rmse.append(test_metrics.get(\"rmse\", float('nan')))\n",
    "        test_smape.append(test_metrics.get(\"smape\", float('nan')))\n",
    "        test_r2.append(test_metrics.get(\"r2\", float('nan')))\n",
    "        \n",
    "        # Confidence interval metrics\n",
    "        ci_metrics = metrics.get(\"confidence_intervals\", {})\n",
    "        coverage_rates.append(ci_metrics.get(\"coverage_rate\", float('nan')))\n",
    "        interval_widths.append(ci_metrics.get(\"avg_interval_width\", float('nan')))\n",
    "        \n",
    "        # Data counts\n",
    "        counts = metrics.get(\"data_counts\", {})\n",
    "        data_counts.append(f\"Val: {counts.get('validation', 0)}, Test: {counts.get('test', 0)}\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Ride Name\": rides,\n",
    "        \"Validation MAE\": val_mae,\n",
    "        \"Validation RMSE\": val_rmse,\n",
    "        \"Validation sMAPE\": val_smape,\n",
    "        \"Test MAE\": test_mae,\n",
    "        \"Test RMSE\": test_rmse,\n",
    "        \"Test sMAPE\": test_smape,\n",
    "        \"Coverage Rate (%)\": coverage_rates,\n",
    "        \"Avg Interval Width\": interval_widths,\n",
    "        \"Data Counts\": data_counts\n",
    "    })\n",
    "    \n",
    "    # Sort by test MAE\n",
    "    summary_df = summary_df.sort_values(\"Test MAE\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    summary_path = os.path.join(output_dir, \"test_evaluation_summary.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST SET EVALUATION SUMMARY:\")\n",
    "    print(f\"Total rides evaluated: {len(summary_df)}\")\n",
    "    print(f\"\\nTest Set Performance:\")\n",
    "    print(f\"  Average MAE: {np.nanmean(test_mae):.2f} ± {np.nanstd(test_mae):.2f}\")\n",
    "    print(f\"  Average RMSE: {np.nanmean(test_rmse):.2f} ± {np.nanstd(test_rmse):.2f}\")\n",
    "    print(f\"  Average sMAPE: {np.nanmean(test_smape):.2f}% ± {np.nanstd(test_smape):.2f}%\")\n",
    "    print(f\"\\nConfidence Intervals:\")\n",
    "    print(f\"  Average Coverage Rate: {np.nanmean(coverage_rates):.1f}% (target: 95%)\")\n",
    "    print(f\"  Average Interval Width: {np.nanmean(interval_widths):.2f} minutes\")\n",
    "    print(f\"\\nValidation vs Test Performance:\")\n",
    "    val_test_mae_diff = np.nanmean(test_mae) - np.nanmean(val_mae)\n",
    "    print(f\"  MAE difference (Test - Validation): {val_test_mae_diff:.2f}\")\n",
    "    val_test_rmse_diff = np.nanmean(test_rmse) - np.nanmean(val_rmse)\n",
    "    print(f\"  RMSE difference (Test - Validation): {val_test_rmse_diff:.2f}\")\n",
    "    print(f\"\\nSummary saved to: {summary_path}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create comprehensive visualizations\n",
    "    create_test_evaluation_plots(summary_df, output_dir)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "def create_test_evaluation_plots(summary_df, output_dir):\n",
    "    # Create a comprehensive plot with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # 1. Validation vs Test MAE comparison\n",
    "    valid_data = summary_df.dropna(subset=['Validation MAE', 'Test MAE'])\n",
    "    axes[0,0].scatter(valid_data['Validation MAE'], valid_data['Test MAE'], alpha=0.6, s=50)\n",
    "    max_mae = max(valid_data['Validation MAE'].max(), valid_data['Test MAE'].max())\n",
    "    axes[0,0].plot([0, max_mae], [0, max_mae], 'k--', alpha=0.7)\n",
    "    axes[0,0].set_xlabel('Validation MAE (minutes)')\n",
    "    axes[0,0].set_ylabel('Test MAE (minutes)')\n",
    "    axes[0,0].set_title('Validation vs Test MAE')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Test MAE distribution\n",
    "    axes[0,1].hist(summary_df['Test MAE'].dropna(), bins=20, alpha=0.7, \n",
    "                   color=TEST_COLOR, edgecolor='black')\n",
    "    axes[0,1].axvline(summary_df['Test MAE'].mean(), color='red', linestyle='--', \n",
    "                      label=f'Mean: {summary_df[\"Test MAE\"].mean():.1f}')\n",
    "    axes[0,1].set_xlabel('Test MAE (minutes)')\n",
    "    axes[0,1].set_ylabel('Number of Rides')\n",
    "    axes[0,1].set_title('Distribution of Test MAE')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "    # 4. Coverage rate analysis\n",
    "    axes[1,0].hist(summary_df['Coverage Rate (%)'].dropna(), bins=20, alpha=0.7, \n",
    "                   color=TEST_COLOR, edgecolor='black')\n",
    "    axes[1,0].axvline(95, color='red', linestyle='--', label='Target: 95%')\n",
    "    axes[1,0].axvline(summary_df['Coverage Rate (%)'].mean(), color='orange', \n",
    "                      linestyle='--', label=f'Mean: {summary_df[\"Coverage Rate (%)\"].mean():.1f}%')\n",
    "    axes[1,0].set_xlabel('Coverage Rate (%)')\n",
    "    axes[1,0].set_ylabel('Number of Rides')\n",
    "    axes[1,0].set_title('Confidence Interval Coverage Rate')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Top 15 rides by test performance\n",
    "    top_rides = summary_df.nsmallest(15, 'Test MAE')\n",
    "    y_pos = np.arange(len(top_rides))\n",
    "    axes[1,1].barh(y_pos, top_rides['Test MAE'], color=TEST_COLOR, alpha=0.7)\n",
    "    axes[1,1].set_yticks(y_pos)\n",
    "    axes[1,1].set_yticklabels(top_rides['Ride Name'], fontsize=8)\n",
    "    axes[1,1].set_xlabel('Test MAE (minutes)')\n",
    "    axes[1,1].set_title('Top 15 Rides by Test Performance')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Performance degradation (Test vs Validation)\n",
    "    perf_change = summary_df['Test MAE'] - summary_df['Validation MAE']\n",
    "    valid_change = perf_change.dropna()\n",
    "    axes[1,2].hist(valid_change, bins=20, alpha=0.7, color=TEST_COLOR, edgecolor='black')\n",
    "    axes[1,2].axvline(0, color='red', linestyle='--', label='No change')\n",
    "    axes[1,2].axvline(valid_change.mean(), color='orange', linestyle='--',\n",
    "                      label=f'Mean: {valid_change.mean():.1f}')\n",
    "    axes[1,2].set_xlabel('MAE Change (Test - Validation)')\n",
    "    axes[1,2].set_ylabel('Number of Rides')\n",
    "    axes[1,2].set_title('Performance Change: Test vs Validation')\n",
    "    axes[1,2].legend()\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"test_evaluation_comprehensive.png\"), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a separate plot for model performance ranking\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Sort by test MAE and show top 20\n",
    "    top_20 = summary_df.nsmallest(20, 'Test MAE')\n",
    "    \n",
    "    x_pos = np.arange(len(top_20))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x_pos - width/2, top_20['Validation MAE'], width, \n",
    "            label='Validation MAE', color=VAL_COLOR, alpha=0.8)\n",
    "    plt.bar(x_pos + width/2, top_20['Test MAE'], width, \n",
    "            label='Test MAE', color=TEST_COLOR, alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Rides (Ranked by Test Performance)')\n",
    "    plt.ylabel('Mean Absolute Error (minutes)')\n",
    "    plt.title('Top 20 Rides: Validation vs Test Performance')\n",
    "    plt.xticks(x_pos, top_20['Ride Name'], rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, \"top_20_rides_comparison.png\"), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba3ddd",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dcc9c89",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded data with 7834739 rows\n",
      "No missing values found in the dataset.\n",
      "Train data size: 297362\n",
      "Validation data size: 61851\n",
      "Test data size: 55699\n",
      "Found 30 rides with saved models\n",
      "\n",
      "Starting test set evaluation...\n",
      "Found 30 rides with saved models\n",
      "\n",
      "Evaluating ride 1/30: volo da vinci\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: volo da vinci\n",
      "==================================================\n",
      "Test data size: 1830\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "volo da vinci - Test Set MAE: 6.98 minutes\n",
      "volo da vinci - Test Set RMSE: 8.63 minutes\n",
      "volo da vinci - Test Set MSE: 74.48\n",
      "volo da vinci - Test Set sMAPE: 24.99%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 80.3% (target: 95%)\n",
      "Average Interval Width: 22.53 minutes\n",
      "Interval Score: 55.68\n",
      "Test results saved for volo da vinci\n",
      "\n",
      "Evaluating ride 2/30: matterhornblitz\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: matterhornblitz\n",
      "==================================================\n",
      "Test data size: 1830\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "matterhornblitz - Test Set MAE: 18.32 minutes\n",
      "matterhornblitz - Test Set RMSE: 21.84 minutes\n",
      "matterhornblitz - Test Set MSE: 477.18\n",
      "matterhornblitz - Test Set sMAPE: 29.89%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 46.7% (target: 95%)\n",
      "Average Interval Width: 32.25 minutes\n",
      "Interval Score: 250.09\n",
      "Test results saved for matterhornblitz\n",
      "\n",
      "Evaluating ride 3/30: fjordrafting\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: fjordrafting\n",
      "==================================================\n",
      "Test data size: 1825\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "fjordrafting - Test Set MAE: 8.86 minutes\n",
      "fjordrafting - Test Set RMSE: 11.55 minutes\n",
      "fjordrafting - Test Set MSE: 133.49\n",
      "fjordrafting - Test Set sMAPE: 28.00%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 93.5% (target: 95%)\n",
      "Average Interval Width: 37.25 minutes\n",
      "Interval Score: 54.17\n",
      "Test results saved for fjordrafting\n",
      "\n",
      "Evaluating ride 4/30: poppy towers\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: poppy towers\n",
      "==================================================\n",
      "Test data size: 1822\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "poppy towers - Test Set MAE: 1.95 minutes\n",
      "poppy towers - Test Set RMSE: 2.47 minutes\n",
      "poppy towers - Test Set MSE: 6.09\n",
      "poppy towers - Test Set sMAPE: 22.33%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 97.5% (target: 95%)\n",
      "Average Interval Width: 11.29 minutes\n",
      "Interval Score: 14.55\n",
      "Test results saved for poppy towers\n",
      "\n",
      "Evaluating ride 5/30: old mac donalds tractor fun\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: old mac donalds tractor fun\n",
      "==================================================\n",
      "Test data size: 1818\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "old mac donalds tractor fun - Test Set MAE: 2.17 minutes\n",
      "old mac donalds tractor fun - Test Set RMSE: 2.76 minutes\n",
      "old mac donalds tractor fun - Test Set MSE: 7.61\n",
      "old mac donalds tractor fun - Test Set sMAPE: 39.70%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 96.4% (target: 95%)\n",
      "Average Interval Width: 9.81 minutes\n",
      "Interval Score: 16.63\n",
      "Test results saved for old mac donalds tractor fun\n",
      "\n",
      "Evaluating ride 6/30: swiss bob run\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: swiss bob run\n",
      "==================================================\n",
      "Test data size: 1826\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "swiss bob run - Test Set MAE: 9.28 minutes\n",
      "swiss bob run - Test Set RMSE: 12.13 minutes\n",
      "swiss bob run - Test Set MSE: 147.14\n",
      "swiss bob run - Test Set sMAPE: 19.03%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 83.2% (target: 95%)\n",
      "Average Interval Width: 31.05 minutes\n",
      "Interval Score: 78.47\n",
      "Test results saved for swiss bob run\n",
      "\n",
      "Evaluating ride 7/30: dancing dingie\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: dancing dingie\n",
      "==================================================\n",
      "Test data size: 1824\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "dancing dingie - Test Set MAE: 5.04 minutes\n",
      "dancing dingie - Test Set RMSE: 6.48 minutes\n",
      "dancing dingie - Test Set MSE: 41.96\n",
      "dancing dingie - Test Set sMAPE: 31.73%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 73.4% (target: 95%)\n",
      "Average Interval Width: 14.26 minutes\n",
      "Interval Score: 54.84\n",
      "Test results saved for dancing dingie\n",
      "\n",
      "Evaluating ride 8/30: voletarium\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: voletarium\n",
      "==================================================\n",
      "Test data size: 1832\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "voletarium - Test Set MAE: 22.70 minutes\n",
      "voletarium - Test Set RMSE: 28.04 minutes\n",
      "voletarium - Test Set MSE: 786.02\n",
      "voletarium - Test Set sMAPE: 69.15%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 13.1% (target: 95%)\n",
      "Average Interval Width: 7.67 minutes\n",
      "Interval Score: 838.02\n",
      "Test results saved for voletarium\n",
      "\n",
      "Evaluating ride 9/30: euromir\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: euromir\n",
      "==================================================\n",
      "Test data size: 1832\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "euromir - Test Set MAE: 19.06 minutes\n",
      "euromir - Test Set RMSE: 23.31 minutes\n",
      "euromir - Test Set MSE: 543.26\n",
      "euromir - Test Set sMAPE: 36.37%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 59.4% (target: 95%)\n",
      "Average Interval Width: 39.80 minutes\n",
      "Interval Score: 204.14\n",
      "Test results saved for euromir\n",
      "\n",
      "Evaluating ride 10/30: castello dei medici\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: castello dei medici\n",
      "==================================================\n",
      "Test data size: 1143\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "castello dei medici - Test Set MAE: 1.72 minutes\n",
      "castello dei medici - Test Set RMSE: 2.36 minutes\n",
      "castello dei medici - Test Set MSE: 5.58\n",
      "castello dei medici - Test Set sMAPE: 13.05%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 95.3% (target: 95%)\n",
      "Average Interval Width: 10.14 minutes\n",
      "Interval Score: 13.34\n",
      "Test results saved for castello dei medici\n",
      "\n",
      "Completed 10/30 rides\n",
      "\n",
      "Evaluating ride 11/30: pegasus\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: pegasus\n",
      "==================================================\n",
      "Test data size: 1831\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "pegasus - Test Set MAE: 14.41 minutes\n",
      "pegasus - Test Set RMSE: 17.92 minutes\n",
      "pegasus - Test Set MSE: 321.15\n",
      "pegasus - Test Set sMAPE: 45.59%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 48.4% (target: 95%)\n",
      "Average Interval Width: 25.05 minutes\n",
      "Interval Score: 207.16\n",
      "Test results saved for pegasus\n",
      "\n",
      "Evaluating ride 12/30: silver star\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: silver star\n",
      "==================================================\n",
      "Test data size: 1826\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "silver star - Test Set MAE: 13.00 minutes\n",
      "silver star - Test Set RMSE: 16.25 minutes\n",
      "silver star - Test Set MSE: 264.08\n",
      "silver star - Test Set sMAPE: 25.62%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 76.1% (target: 95%)\n",
      "Average Interval Width: 37.80 minutes\n",
      "Interval Score: 116.32\n",
      "Test results saved for silver star\n",
      "\n",
      "Evaluating ride 13/30: baaa express\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: baaa express\n",
      "==================================================\n",
      "Test data size: 1824\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "baaa express - Test Set MAE: 6.20 minutes\n",
      "baaa express - Test Set RMSE: 7.90 minutes\n",
      "baaa express - Test Set MSE: 62.37\n",
      "baaa express - Test Set sMAPE: 30.04%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 76.2% (target: 95%)\n",
      "Average Interval Width: 19.70 minutes\n",
      "Interval Score: 56.24\n",
      "Test results saved for baaa express\n",
      "\n",
      "Evaluating ride 14/30: poseidon\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: poseidon\n",
      "==================================================\n",
      "Test data size: 1826\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "poseidon - Test Set MAE: 9.32 minutes\n",
      "poseidon - Test Set RMSE: 12.99 minutes\n",
      "poseidon - Test Set MSE: 168.81\n",
      "poseidon - Test Set sMAPE: 21.49%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 93.3% (target: 95%)\n",
      "Average Interval Width: 38.17 minutes\n",
      "Interval Score: 62.59\n",
      "Test results saved for poseidon\n",
      "\n",
      "Evaluating ride 15/30: tirol log flume\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: tirol log flume\n",
      "==================================================\n",
      "Test data size: 1752\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "tirol log flume - Test Set MAE: 4.76 minutes\n",
      "tirol log flume - Test Set RMSE: 8.15 minutes\n",
      "tirol log flume - Test Set MSE: 66.44\n",
      "tirol log flume - Test Set sMAPE: 30.29%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 90.6% (target: 95%)\n",
      "Average Interval Width: 20.89 minutes\n",
      "Interval Score: 45.44\n",
      "Test results saved for tirol log flume\n",
      "\n",
      "Evaluating ride 16/30: whale adventures  northern lights\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: whale adventures  northern lights\n",
      "==================================================\n",
      "Test data size: 1832\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "whale adventures  northern lights - Test Set MAE: 1.43 minutes\n",
      "whale adventures  northern lights - Test Set RMSE: 3.02 minutes\n",
      "whale adventures  northern lights - Test Set MSE: 9.09\n",
      "whale adventures  northern lights - Test Set sMAPE: 56.82%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 86.6% (target: 95%)\n",
      "Average Interval Width: 3.05 minutes\n",
      "Interval Score: 37.86\n",
      "Test results saved for whale adventures  northern lights\n",
      "\n",
      "Evaluating ride 17/30: atlantis adventure\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: atlantis adventure\n",
      "==================================================\n",
      "Test data size: 1832\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "atlantis adventure - Test Set MAE: 7.12 minutes\n",
      "atlantis adventure - Test Set RMSE: 9.16 minutes\n",
      "atlantis adventure - Test Set MSE: 83.90\n",
      "atlantis adventure - Test Set sMAPE: 39.07%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 46.0% (target: 95%)\n",
      "Average Interval Width: 10.64 minutes\n",
      "Interval Score: 113.11\n",
      "Test results saved for atlantis adventure\n",
      "\n",
      "Evaluating ride 18/30: kolumbusjolle\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: kolumbusjolle\n",
      "==================================================\n",
      "Test data size: 1821\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "kolumbusjolle - Test Set MAE: 1.54 minutes\n",
      "kolumbusjolle - Test Set RMSE: 2.13 minutes\n",
      "kolumbusjolle - Test Set MSE: 4.52\n",
      "kolumbusjolle - Test Set sMAPE: 13.10%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 92.8% (target: 95%)\n",
      "Average Interval Width: 8.84 minutes\n",
      "Interval Score: 12.36\n",
      "Test results saved for kolumbusjolle\n",
      "\n",
      "Evaluating ride 19/30: eurosat  cancan coaster\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: eurosat  cancan coaster\n",
      "==================================================\n",
      "Test data size: 1832\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "eurosat  cancan coaster - Test Set MAE: 17.50 minutes\n",
      "eurosat  cancan coaster - Test Set RMSE: 20.71 minutes\n",
      "eurosat  cancan coaster - Test Set MSE: 428.75\n",
      "eurosat  cancan coaster - Test Set sMAPE: 27.00%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 70.5% (target: 95%)\n",
      "Average Interval Width: 46.00 minutes\n",
      "Interval Score: 140.92\n",
      "Test results saved for eurosat  cancan coaster\n",
      "\n",
      "Evaluating ride 20/30: blue fire megacoaster\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: blue fire megacoaster\n",
      "==================================================\n",
      "Test data size: 1823\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "blue fire megacoaster - Test Set MAE: 33.41 minutes\n",
      "blue fire megacoaster - Test Set RMSE: 39.16 minutes\n",
      "blue fire megacoaster - Test Set MSE: 1533.26\n",
      "blue fire megacoaster - Test Set sMAPE: 41.46%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 51.8% (target: 95%)\n",
      "Average Interval Width: 66.11 minutes\n",
      "Interval Score: 398.51\n",
      "Test results saved for blue fire megacoaster\n",
      "\n",
      "Completed 20/30 rides\n",
      "\n",
      "Evaluating ride 21/30: josefinas magical imperial journey\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: josefinas magical imperial journey\n",
      "==================================================\n",
      "Test data size: 1821\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "josefinas magical imperial journey - Test Set MAE: 6.71 minutes\n",
      "josefinas magical imperial journey - Test Set RMSE: 8.56 minutes\n",
      "josefinas magical imperial journey - Test Set MSE: 73.28\n",
      "josefinas magical imperial journey - Test Set sMAPE: 35.81%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 59.3% (target: 95%)\n",
      "Average Interval Width: 13.25 minutes\n",
      "Interval Score: 87.32\n",
      "Test results saved for josefinas magical imperial journey\n",
      "\n",
      "Evaluating ride 22/30: vienna wave swing  glckspilz\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: vienna wave swing  glckspilz\n",
      "==================================================\n",
      "Test data size: 1822\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "vienna wave swing  glckspilz - Test Set MAE: 3.65 minutes\n",
      "vienna wave swing  glckspilz - Test Set RMSE: 4.33 minutes\n",
      "vienna wave swing  glckspilz - Test Set MSE: 18.73\n",
      "vienna wave swing  glckspilz - Test Set sMAPE: 24.78%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 73.5% (target: 95%)\n",
      "Average Interval Width: 10.02 minutes\n",
      "Interval Score: 28.42\n",
      "Test results saved for vienna wave swing  glckspilz\n",
      "\n",
      "Evaluating ride 23/30: vindjammer\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: vindjammer\n",
      "==================================================\n",
      "Test data size: 1802\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "vindjammer - Test Set MAE: 3.27 minutes\n",
      "vindjammer - Test Set RMSE: 4.04 minutes\n",
      "vindjammer - Test Set MSE: 16.31\n",
      "vindjammer - Test Set sMAPE: 23.10%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 85.2% (target: 95%)\n",
      "Average Interval Width: 11.57 minutes\n",
      "Interval Score: 24.27\n",
      "Test results saved for vindjammer\n",
      "\n",
      "Evaluating ride 24/30: madame freudenreich curiosits\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: madame freudenreich curiosits\n",
      "==================================================\n",
      "Test data size: 1832\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "madame freudenreich curiosits - Test Set MAE: 0.08 minutes\n",
      "madame freudenreich curiosits - Test Set RMSE: 0.64 minutes\n",
      "madame freudenreich curiosits - Test Set MSE: 0.41\n",
      "madame freudenreich curiosits - Test Set sMAPE: 99.28%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 98.5% (target: 95%)\n",
      "Average Interval Width: 2.47 minutes\n",
      "Interval Score: 4.58\n",
      "Test results saved for madame freudenreich curiosits\n",
      "\n",
      "Evaluating ride 25/30: atlantica supersplash\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: atlantica supersplash\n",
      "==================================================\n",
      "Test data size: 1826\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "atlantica supersplash - Test Set MAE: 38.14 minutes\n",
      "atlantica supersplash - Test Set RMSE: 44.54 minutes\n",
      "atlantica supersplash - Test Set MSE: 1983.96\n",
      "atlantica supersplash - Test Set sMAPE: 55.18%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 22.9% (target: 95%)\n",
      "Average Interval Width: 34.83 minutes\n",
      "Interval Score: 909.46\n",
      "Test results saved for atlantica supersplash\n",
      "\n",
      "Evaluating ride 26/30: arthur\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: arthur\n",
      "==================================================\n",
      "Test data size: 1832\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "arthur - Test Set MAE: 41.93 minutes\n",
      "arthur - Test Set RMSE: 48.16 minutes\n",
      "arthur - Test Set MSE: 2319.75\n",
      "arthur - Test Set sMAPE: 44.10%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 21.8% (target: 95%)\n",
      "Average Interval Width: 50.35 minutes\n",
      "Interval Score: 831.22\n",
      "Test results saved for arthur\n",
      "\n",
      "Evaluating ride 27/30: jim button  journey through morrowland\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: jim button  journey through morrowland\n",
      "==================================================\n",
      "Test data size: 1820\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "jim button  journey through morrowland - Test Set MAE: 6.96 minutes\n",
      "jim button  journey through morrowland - Test Set RMSE: 8.07 minutes\n",
      "jim button  journey through morrowland - Test Set MSE: 65.11\n",
      "jim button  journey through morrowland - Test Set sMAPE: 32.23%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 42.3% (target: 95%)\n",
      "Average Interval Width: 11.39 minutes\n",
      "Interval Score: 96.27\n",
      "Test results saved for jim button  journey through morrowland\n",
      "\n",
      "Evaluating ride 28/30: eurotower\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: eurotower\n",
      "==================================================\n",
      "Test data size: 1824\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "eurotower - Test Set MAE: 3.76 minutes\n",
      "eurotower - Test Set RMSE: 5.50 minutes\n",
      "eurotower - Test Set MSE: 30.30\n",
      "eurotower - Test Set sMAPE: 21.93%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 87.9% (target: 95%)\n",
      "Average Interval Width: 16.36 minutes\n",
      "Interval Score: 38.43\n",
      "Test results saved for eurotower\n",
      "\n",
      "Evaluating ride 29/30: arena of football  be part of it\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: arena of football  be part of it\n",
      "==================================================\n",
      "Test data size: 1816\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "arena of football  be part of it - Test Set MAE: 24.86 minutes\n",
      "arena of football  be part of it - Test Set RMSE: 28.00 minutes\n",
      "arena of football  be part of it - Test Set MSE: 783.90\n",
      "arena of football  be part of it - Test Set sMAPE: 66.32%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 7.3% (target: 95%)\n",
      "Average Interval Width: 10.23 minutes\n",
      "Interval Score: 800.66\n",
      "Test results saved for arena of football  be part of it\n",
      "\n",
      "Evaluating ride 30/30: alpine express enzian\n",
      "\n",
      "==================================================\n",
      "Evaluating ride: alpine express enzian\n",
      "==================================================\n",
      "Test data size: 1753\n",
      "Generating predictions on test set...\n",
      "Evaluating on test set...\n",
      "\n",
      "alpine express enzian - Test Set MAE: 7.29 minutes\n",
      "alpine express enzian - Test Set RMSE: 9.58 minutes\n",
      "alpine express enzian - Test Set MSE: 91.87\n",
      "alpine express enzian - Test Set sMAPE: 18.58%\n",
      "Confidence Interval Evaluation:\n",
      "Coverage Rate: 89.6% (target: 95%)\n",
      "Average Interval Width: 30.62 minutes\n",
      "Interval Score: 57.69\n",
      "Test results saved for alpine express enzian\n",
      "\n",
      "Completed 30/30 rides\n",
      "\n",
      "Generating test evaluation summary...\n",
      "\n",
      "================================================================================\n",
      "TEST SET EVALUATION SUMMARY:\n",
      "Total rides evaluated: 30\n",
      "\n",
      "Test Set Performance:\n",
      "  Average MAE: 11.38 ± 10.95\n",
      "  Average RMSE: 13.95 ± 12.54\n",
      "  Average sMAPE: 35.53% ± 18.32%\n",
      "\n",
      "Confidence Intervals:\n",
      "  Average Coverage Rate: 68.6% (target: 95%)\n",
      "  Average Interval Width: 22.78 minutes\n",
      "\n",
      "Validation vs Test Performance:\n",
      "  MAE difference (Test - Validation): 4.28\n",
      "  RMSE difference (Test - Validation): 4.79\n",
      "\n",
      "Summary saved to: ../models/prophet_enhanced/test_evaluation_summary.csv\n",
      "================================================================================\n",
      "\n",
      "Test evaluation completed successfully!\n",
      "Results saved to: ../models/prophet_enhanced/\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "data = load_data(\"../data/processed/ep/final_cleaned_processed_wait_times.parquet\")\n",
    "print(f\"Loaded data with {len(data)} rows\")\n",
    "\n",
    "check_for_missing_values(data)\n",
    "\n",
    "data = filter_to_operating_hours(data)\n",
    "\n",
    "# Define time periods for splitting\n",
    "train_years, val_year, test_year = list(range(2017, 2023)), 2023, 2024\n",
    "\n",
    "# Split the data\n",
    "train_data, val_data, test_data = split_data(data, train_years, val_year, test_year)\n",
    "\n",
    "# Set model directory\n",
    "model_dir = \"../models/prophet_enhanced/\"\n",
    "\n",
    "# Check if models exist\n",
    "processed_rides = get_processed_rides(model_dir)\n",
    "if not processed_rides:\n",
    "    print(f\"No saved models found in {model_dir}\")\n",
    "    print(\"Please run the training script first to create models.\")\n",
    "else:\n",
    "    print(f\"Found {len(processed_rides)} rides with saved models\")\n",
    "    \n",
    "    # Evaluate all rides on test set\n",
    "    print(\"\\nStarting test set evaluation...\")\n",
    "    test_results = evaluate_all_rides_on_test(test_data, model_dir)\n",
    "    \n",
    "    # Generate comprehensive summary report\n",
    "    print(\"\\nGenerating test evaluation summary...\")\n",
    "    test_summary_df = generate_test_summary_report(test_results, model_dir)\n",
    "    \n",
    "    print(\"\\nTest evaluation completed successfully!\")\n",
    "    print(f\"Results saved to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ce830",
   "metadata": {},
   "source": [
    "## Additional Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0846fe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 RIDES BY TEST PERFORMANCE:\n",
      "==================================================\n",
      "                        Ride Name  Test MAE  Test RMSE  Coverage Rate (%)\n",
      "    madame freudenreich curiosits  0.084556   0.638820          98.526201\n",
      "whale adventures  northern lights  1.428335   3.015351          86.626638\n",
      "                    kolumbusjolle  1.542395   2.125228          92.806150\n",
      "              castello dei medici  1.716850   2.361465          95.275591\n",
      "                     poppy towers  1.954676   2.466868          97.475302\n",
      "      old mac donalds tractor fun  2.170635   2.758021          96.369637\n",
      "                       vindjammer  3.265203   4.038041          85.183130\n",
      "     vienna wave swing  glckspilz  3.649488   4.327656          73.490670\n",
      "                        eurotower  3.755417   5.504243          87.883772\n",
      "                  tirol log flume  4.759194   8.151171          90.639269\n",
      "\n",
      "BOTTOM 10 RIDES BY TEST PERFORMANCE:\n",
      "==================================================\n",
      "                       Ride Name  Test MAE  Test RMSE  Coverage Rate (%)\n",
      "                     silver star 13.004992  16.250464          76.067908\n",
      "                         pegasus 14.411044  17.920708          48.443474\n",
      "         eurosat  cancan coaster 17.498808  20.706399          70.524017\n",
      "                 matterhornblitz 18.323091  21.844416          46.721311\n",
      "                         euromir 19.056448  23.307911          59.388646\n",
      "                      voletarium 22.697576  28.036073          13.100437\n",
      "arena of football  be part of it 24.863766  27.998276           7.268722\n",
      "           blue fire megacoaster 33.413568  39.156882          51.782776\n",
      "           atlantica supersplash 38.135650  44.541714          22.891566\n",
      "                          arthur 41.925481  48.163756          21.779476\n",
      "\n",
      "OVERALL STATISTICS:\n",
      "==================================================\n",
      "Number of rides evaluated: 30\n",
      "Test MAE - Mean: 11.38, Std: 11.14\n",
      "Test RMSE - Mean: 13.95, Std: 12.75\n",
      "Test sMAPE - Mean: 35.5341, Std: 18.6344\n",
      "Coverage Rate - Mean: 68.6%, Target: 95%\n"
     ]
    }
   ],
   "source": [
    "def analyze_test_results(model_dir=\"../models/prophet_enhanced/\"):\n",
    "    # Load the test summary results\n",
    "    summary_path = os.path.join(model_dir, \"test_evaluation_summary.csv\")\n",
    "    if not os.path.exists(summary_path):\n",
    "        print(\"Test evaluation summary not found. Run the test evaluation first.\")\n",
    "        return None\n",
    "    \n",
    "    summary_df = pd.read_csv(summary_path)\n",
    "    \n",
    "    # Display top performing rides\n",
    "    print(\"TOP 10 RIDES BY TEST PERFORMANCE:\")\n",
    "    print(\"=\"*50)\n",
    "    top_10 = summary_df.head(10)[['Ride Name', 'Test MAE', 'Test RMSE', 'Coverage Rate (%)']]\n",
    "    print(top_10.to_string(index=False))\n",
    "    \n",
    "    # Display bottom performing rides\n",
    "    print(\"\\nBOTTOM 10 RIDES BY TEST PERFORMANCE:\")\n",
    "    print(\"=\"*50)\n",
    "    bottom_10 = summary_df.tail(10)[['Ride Name', 'Test MAE', 'Test RMSE', 'Coverage Rate (%)']]\n",
    "    print(bottom_10.to_string(index=False))\n",
    "    \n",
    "    # Performance statistics\n",
    "    print(f\"\\nOVERALL STATISTICS:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Number of rides evaluated: {len(summary_df)}\")\n",
    "    print(f\"Test MAE - Mean: {summary_df['Test MAE'].mean():.2f}, Std: {summary_df['Test MAE'].std():.2f}\")\n",
    "    print(f\"Test RMSE - Mean: {summary_df['Test RMSE'].mean():.2f}, Std: {summary_df['Test RMSE'].std():.2f}\")\n",
    "    print(f\"Test sMAPE - Mean: {summary_df['Test sMAPE'].mean():.4f}, Std: {summary_df['Test sMAPE'].std():.4f}\")\n",
    "    print(f\"Coverage Rate - Mean: {summary_df['Coverage Rate (%)'].mean():.1f}%, Target: 95%\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Run analysis if results exist\n",
    "if os.path.exists(\"../models/prophet_enhanced/test_evaluation_summary.csv\"):\n",
    "    test_analysis_df = analyze_test_results()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

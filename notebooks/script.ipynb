{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febcdb34",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from prophet import Prophet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e60c74",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dddfb2e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load data from parquet file.\"\"\"\n",
    "    data = pd.read_parquet(file_path)\n",
    "    data = data.dropna(subset=['rain', 'wind'])\n",
    "    return data\n",
    "\n",
    "def split_data(data, train_years, val_year, test_year):\n",
    "    \"\"\"Split data into train, validation and test sets based on years.\"\"\"\n",
    "    data['time_bucket'] = pd.to_datetime(data['time_bucket'])\n",
    "    \n",
    "    train_data = data[data['time_bucket'].dt.year.isin(train_years)]\n",
    "    val_data = data[data['time_bucket'].dt.year == val_year]\n",
    "    test_data = data[data['time_bucket'].dt.year == test_year]\n",
    "    \n",
    "    print(f\"Train data size: {len(train_data)}\")\n",
    "    print(f\"Validation data size: {len(val_data)}\")\n",
    "    print(f\"Test data size: {len(test_data)}\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def filter_ride_data(data, ride_name):\n",
    "    \"\"\"Filter data for a specific ride.\"\"\"\n",
    "    return data[data[f'ride_name_{ride_name}'] == True].copy()\n",
    "\n",
    "def get_all_rides(data):\n",
    "    \"\"\"Extract all unique rides from the dataset.\"\"\"\n",
    "    ride_columns = [col for col in data.columns if col.startswith('ride_name_')]\n",
    "    return [col.replace('ride_name_', '') for col in ride_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = load_data(\"../data/processed/ep/final_cleaned_processed_wait_times.parquet\")\n",
    "print(f\"Loaded data with {len(data)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time periods for splitting\n",
    "train_years, val_year, test_year = list(range(2017, 2023)), 2023, 2024\n",
    "\n",
    "# Split the data\n",
    "train_data, val_data, test_data = split_data(data, train_years, val_year, test_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364d54c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Get all rides in the dataset\n",
    "all_rides = get_all_rides(data)\n",
    "print(f\"Found {len(all_rides)} rides in the dataset:\")\n",
    "for i, ride in enumerate(all_rides):\n",
    "    print(f\"{i+1}. {ride}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6544c0",
   "metadata": {},
   "source": [
    "## Time Series Decomposition with Prophet (Modified to exclude holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6c037",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BaseTimeSeriesModel:\n",
    "    \"\"\"Class to handle basic Prophet time series modeling without holidays.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.forecast = None\n",
    "        \n",
    "    def is_operating_month(self, ds):\n",
    "        month = ds.month\n",
    "        return 1 if (month >= 4 and month <= 12) else 0\n",
    "\n",
    "    def prepare_prophet_dataframe(self, data):\n",
    "        \"\"\"Prepare data for Prophet.\"\"\"\n",
    "        prophet_df = data[['time_bucket', 'wait_time']].copy()\n",
    "        prophet_df = prophet_df.rename(columns={'time_bucket': 'ds', 'wait_time': 'y'})\n",
    "        prophet_df['operating_month'] = prophet_df['ds'].apply(self.is_operating_month)\n",
    "        return prophet_df\n",
    "    \n",
    "    def fit(self, prophet_df):\n",
    "        \"\"\"Fit the Prophet model without using holidays.\"\"\"\n",
    "        # Create a Prophet model without holidays\n",
    "        self.model = Prophet(\n",
    "            yearly_seasonality=False,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=True,\n",
    "            interval_width=0.95\n",
    "        )\n",
    "        \n",
    "        # Define custom seasonality for operating months (April-December)\n",
    "        self.model.add_seasonality(\n",
    "            name='operating_season', \n",
    "            period=274,  # 9 months period\n",
    "            fourier_order=9,\n",
    "            condition_name='operating_month'\n",
    "        )\n",
    "        \n",
    "        # Add COVID period as regressors instead of holidays\n",
    "        prophet_df['during_covid_era'] = 0\n",
    "        covid_period = (prophet_df['ds'] >= '2020-04-15') & (prophet_df['ds'] <= '2020-05-20')\n",
    "        prophet_df.loc[covid_period, 'during_covid_era'] = 1\n",
    "        self.model.add_regressor('during_covid_era')\n",
    "        \n",
    "        prophet_df['covid_recovery'] = 0\n",
    "        recovery_period = (prophet_df['ds'] >= '2021-05-21') & (prophet_df['ds'] <= '2021-08-31')\n",
    "        prophet_df.loc[recovery_period, 'covid_recovery'] = 1\n",
    "        self.model.add_regressor('covid_recovery')\n",
    "\n",
    "        self.model.fit(prophet_df)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, future_df):\n",
    "        \"\"\"Generate predictions with the fitted model.\"\"\"\n",
    "        # Add required columns for prediction\n",
    "        future_df = future_df.copy()\n",
    "        future_df['operating_month'] = future_df['ds'].apply(self.is_operating_month)\n",
    "        \n",
    "        # Add COVID regressors if they're not already present\n",
    "        if 'during_covid_era' not in future_df.columns:\n",
    "            future_df['during_covid_era'] = 0\n",
    "            covid_period = (future_df['ds'] >= '2020-04-15') & (future_df['ds'] <= '2020-05-20')\n",
    "            future_df.loc[covid_period, 'during_covid_era'] = 1\n",
    "            \n",
    "        if 'covid_recovery' not in future_df.columns:\n",
    "            future_df['covid_recovery'] = 0\n",
    "            recovery_period = (future_df['ds'] >= '2021-05-21') & (future_df['ds'] <= '2021-08-31')\n",
    "            future_df.loc[recovery_period, 'covid_recovery'] = 1\n",
    "        \n",
    "        self.forecast = self.model.predict(future_df)\n",
    "        return self.forecast\n",
    "    \n",
    "    def merge_predictions(self, original_data, forecast_data):\n",
    "        \"\"\"Merge original data with forecasts and calculate residuals.\"\"\"\n",
    "        result = original_data.copy()\n",
    "        \n",
    "        # Identify forecast columns to keep (exclude holiday components since we're not using them)\n",
    "        forecast_columns = ['ds', 'trend', 'operating_season', 'weekly', 'daily', 'yhat']\n",
    "        available_columns = [col for col in forecast_columns if col in forecast_data.columns]\n",
    "        \n",
    "        result = pd.merge(\n",
    "            result, \n",
    "            forecast_data[available_columns], \n",
    "            left_on='time_bucket', \n",
    "            right_on='ds', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        result['residual'] = result['wait_time'] - result['yhat']\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed091e6",
   "metadata": {},
   "source": [
    "## Feature Engineering (With holidays for residual model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e850a1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"Class to handle feature engineering for the wait time prediction model.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_features(ride_data_df, forecast_df):\n",
    "        \"\"\"\n",
    "        Prepare features for the neural network model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ride_data_df : pandas.DataFrame\n",
    "            DataFrame containing ride data\n",
    "        forecast_df : pandas.DataFrame\n",
    "            DataFrame containing Prophet forecasts\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Array of features for the neural network\n",
    "        \"\"\"\n",
    "        # Extract weather features\n",
    "        weather_features = ride_data_df[['temperature', 'rain']].values\n",
    "        \n",
    "        # Extract holiday features (used only in the residual model)\n",
    "        holiday_features = ride_data_df[\n",
    "            ['is_german_holiday', 'is_swiss_holiday', 'is_french_holiday']\n",
    "        ].astype(float).values\n",
    "\n",
    "        # Extract time features\n",
    "        timestamps = pd.to_datetime(ride_data_df['time_bucket'])\n",
    "        time_features = FeatureEngineer._create_cyclical_time_features(timestamps)\n",
    "        \n",
    "        # Add prophet components as features\n",
    "        prophet_features = []\n",
    "        for component in ['trend', 'operating_season', 'weekly', 'daily']:\n",
    "            if component in forecast_df.columns:\n",
    "                # Merge the component from forecast_df to ride_data_df based on time_bucket/ds\n",
    "                component_values = pd.merge(\n",
    "                    ride_data_df[['time_bucket']], \n",
    "                    forecast_df[['ds', component]], \n",
    "                    left_on='time_bucket', \n",
    "                    right_on='ds', \n",
    "                    how='left'\n",
    "                )[component].values.reshape(-1, 1)\n",
    "                prophet_features.append(component_values)\n",
    "        \n",
    "        if prophet_features:\n",
    "            prophet_features = np.hstack(prophet_features)\n",
    "        else:\n",
    "            # If no prophet features are available, create a placeholder column of zeros\n",
    "            prophet_features = np.zeros((len(ride_data_df), 1))\n",
    "\n",
    "        # Combine all features\n",
    "        features = np.hstack([weather_features, holiday_features, time_features, prophet_features])\n",
    "        return features\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_cyclical_time_features(timestamps):\n",
    "        \"\"\"\n",
    "        Create cyclical time features from timestamps.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        timestamps : pandas.Series\n",
    "            Series of timestamps\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Array of cyclical time features\n",
    "        \"\"\"\n",
    "        # Extract time components\n",
    "        hour = timestamps.dt.hour\n",
    "        day_of_week = timestamps.dt.dayofweek\n",
    "        month = timestamps.dt.month\n",
    "        day_of_year = timestamps.dt.dayofyear\n",
    "\n",
    "        # Create cyclical features\n",
    "        hour_sin = np.sin(2 * np.pi * hour / 24)\n",
    "        hour_cos = np.cos(2 * np.pi * hour / 24)\n",
    "        dow_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
    "        dow_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
    "        month_sin = np.sin(2 * np.pi * month / 12)\n",
    "        month_cos = np.cos(2 * np.pi * month / 12)\n",
    "        doy_sin = np.sin(2 * np.pi * day_of_year / 365.25)\n",
    "        doy_cos = np.cos(2 * np.pi * day_of_year / 365.25)\n",
    "\n",
    "        # Stack all cyclical features\n",
    "        cyclical_features = np.column_stack([\n",
    "            hour_sin, hour_cos, \n",
    "            dow_sin, dow_cos,\n",
    "            month_sin, month_cos,\n",
    "            doy_sin, doy_cos\n",
    "        ])\n",
    "        \n",
    "        return cyclical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5186a41",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832feed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RideDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for ride wait time data.\"\"\"\n",
    "    \n",
    "    def __init__(self, features, targets, scaler=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features : numpy.ndarray\n",
    "            Array of features\n",
    "        targets : numpy.ndarray\n",
    "            Array of target values\n",
    "        scaler : StandardScaler, optional\n",
    "            Scaler for features\n",
    "        \"\"\"\n",
    "        if scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.features = self.scaler.fit_transform(features)\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "            self.features = self.scaler.transform(features)\n",
    "            \n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).reshape(-1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a sample from the dataset.\"\"\"\n",
    "        # Handle NaN values\n",
    "        features = np.nan_to_num(self.features[idx])\n",
    "        targets = np.nan_to_num(self.targets[idx])\n",
    "\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1f857",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ResidualPredictor(nn.Module):\n",
    "    \"\"\"Neural network model for predicting residuals.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[64, 32], dropout_prob=0.2):\n",
    "        super(ResidualPredictor, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        layers.append(nn.BatchNorm1d(hidden_dims[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout_prob))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "            \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c93e33",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Class to handle model training and evaluation.\"\"\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def train_nn_model(train_features, train_residuals, val_features, val_residuals, num_epochs=50,\n",
    "                      hidden_dims=[128, 64], dropout_prob=0.2, \n",
    "                      batch_size=512, patience=10, device='cpu'):\n",
    "        \"\"\"\n",
    "        Train the neural network model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        train_features : numpy.ndarray\n",
    "            Training features\n",
    "        train_residuals : numpy.ndarray\n",
    "            Training targets (residuals)\n",
    "        val_features : numpy.ndarray\n",
    "            Validation features\n",
    "        val_residuals : numpy.ndarray\n",
    "            Validation targets (residuals)\n",
    "        num_epochs : int\n",
    "            Number of training epochs\n",
    "        hidden_dims : list\n",
    "            List of hidden layer dimensions\n",
    "        dropout_prob : float\n",
    "            Dropout probability\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        patience : int\n",
    "            Number of epochs to wait for improvement before early stopping\n",
    "        device : str\n",
    "            Device to use for training ('cpu' or 'cuda')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            Trained neural network model and feature scaler\n",
    "        \"\"\"\n",
    "        # Create datasets and dataloaders\n",
    "        train_dataset = RideDataset(train_features, train_residuals)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        feature_scaler = train_dataset.scaler\n",
    "\n",
    "        val_dataset = RideDataset(val_features, val_residuals, scaler=feature_scaler)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize model\n",
    "        input_dim = train_features.shape[1]\n",
    "        model = ResidualPredictor(input_dim, hidden_dims=hidden_dims, dropout_prob=dropout_prob)\n",
    "        model.to(device)\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "        # Training loop parameters\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        # Training loop with tqdm progress bar\n",
    "        pbar = tqdm(range(num_epochs), desc=\"Training\")\n",
    "        for epoch in pbar:\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for batch_features, batch_residuals in train_dataloader:\n",
    "                batch_features = batch_features.to(device)\n",
    "                batch_residuals = batch_residuals.to(device)\n",
    "\n",
    "                # Forward and backward passes\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_residuals)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            train_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "            # Validation phase\n",
    "            val_loss = float('inf')\n",
    "            if val_dataloader:\n",
    "                model.eval()\n",
    "                val_running_loss = 0.0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for val_features_batch, val_residuals_batch in val_dataloader:\n",
    "                        val_features_batch = val_features_batch.to(device)\n",
    "                        val_residuals_batch = val_residuals_batch.to(device)\n",
    "\n",
    "                        val_outputs = model(val_features_batch)\n",
    "                        val_loss_batch = criterion(val_outputs, val_residuals_batch)\n",
    "                        val_running_loss += val_loss_batch.item()\n",
    "                \n",
    "                val_loss = val_running_loss / len(val_dataloader)\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "                # Early stopping logic\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    epochs_no_improve = 0\n",
    "                    best_model_state = model.state_dict().copy()\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve >= patience:\n",
    "                        pbar.set_description(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        model.load_state_dict(best_model_state)\n",
    "                        break\n",
    "\n",
    "            # Update tqdm postfix with loss values\n",
    "            pbar.set_postfix({'train_loss': f'{train_loss:.4f}', 'val_loss': f'{val_loss:.4f}'})\n",
    "        \n",
    "        # Load best model state if available\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "        \n",
    "        return model, feature_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3ee3d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class WaitTimePredictor:\n",
    "    \"\"\"Class to combine Prophet and Neural Network predictions.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_predict(ride_df, prophet_model, nn_model, feature_scaler, \n",
    "                    feature_engineer, device='cpu'):\n",
    "        \"\"\"\n",
    "        Efficient batch prediction for wait times.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ride_df : pandas.DataFrame\n",
    "            DataFrame containing ride data\n",
    "        prophet_model : Prophet\n",
    "            Trained Prophet model\n",
    "        nn_model : ResidualPredictor\n",
    "            Trained neural network model\n",
    "        feature_scaler : StandardScaler\n",
    "            Feature scaler used during training\n",
    "        feature_engineer : FeatureEngineer\n",
    "            Feature engineering class\n",
    "        device : str\n",
    "            Device to use for prediction ('cpu' or 'cuda')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Array of predicted wait times\n",
    "        \"\"\"\n",
    "        # Get Prophet forecasts for all timestamps\n",
    "        timestamps_df = pd.DataFrame({'ds': ride_df['time_bucket'].unique()})\n",
    "        timestamps_df['operating_month'] = timestamps_df['ds'].apply(\n",
    "            lambda x: 1 if (x.month >= 4 and x.month <= 12) else 0\n",
    "        )\n",
    "        \n",
    "        # Add COVID regressors\n",
    "        timestamps_df['during_covid_era'] = 0\n",
    "        covid_period = (timestamps_df['ds'] >= '2020-04-15') & (timestamps_df['ds'] <= '2020-05-20')\n",
    "        timestamps_df.loc[covid_period, 'during_covid_era'] = 1\n",
    "        \n",
    "        timestamps_df['covid_recovery'] = 0\n",
    "        recovery_period = (timestamps_df['ds'] >= '2021-05-21') & (timestamps_df['ds'] <= '2021-08-31')\n",
    "        timestamps_df.loc[recovery_period, 'covid_recovery'] = 1\n",
    "        \n",
    "        # Get Prophet forecasts\n",
    "        prophet_forecasts = prophet_model.predict(timestamps_df)\n",
    "        \n",
    "        # Merge prophet forecasts with the original data\n",
    "        ride_data_with_forecast = pd.merge(\n",
    "            ride_df,\n",
    "            prophet_forecasts[['ds', 'yhat', 'trend', 'operating_season', 'weekly', 'daily']],\n",
    "            left_on='time_bucket', \n",
    "            right_on='ds',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Generate features for neural network\n",
    "        features = feature_engineer.prepare_features(ride_data_with_forecast, prophet_forecasts)\n",
    "        \n",
    "        # Scale features\n",
    "        scaled_features = feature_scaler.transform(features)\n",
    "        features_tensor = torch.tensor(scaled_features, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Get residual predictions from neural network\n",
    "        nn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            residuals = nn_model(features_tensor).cpu().numpy().flatten()\n",
    "        \n",
    "        # Combine predictions\n",
    "        final_predictions = ride_data_with_forecast['yhat'].values + residuals\n",
    "        \n",
    "        # Ensure non-negative wait times\n",
    "        final_predictions = np.maximum(0, final_predictions)\n",
    "        \n",
    "        return final_predictions, ride_data_with_forecast['yhat'].values, residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf1a6bc",
   "metadata": {},
   "source": [
    "## Model Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6425d8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(ride_df, actual_values, predictions, baseline_predictions, title=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate model performance and generate visualizations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ride_df : pandas.DataFrame\n",
    "        DataFrame containing ride data\n",
    "    actual_values : numpy.ndarray\n",
    "        Array of actual wait times\n",
    "    predictions : numpy.ndarray\n",
    "        Array of predicted wait times\n",
    "    baseline_predictions : numpy.ndarray\n",
    "        Array of baseline (Prophet only) predictions\n",
    "    title : str\n",
    "        Title for the plots\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    mae = np.mean(np.abs(predictions - actual_values))\n",
    "    rmse = np.sqrt(np.mean(np.square(predictions - actual_values)))\n",
    "    baseline_mae = np.mean(np.abs(baseline_predictions - actual_values))\n",
    "    baseline_rmse = np.sqrt(np.mean(np.square(baseline_predictions - actual_values)))\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\n{title} MAE: {mae:.2f} minutes (Baseline: {baseline_mae:.2f})\")\n",
    "    print(f\"{title} RMSE: {rmse:.2f} minutes (Baseline: {baseline_rmse:.2f})\")\n",
    "    \n",
    "    # Create a DataFrame with results for time-based analysis\n",
    "    results_df = pd.DataFrame({\n",
    "        'time_bucket': ride_df['time_bucket'].values,\n",
    "        'actual': actual_values,\n",
    "        'predicted': predictions,\n",
    "        'baseline': baseline_predictions,\n",
    "    })\n",
    "    \n",
    "    # Add time components\n",
    "    results_df['hour'] = results_df['time_bucket'].dt.hour\n",
    "    results_df['day_of_week'] = results_df['time_bucket'].dt.dayofweek\n",
    "    results_df['month'] = results_df['time_bucket'].dt.month\n",
    "    \n",
    "    # Calculate errors\n",
    "    results_df['error'] = results_df['predicted'] - results_df['actual']\n",
    "    results_df['abs_error'] = np.abs(results_df['error'])\n",
    "    results_df['baseline_error'] = results_df['baseline'] - results_df['actual']\n",
    "    results_df['baseline_abs_error'] = np.abs(results_df['baseline_error'])\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 16))\n",
    "    \n",
    "    # Actual vs Predicted scatter plot\n",
    "    axes[0].scatter(actual_values, predictions, alpha=0.5, label='Combined Model')\n",
    "    axes[0].scatter(actual_values, baseline_predictions, alpha=0.3, color='red', label='Prophet Only')\n",
    "    max_val = max(np.max(actual_values), np.max(predictions), np.max(baseline_predictions))\n",
    "    axes[0].plot([0, max_val], [0, max_val], 'k--')\n",
    "    axes[0].set_xlabel('Actual Wait Time (minutes)')\n",
    "    axes[0].set_ylabel('Predicted Wait Time (minutes)')\n",
    "    axes[0].set_title(f'{title} - Actual vs Predicted')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Hourly analysis\n",
    "    hourly_errors = results_df.groupby('hour')[['abs_error', 'baseline_abs_error']].mean()\n",
    "    hourly_errors = hourly_errors.rename(columns={\n",
    "        'abs_error': 'Combined Model MAE', \n",
    "        'baseline_abs_error': 'Prophet Only MAE'\n",
    "    })\n",
    "    hourly_errors.plot(kind='bar', ax=axes[1])\n",
    "    axes[1].set_xlabel('Hour of Day')\n",
    "    axes[1].set_ylabel('Mean Absolute Error (minutes)')\n",
    "    axes[1].set_title(f'{title} - Error Analysis by Hour of Day')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "        \"baseline_mae\": baseline_mae,\n",
    "        \"baseline_rmse\": baseline_rmse,\n",
    "        \"improvement_percentage\": ((baseline_mae - mae) / baseline_mae) * 100\n",
    "    }\n",
    "    \n",
    "    return metrics, results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668d0ed",
   "metadata": {},
   "source": [
    "## Model Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71681f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_model(ride_name, prophet_model, nn_model, feature_scaler, metrics, output_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Save trained models and results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ride_name : str\n",
    "        Name of the ride\n",
    "    prophet_model : Prophet\n",
    "        Trained Prophet model\n",
    "    nn_model : ResidualPredictor\n",
    "        Trained neural network model\n",
    "    feature_scaler : StandardScaler\n",
    "        Feature scaler used during training\n",
    "    metrics : dict\n",
    "        Dictionary of evaluation metrics\n",
    "    output_dir : str\n",
    "        Directory to save models and results\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create ride-specific directory\n",
    "    ride_dir = os.path.join(output_dir, ride_name.replace(\" \", \"_\"))\n",
    "    os.makedirs(ride_dir, exist_ok=True)\n",
    "    \n",
    "    # Save Prophet model (using pickle)\n",
    "    with open(os.path.join(ride_dir, \"prophet_model.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(prophet_model.model, f)\n",
    "    \n",
    "    # Save neural network model\n",
    "    torch.save(nn_model.state_dict(), os.path.join(ride_dir, \"nn_model.pt\"))\n",
    "    \n",
    "    # Save feature scaler\n",
    "    with open(os.path.join(ride_dir, \"feature_scaler.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(feature_scaler, f)\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(os.path.join(ride_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    print(f\"Models and results saved to {ride_dir}\")\n",
    "\n",
    "def load_model(ride_name, output_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Load trained models and results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ride_name : str\n",
    "        Name of the ride\n",
    "    output_dir : str\n",
    "        Directory to load models and results from\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (prophet_model, nn_model, feature_scaler, metrics)\n",
    "    \"\"\"\n",
    "    # Create ride-specific directory path\n",
    "    ride_dir = os.path.join(output_dir, ride_name.replace(\" \", \"_\"))\n",
    "    \n",
    "    # Check if models exist\n",
    "    if not os.path.exists(ride_dir):\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Load Prophet model\n",
    "    with open(os.path.join(ride_dir, \"prophet_model.pkl\"), \"rb\") as f:\n",
    "        prophet_model = pickle.load(f)\n",
    "    \n",
    "    # Initialize BaseTimeSeriesModel and set the loaded model\n",
    "    prophet_ts = BaseTimeSeriesModel()\n",
    "    prophet_ts.model = prophet_model\n",
    "    \n",
    "    # Load feature scaler\n",
    "    with open(os.path.join(ride_dir, \"feature_scaler.pkl\"), \"rb\") as f:\n",
    "        feature_scaler = pickle.load(f)\n",
    "    \n",
    "    # Load metrics\n",
    "    with open(os.path.join(ride_dir, \"metrics.json\"), \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    # Load neural network model\n",
    "    # First determine input dimension from feature scaler\n",
    "    input_dim = feature_scaler.n_features_in_\n",
    "    nn_model = ResidualPredictor(input_dim, hidden_dims=[128, 64, 32], dropout_prob=0.2)\n",
    "    nn_model.load_state_dict(torch.load(os.path.join(ride_dir, \"nn_model.pt\")))\n",
    "    nn_model.eval()\n",
    "    \n",
    "    return prophet_ts, nn_model, feature_scaler, metrics\n",
    "\n",
    "def get_processed_rides(output_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Get a list of rides that have already been processed.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    output_dir : str\n",
    "        Directory to check for processed rides\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of processed ride names\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        return []\n",
    "    \n",
    "    # Get all subdirectories in the output directory\n",
    "    processed_rides = [d for d in os.listdir(output_dir) \n",
    "                      if os.path.isdir(os.path.join(output_dir, d))]\n",
    "    \n",
    "    # Convert directory names back to ride names\n",
    "    processed_rides = [ride.replace(\"_\", \" \") for ride in processed_rides]\n",
    "    \n",
    "    return processed_rides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ee741",
   "metadata": {},
   "source": [
    "## Multi-Ride Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a34d7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_checkpoint_file(processed_rides, output_dir=\"models\"):\n",
    "    \"\"\"Create a checkpoint file with the list of processed rides.\"\"\"\n",
    "    checkpoint_path = os.path.join(output_dir, \"checkpoint.json\")\n",
    "    with open(checkpoint_path, \"w\") as f:\n",
    "        json.dump({\"processed_rides\": processed_rides}, f, indent=4)\n",
    "\n",
    "def load_checkpoint_file(output_dir=\"models\"):\n",
    "    \"\"\"Load the checkpoint file to get the list of processed rides.\"\"\"\n",
    "    checkpoint_path = os.path.join(output_dir, \"checkpoint.json\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, \"r\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "        return checkpoint.get(\"processed_rides\", [])\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96424a7",
   "metadata": {},
   "source": [
    "## Training Pipeline for All Rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ac4db",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_single_ride(ride_name, train_data, val_data, test_data, device='cpu', output_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Process a single ride: train models, evaluate, and save results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ride_name : str\n",
    "        Name of the ride to process\n",
    "    train_data : pandas.DataFrame\n",
    "        Training data\n",
    "    val_data : pandas.DataFrame\n",
    "        Validation data\n",
    "    test_data : pandas.DataFrame\n",
    "        Test data\n",
    "    device : str\n",
    "        Device to use for training ('cpu' or 'cuda')\n",
    "    output_dir : str\n",
    "        Directory to save models and results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of validation and test metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing ride: {ride_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Filter data for the current ride\n",
    "    ride_train_data = filter_ride_data(train_data, ride_name)\n",
    "    ride_val_data = filter_ride_data(val_data, ride_name)\n",
    "    ride_test_data = filter_ride_data(test_data, ride_name)\n",
    "    \n",
    "    print(f\"Training data size: {len(ride_train_data)}\")\n",
    "    print(f\"Validation data size: {len(ride_val_data)}\")\n",
    "    print(f\"Test data size: {len(ride_test_data)}\")\n",
    "    \n",
    "    # Skip if not enough data\n",
    "    if len(ride_train_data) < 100 or len(ride_val_data) < 50 or len(ride_test_data) < 50:\n",
    "        print(f\"Skipping {ride_name} due to insufficient data\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize feature engineer\n",
    "    feature_engineer = FeatureEngineer()\n",
    "    \n",
    "    # Train Prophet model\n",
    "    print(\"Training Prophet model...\")\n",
    "    prophet_ts = BaseTimeSeriesModel()\n",
    "    prophet_df = prophet_ts.prepare_prophet_dataframe(ride_train_data)\n",
    "    prophet_model = prophet_ts.fit(prophet_df)\n",
    "    \n",
    "    # Generate Prophet forecasts for training data\n",
    "    future_train = pd.DataFrame({'ds': ride_train_data['time_bucket'].unique()})\n",
    "    train_forecast = prophet_ts.predict(future_train)\n",
    "    \n",
    "    # Merge predictions with original data\n",
    "    result = prophet_ts.merge_predictions(ride_train_data, train_forecast)\n",
    "    \n",
    "    # Prepare features for neural network\n",
    "    train_features = feature_engineer.prepare_features(ride_train_data, train_forecast)\n",
    "    train_residuals = result['residual'].values\n",
    "    \n",
    "    # Prepare validation data\n",
    "    val_dates = pd.DataFrame({'ds': ride_val_data['time_bucket'].unique()})\n",
    "    val_dates['operating_month'] = val_dates['ds'].apply(\n",
    "        lambda x: 1 if (x.month >= 4 and x.month <= 12) else 0\n",
    "    )\n",
    "    val_dates['during_covid_era'] = 0\n",
    "    val_dates['covid_recovery'] = 0\n",
    "    \n",
    "    val_forecast = prophet_ts.predict(val_dates)\n",
    "    \n",
    "    ride_val_data_with_forecast = pd.merge(\n",
    "        ride_val_data,\n",
    "        val_forecast[['ds', 'yhat', 'trend', 'operating_season', 'weekly', 'daily']],\n",
    "        left_on='time_bucket', \n",
    "        right_on='ds',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    ride_val_data_with_forecast['residual'] = (\n",
    "        ride_val_data_with_forecast['wait_time'] - ride_val_data_with_forecast['yhat']\n",
    "    )\n",
    "    \n",
    "    val_features = feature_engineer.prepare_features(ride_val_data_with_forecast, val_forecast)\n",
    "    val_residuals = ride_val_data_with_forecast['residual'].values\n",
    "    \n",
    "    # Train neural network\n",
    "    print(\"Training neural network...\")\n",
    "    nn_model, feature_scaler = ModelTrainer.train_nn_model(\n",
    "        train_features, train_residuals, \n",
    "        val_features, val_residuals,\n",
    "        num_epochs=50,  \n",
    "        hidden_dims=[128, 64, 32],  \n",
    "        dropout_prob=0.2,  \n",
    "        batch_size=512,\n",
    "        patience=10,  \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"Evaluating on validation set...\")\n",
    "    val_predictions, val_baseline_predictions, _ = WaitTimePredictor.batch_predict(\n",
    "        ride_val_data, \n",
    "        prophet_model, \n",
    "        nn_model, \n",
    "        feature_scaler,\n",
    "        feature_engineer,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    val_actuals = ride_val_data['wait_time'].values\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_predictions, test_baseline_predictions, _ = WaitTimePredictor.batch_predict(\n",
    "        ride_test_data, \n",
    "        prophet_model, \n",
    "        nn_model, \n",
    "        feature_scaler,\n",
    "        feature_engineer,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    test_actuals = ride_test_data['wait_time'].values\n",
    "    \n",
    "    # Evaluate and visualize results\n",
    "    val_metrics, val_results_df = evaluate_model(\n",
    "        ride_val_data, val_actuals, val_predictions, val_baseline_predictions, \n",
    "        title=f\"{ride_name} - Validation\"\n",
    "    )\n",
    "    \n",
    "    test_metrics, test_results_df = evaluate_model(\n",
    "        ride_test_data, test_actuals, test_predictions, test_baseline_predictions, \n",
    "        title=f\"{ride_name} - Test\"\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    combined_metrics = {\n",
    "        \"validation\": val_metrics,\n",
    "        \"test\": test_metrics,\n",
    "        \"data_counts\": {\n",
    "            \"train\": len(ride_train_data),\n",
    "            \"validation\": len(ride_val_data),\n",
    "            \"test\": len(ride_test_data)\n",
    "        },\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    # Save models and results\n",
    "    save_model(ride_name, prophet_ts, nn_model, feature_scaler, combined_metrics, output_dir)\n",
    "    \n",
    "    # Save detailed results as CSV\n",
    "    results_dir = os.path.join(output_dir, ride_name.replace(\" \", \"_\"), \"results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    val_results_df.to_csv(os.path.join(results_dir, \"validation_results.csv\"), index=False)\n",
    "    test_results_df.to_csv(os.path.join(results_dir, \"test_results.csv\"), index=False)\n",
    "    \n",
    "    return combined_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ecf37b",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ddf70",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_all_rides(all_rides, train_data, val_data, test_data, \n",
    "                      output_dir=\"models\", device='cpu', resume=True):\n",
    "    \"\"\"\n",
    "    Process all rides, with support for resuming from checkpoints.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_rides : list\n",
    "        List of all ride names to process\n",
    "    train_data : pandas.DataFrame\n",
    "        Training data\n",
    "    val_data : pandas.DataFrame\n",
    "        Validation data\n",
    "    test_data : pandas.DataFrame\n",
    "        Test data\n",
    "    output_dir : str\n",
    "        Directory to save models and results\n",
    "    device : str\n",
    "        Device to use for training ('cpu' or 'cuda')\n",
    "    resume : bool\n",
    "        Whether to resume from a previous run\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get list of already processed rides\n",
    "    processed_rides = []\n",
    "    if resume:\n",
    "        processed_rides = load_checkpoint_file(output_dir)\n",
    "        if processed_rides:\n",
    "            print(f\"Resuming from checkpoint. {len(processed_rides)} rides already processed.\")\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    all_results = {}\n",
    "    \n",
    "    # Process each ride\n",
    "    for i, ride_name in enumerate(all_rides):\n",
    "        if ride_name in processed_rides:\n",
    "            print(f\"Skipping {ride_name} (already processed)\")\n",
    "            # Load metrics for the summary\n",
    "            _, _, _, metrics = load_model(ride_name, output_dir)\n",
    "            if metrics:\n",
    "                all_results[ride_name] = metrics\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(f\"\\nProcessing ride {i+1}/{len(all_rides)}: {ride_name}\")\n",
    "            ride_metrics = process_single_ride(ride_name, train_data, val_data, test_data, \n",
    "                                             device=device, output_dir=output_dir)\n",
    "            \n",
    "            if ride_metrics:\n",
    "                all_results[ride_name] = ride_metrics\n",
    "                processed_rides.append(ride_name)\n",
    "                \n",
    "                # Update checkpoint after each ride\n",
    "                create_checkpoint_file(processed_rides, output_dir)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ride_name}: {str(e)}\")\n",
    "            \n",
    "    # Generate summary report\n",
    "    generate_summary_report(all_results, output_dir)\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d0d09",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_summary_report(all_results, output_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Generate a summary report of all ride models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_results : dict\n",
    "        Dictionary of results for all rides\n",
    "    output_dir : str\n",
    "        Directory to save the summary report\n",
    "    \"\"\"\n",
    "    # Create lists to store summary data\n",
    "    rides = []\n",
    "    val_mae = []\n",
    "    val_rmse = []\n",
    "    val_baseline_mae = []\n",
    "    val_improvement = []\n",
    "    test_mae = []\n",
    "    test_rmse = []\n",
    "    test_baseline_mae = []\n",
    "    test_improvement = []\n",
    "    data_counts = []\n",
    "    \n",
    "    # Extract data from results\n",
    "    for ride_name, metrics in all_results.items():\n",
    "        if not metrics:\n",
    "            continue\n",
    "            \n",
    "        rides.append(ride_name)\n",
    "        \n",
    "        # Validation metrics\n",
    "        val_metrics = metrics.get(\"validation\", {})\n",
    "        val_mae.append(val_metrics.get(\"mae\", float('nan')))\n",
    "        val_rmse.append(val_metrics.get(\"rmse\", float('nan')))\n",
    "        val_baseline_mae.append(val_metrics.get(\"baseline_mae\", float('nan')))\n",
    "        val_improvement.append(val_metrics.get(\"improvement_percentage\", float('nan')))\n",
    "        \n",
    "        # Test metrics\n",
    "        test_metrics = metrics.get(\"test\", {})\n",
    "        test_mae.append(test_metrics.get(\"mae\", float('nan')))\n",
    "        test_rmse.append(test_metrics.get(\"rmse\", float('nan')))\n",
    "        test_baseline_mae.append(test_metrics.get(\"baseline_mae\", float('nan')))\n",
    "        test_improvement.append(test_metrics.get(\"improvement_percentage\", float('nan')))\n",
    "        \n",
    "        # Data counts\n",
    "        counts = metrics.get(\"data_counts\", {})\n",
    "        data_counts.append(f\"Train: {counts.get('train', 0)}, Val: {counts.get('validation', 0)}, Test: {counts.get('test', 0)}\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Ride Name\": rides,\n",
    "        \"Validation MAE\": val_mae,\n",
    "        \"Validation RMSE\": val_rmse,\n",
    "        \"Validation Baseline MAE\": val_baseline_mae,\n",
    "        \"Validation Improvement (%)\": val_improvement,\n",
    "        \"Test MAE\": test_mae,\n",
    "        \"Test RMSE\": test_rmse,\n",
    "        \"Test Baseline MAE\": test_baseline_mae,\n",
    "        \"Test Improvement (%)\": test_improvement,\n",
    "        \"Data Counts\": data_counts\n",
    "    })\n",
    "    \n",
    "    # Sort by test improvement\n",
    "    summary_df = summary_df.sort_values(\"Test Improvement (%)\", ascending=False)\n",
    "    \n",
    "    # Save to CSV\n",
    "    summary_path = os.path.join(output_dir, \"model_summary.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    \n",
    "    # Print a brief summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Model Summary:\")\n",
    "    print(f\"Total rides processed: {len(summary_df)}\")\n",
    "    print(f\"Average validation MAE: {np.mean(val_mae):.2f} (baseline: {np.mean(val_baseline_mae):.2f})\")\n",
    "    print(f\"Average test MAE: {np.mean(test_mae):.2f} (baseline: {np.mean(test_baseline_mae):.2f})\")\n",
    "    print(f\"Average improvement: {np.mean(test_improvement):.2f}%\")\n",
    "    print(f\"Summary saved to: {summary_path}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create a visualization of the results\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Sort rides by test improvement for the plot\n",
    "    plot_df = summary_df.sort_values(\"Test Improvement (%)\")\n",
    "    \n",
    "    # Plot test improvement\n",
    "    plt.barh(plot_df[\"Ride Name\"], plot_df[\"Test Improvement (%)\"])\n",
    "    plt.xlabel(\"Improvement over Baseline (%)\")\n",
    "    plt.ylabel(\"Ride Name\")\n",
    "    plt.title(\"Model Improvement over Baseline (Prophet) by Ride\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, \"model_improvement_summary.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf9760",
   "metadata": {},
   "source": [
    "## Execute the Training Pipeline for All Rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f51fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set output directory for models and results\n",
    "output_dir = \"../models/wait_time_prediction\"\n",
    "\n",
    "# Process all rides\n",
    "results = process_all_rides(\n",
    "    all_rides=all_rides,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    output_dir=output_dir,\n",
    "    device=device,\n",
    "    resume=True  # Resume from checkpoint if available\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef04410",
   "metadata": {},
   "source": [
    "## Load and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca664a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load the summary results\n",
    "summary_path = os.path.join(output_dir, \"model_summary.csv\")\n",
    "if os.path.exists(summary_path):\n",
    "    summary_df = pd.read_csv(summary_path)\n",
    "    \n",
    "    # Display the top 10 performing rides\n",
    "    print(\"Top 10 rides by test improvement:\")\n",
    "    display(summary_df.head(10))\n",
    "    \n",
    "    # Create a visualization of MAE comparison\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Sort rides by test MAE for the plot (show top 15)\n",
    "    top_rides = summary_df.sort_values(\"Test MAE\").head(15)\n",
    "    \n",
    "    # Plot side by side bars for combined model and baseline\n",
    "    x = np.arange(len(top_rides))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, top_rides[\"Test MAE\"], width, label=\"Combined Model\")\n",
    "    plt.bar(x + width/2, top_rides[\"Test Baseline MAE\"], width, label=\"Prophet Baseline\")\n",
    "    \n",
    "    plt.xlabel(\"Ride Name\")\n",
    "    plt.ylabel(\"Mean Absolute Error (minutes)\")\n",
    "    plt.title(\"Model Performance Comparison - Top 15 Rides\")\n",
    "    plt.xticks(x, top_rides[\"Ride Name\"], rotation=45, ha=\"right\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, \"top_rides_mae_comparison.png\"))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Summary file not found. Run the processing pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575b632",
   "metadata": {},
   "source": [
    "## Example: Load a Specific Model and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb5aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_test_specific_model(ride_name, test_data, output_dir=\"../models/wait_time_prediction\"):\n",
    "    \"\"\"\n",
    "    Load a specific model and test it on the test dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ride_name : str\n",
    "        Name of the ride to load\n",
    "    test_data : pandas.DataFrame\n",
    "        Test data to use for predictions\n",
    "    output_dir : str\n",
    "        Directory where models are saved\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    prophet_model, nn_model, feature_scaler, _ = load_model(ride_name, output_dir)\n",
    "    \n",
    "    if prophet_model is None:\n",
    "        print(f\"No model found for {ride_name}\")\n",
    "        return\n",
    "    \n",
    "    # Filter test data for the ride\n",
    "    ride_test_data = filter_ride_data(test_data, ride_name)\n",
    "    \n",
    "    if len(ride_test_data) == 0:\n",
    "        print(f\"No test data found for {ride_name}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize feature engineer\n",
    "    feature_engineer = FeatureEngineer()\n",
    "    \n",
    "    # Get device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions, baseline_predictions, residuals = WaitTimePredictor.batch_predict(\n",
    "        ride_test_data, \n",
    "        prophet_model, \n",
    "        nn_model, \n",
    "        feature_scaler,\n",
    "        feature_engineer,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    actuals = ride_test_data['wait_time'].values\n",
    "    \n",
    "    # Evaluate and visualize\n",
    "    metrics, _ = evaluate_model(\n",
    "        ride_test_data, actuals, predictions, baseline_predictions, \n",
    "        title=f\"{ride_name} - Test Data\"\n",
    "    )\n",
    "    \n",
    "    # Show a time series plot for a sample period\n",
    "    ride_test_data = ride_test_data.sort_values('time_bucket')\n",
    "    \n",
    "    # Take the most recent month of data\n",
    "    recent_data = ride_test_data.tail(24*30)  # Assuming hourly data, ~30 days\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(recent_data['time_bucket'], recent_data['wait_time'], 'k-', label='Actual')\n",
    "    \n",
    "    # Find corresponding predictions\n",
    "    mask = np.isin(ride_test_data.index, recent_data.index)\n",
    "    recent_preds = predictions[mask]\n",
    "    recent_baseline = baseline_predictions[mask]\n",
    "    \n",
    "    plt.plot(recent_data['time_bucket'], recent_preds, 'b-', label='Combined Model')\n",
    "    plt.plot(recent_data['time_bucket'], recent_baseline, 'r--', label='Prophet Baseline')\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Wait Time (minutes)')\n",
    "    plt.title(f'Wait Time Predictions for {ride_name} - Most Recent Month')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# You can uncomment and run this after the models have been trained\n",
    "# Example usage:\n",
    "# ride_to_test = \"silver star\"  # Replace with a ride name from your dataset\n",
    "# load_and_test_specific_model(ride_to_test, test_data)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from prophet import Prophet\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Prophet can be verbose with warnings\n",
    "\n",
    "class RideDataset(Dataset):\n",
    "    def __init__(self, features, ride_indices, targets):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.ride_indices = torch.tensor(ride_indices, dtype=torch.long)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).reshape(-1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.ride_indices[idx], self.targets[idx]\n",
    "\n",
    "class ResidualPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, num_rides, hidden_dims=[64, 32]):\n",
    "        super(ResidualPredictor, self).__init__()\n",
    "        self.ride_embedding = nn.Embedding(num_rides, 8)  # 8-dimensional ride embedding\n",
    "        \n",
    "        layers = []\n",
    "        # Input layer (features + ride embedding)\n",
    "        layers.append(nn.Linear(input_dim + 8, hidden_dims[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(hidden_dims[0]))\n",
    "        layers.append(nn.Dropout(0.2))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, ride_idx):\n",
    "        ride_emb = self.ride_embedding(ride_idx)\n",
    "        combined = torch.cat([x, ride_emb], dim=1)\n",
    "        return self.model(combined)\n",
    "\n",
    "def extract_time_features(df):\n",
    "    \"\"\"Extract cyclical time features from timestamp\"\"\"\n",
    "    # Hour of day (0-23)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.hour / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.hour / 24)\n",
    "    \n",
    "    # Day of week (0-6)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.dayofweek / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.dayofweek / 7)\n",
    "    \n",
    "    # Month (1-12)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.month / 12)\n",
    "    \n",
    "    # Day of year (1-366)\n",
    "    df['day_of_year_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.dayofyear / 366)\n",
    "    df['day_of_year_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.dayofyear / 366)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def decompose_time_series(df, val_cutoff_date, test_cutoff_date):\n",
    "    \"\"\"\n",
    "    Decompose time series using Prophet for all rides\n",
    "    \"\"\"\n",
    "    # Get unique rides\n",
    "    rides = df['ride_name'].unique()\n",
    "    \n",
    "    # Create dataframe to store results\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    # Store Prophet models\n",
    "    prophet_models = {}\n",
    "    \n",
    "    for ride in tqdm(rides, desc=\"Fitting Prophet models\"):\n",
    "        # Filter data for this ride\n",
    "        ride_data = df[df['ride_name'] == ride].copy()\n",
    "        \n",
    "        # Prepare data for Prophet\n",
    "        prophet_df = ride_data.rename(columns={'timestamp': 'ds', 'wait_time': 'y'})\n",
    "        prophet_df = prophet_df.dropna(subset=['y'])\n",
    "        \n",
    "        # Split into train and validation\n",
    "        train_prophet = prophet_df[prophet_df['ds'] < val_cutoff_date]\n",
    "        \n",
    "        # Fit Prophet model on training data\n",
    "        model = Prophet(\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=True,\n",
    "            seasonality_mode='multiplicative'  # Often works better for wait times\n",
    "        )\n",
    "        \n",
    "        # Add holiday effects if they seem important\n",
    "        if ride_data['is_german_holiday'].sum() > 0:\n",
    "            model.add_country_holidays(country_name='DE')\n",
    "            \n",
    "        model.fit(train_prophet)\n",
    "        prophet_models[ride] = model\n",
    "        \n",
    "        # Predict for all timestamps\n",
    "        future = pd.DataFrame(df['timestamp'].unique(), columns=['ds'])\n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        # Merge forecasts back to original data\n",
    "        ride_data = pd.merge(\n",
    "            ride_data, \n",
    "            forecast[['ds', 'trend', 'yearly', 'weekly', 'daily', 'yhat']], \n",
    "            left_on='timestamp', \n",
    "            right_on='ds', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Calculate residuals where we have actual wait times\n",
    "        ride_data['residual'] = ride_data['wait_time'] - ride_data['yhat']\n",
    "        ride_data['baseline'] = ride_data['yhat']\n",
    "        \n",
    "        # Add to result dataframe\n",
    "        result_df = pd.concat([result_df, ride_data])\n",
    "    \n",
    "    return result_df, prophet_models\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Prepare features for neural network\n",
    "    \"\"\"\n",
    "    # Numerical features\n",
    "    numerical_features = ['temperature', 'rain', 'wind', \n",
    "                         'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
    "                         'month_sin', 'month_cos', 'day_of_year_sin', 'day_of_year_cos']\n",
    "    \n",
    "    # Boolean features (convert to int)\n",
    "    boolean_features = ['is_german_holiday', 'is_swiss_holiday', 'is_french_holiday', 'closed']\n",
    "    \n",
    "    # Combine features\n",
    "    feature_df = df[numerical_features + boolean_features].copy()\n",
    "    \n",
    "    # Convert boolean to int\n",
    "    for col in boolean_features:\n",
    "        feature_df[col] = feature_df[col].astype(int)\n",
    "    \n",
    "    # Handle missing values\n",
    "    feature_df = feature_df.fillna(0)\n",
    "    \n",
    "    # Get ride indices (for embedding)\n",
    "    rides = df['ride_name'].unique()\n",
    "    ride_to_idx = {ride: idx for idx, ride in enumerate(rides)}\n",
    "    ride_indices = df['ride_name'].map(ride_to_idx).values\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    feature_array = scaler.fit_transform(feature_df.values)\n",
    "    \n",
    "    return feature_array, ride_indices, ride_to_idx, scaler\n",
    "\n",
    "def train_neural_network(train_features, train_ride_indices, train_residuals,\n",
    "                         val_features, val_ride_indices, val_residuals,\n",
    "                         num_rides, input_dim):\n",
    "    \"\"\"\n",
    "    Train neural network to predict residuals\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = RideDataset(train_features, train_ride_indices, train_residuals)\n",
    "    val_dataset = RideDataset(val_features, val_ride_indices, val_residuals)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ResidualPredictor(input_dim, num_rides, hidden_dims=[128, 64, 32])\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_features, batch_rides, batch_residuals in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features, batch_rides)\n",
    "            loss = criterion(outputs, batch_residuals)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_rides, batch_residuals in val_loader:\n",
    "                outputs = model(batch_features, batch_rides)\n",
    "                loss = criterion(outputs, batch_residuals)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict_wait_times(df, prophet_models, nn_model, ride_to_idx, scaler):\n",
    "    \"\"\"\n",
    "    Make predictions by combining Prophet and neural network\n",
    "    \"\"\"\n",
    "    # Get features\n",
    "    features = df[['temperature', 'rain', 'wind', \n",
    "                  'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
    "                  'month_sin', 'month_cos', 'day_of_year_sin', 'day_of_year_cos',\n",
    "                  'is_german_holiday', 'is_swiss_holiday', 'is_french_holiday', 'closed']].copy()\n",
    "    \n",
    "    # Convert boolean to int\n",
    "    for col in ['is_german_holiday', 'is_swiss_holiday', 'is_french_holiday', 'closed']:\n",
    "        features[col] = features[col].astype(int)\n",
    "    \n",
    "    # Handle missing values\n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    # Scale features\n",
    "    scaled_features = scaler.transform(features.values)\n",
    "    \n",
    "    # Get ride indices\n",
    "    ride_indices = df['ride_name'].map(ride_to_idx).values\n",
    "    \n",
    "    # Convert to tensors\n",
    "    features_tensor = torch.tensor(scaled_features, dtype=torch.float32)\n",
    "    ride_indices_tensor = torch.tensor(ride_indices, dtype=torch.long)\n",
    "    \n",
    "    # Predict residuals\n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        residuals = nn_model(features_tensor, ride_indices_tensor).numpy().flatten()\n",
    "    \n",
    "    # Add residuals to baseline prediction\n",
    "    df['predicted_residual'] = residuals\n",
    "    df['predicted_wait_time'] = df['baseline'] + df['predicted_residual']\n",
    "    \n",
    "    # Ensure non-negative wait times\n",
    "    df['predicted_wait_time'] = df['predicted_wait_time'].clip(lower=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def evaluate_model(df):\n",
    "    \"\"\"\n",
    "    Evaluate model performance\n",
    "    \"\"\"\n",
    "    # Filter to rows with actual wait times\n",
    "    eval_df = df.dropna(subset=['wait_time'])\n",
    "    \n",
    "    # Calculate errors\n",
    "    eval_df['error'] = eval_df['predicted_wait_time'] - eval_df['wait_time']\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(eval_df['wait_time'], eval_df['predicted_wait_time']))\n",
    "    mae = mean_absolute_error(eval_df['wait_time'], eval_df['predicted_wait_time'])\n",
    "    \n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    \n",
    "    # Calculate metrics by ride\n",
    "    ride_metrics = eval_df.groupby('ride_name').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'RMSE': np.sqrt(mean_squared_error(x['wait_time'], x['predicted_wait_time'])),\n",
    "            'MAE': mean_absolute_error(x['wait_time'], x['predicted_wait_time']),\n",
    "            'Count': len(x)\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    return rmse, mae, ride_metrics\n",
    "\n",
    "def plot_predictions(df, ride_name, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted wait times for a specific ride and time period\n",
    "    \"\"\"\n",
    "    ride_df = df[(df['ride_name'] == ride_name) & \n",
    "                (df['timestamp'] >= start_date) & \n",
    "                (df['timestamp'] <= end_date)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(ride_df['timestamp'], ride_df['wait_time'], 'o-', label='Actual')\n",
    "    plt.plot(ride_df['timestamp'], ride_df['baseline'], '--', label='Prophet Baseline')\n",
    "    plt.plot(ride_df['timestamp'], ride_df['predicted_wait_time'], 'r-', label='Final Prediction')\n",
    "    plt.title(f'Wait Time Predictions for {ride_name}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Wait Time (minutes)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main execution function\n",
    "def run_wait_time_prediction(df, val_cutoff_date='2023-01-01', test_cutoff_date='2024-01-01'):\n",
    "    \"\"\"\n",
    "    Run the full wait time prediction pipeline\n",
    "    \"\"\"\n",
    "    # Convert dates if needed\n",
    "    if isinstance(val_cutoff_date, str):\n",
    "        val_cutoff_date = pd.to_datetime(val_cutoff_date)\n",
    "    if isinstance(test_cutoff_date, str):\n",
    "        test_cutoff_date = pd.to_datetime(test_cutoff_date)\n",
    "    \n",
    "    # Make sure timestamp is datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Extract time features\n",
    "    print(\"Extracting time features...\")\n",
    "    df = extract_time_features(df)\n",
    "    \n",
    "    # Split data\n",
    "    train_data = df[df['timestamp'] < val_cutoff_date]\n",
    "    val_data = df[(df['timestamp'] >= val_cutoff_date) & (df['timestamp'] < test_cutoff_date)]\n",
    "    test_data = df[df['timestamp'] >= test_cutoff_date]\n",
    "    \n",
    "    print(f\"Train data: {len(train_data)} rows ({train_data['timestamp'].min()} to {train_data['timestamp'].max()})\")\n",
    "    print(f\"Validation data: {len(val_data)} rows ({val_data['timestamp'].min()} to {val_data['timestamp'].max()})\")\n",
    "    print(f\"Test data: {len(test_data)} rows ({test_data['timestamp'].min()} to {test_data['timestamp'].max()})\")\n",
    "    \n",
    "    # Decompose time series\n",
    "    print(\"Decomposing time series with Prophet...\")\n",
    "    decomposed_df, prophet_models = decompose_time_series(df, val_cutoff_date, test_cutoff_date)\n",
    "    \n",
    "    # Prepare features\n",
    "    print(\"Preparing features...\")\n",
    "    feature_array, ride_indices, ride_to_idx, scaler = prepare_features(decomposed_df)\n",
    "    \n",
    "    # Split into train/val/test sets\n",
    "    train_mask = decomposed_df['timestamp'] < val_cutoff_date\n",
    "    val_mask = (decomposed_df['timestamp'] >= val_cutoff_date) & (decomposed_df['timestamp'] < test_cutoff_date)\n",
    "    test_mask = decomposed_df['timestamp'] >= test_cutoff_date\n",
    "    \n",
    "    # Filter to rows with actual wait times for training\n",
    "    train_has_wait = ~decomposed_df.loc[train_mask, 'wait_time'].isna()\n",
    "    val_has_wait = ~decomposed_df.loc[val_mask, 'wait_time'].isna()\n",
    "    \n",
    "    train_features = feature_array[train_mask][train_has_wait]\n",
    "    train_ride_indices = ride_indices[train_mask][train_has_wait]\n",
    "    train_residuals = decomposed_df.loc[train_mask, 'residual'].dropna().values\n",
    "    \n",
    "    val_features = feature_array[val_mask][val_has_wait]\n",
    "    val_ride_indices = ride_indices[val_mask][val_has_wait]\n",
    "    val_residuals = decomposed_df.loc[val_mask, 'residual'].dropna().values\n",
    "    \n",
    "    num_rides = len(decomposed_df['ride_name'].unique())\n",
    "    input_dim = train_features.shape[1]\n",
    "    \n",
    "    # Train neural network\n",
    "    print(\"Training neural network...\")\n",
    "    nn_model = train_neural_network(\n",
    "        train_features, train_ride_indices, train_residuals,\n",
    "        val_features, val_ride_indices, val_residuals,\n",
    "        num_rides, input_dim\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    train_predictions = predict_wait_times(\n",
    "        decomposed_df[train_mask], prophet_models, nn_model, ride_to_idx, scaler\n",
    "    )\n",
    "    val_predictions = predict_wait_times(\n",
    "        decomposed_df[val_mask], prophet_models, nn_model, ride_to_idx, scaler\n",
    "    )\n",
    "    test_predictions = predict_wait_times(\n",
    "        decomposed_df[test_mask], prophet_models, nn_model, ride_to_idx, scaler\n",
    "    )\n",
    "    \n",
    "    # Combine predictions\n",
    "    predictions_df = pd.concat([train_predictions, val_predictions, test_predictions])\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nTraining Set Performance:\")\n",
    "    train_rmse, train_mae, train_ride_metrics = evaluate_model(train_predictions)\n",
    "    \n",
    "    print(\"\\nValidation Set Performance:\")\n",
    "    val_rmse, val_mae, val_ride_metrics = evaluate_model(val_predictions)\n",
    "    \n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    test_rmse, test_mae, test_ride_metrics = evaluate_model(test_predictions)\n",
    "    \n",
    "    return predictions_df, prophet_models, nn_model, ride_to_idx, scaler\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    # df = pd.read_csv('amusement_park_data.csv')\n",
    "    \n",
    "    # Run prediction pipeline\n",
    "    # predictions_df, prophet_models, nn_model, ride_to_idx, scaler = run_wait_time_prediction(df)\n",
    "    \n",
    "    # Plot predictions for a specific ride\n",
    "    # plot_predictions(predictions_df, 'alpine express enzian', '2023-07-01', '2023-07-07')\n",
    "    \n",
    "    print(\"This script provides a complete implementation of the Prophet + PyTorch approach\")\n",
    "    print(\"for predicting amusement park wait times.\")\n",
    "    print(\"\\nTo use this code, you would need to:\")\n",
    "    print(\"1. Load your dataset\")\n",
    "    print(\"2. Call run_wait_time_prediction() with appropriate date cutoffs\")\n",
    "    print(\"3. Analyze the results using the evaluate_model() and plot_predictions() functions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspro2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2508ec3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b078514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from pytorch_tcn import TCN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4def4",
   "metadata": {},
   "source": [
    "# Reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a64199a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926905b7",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482636e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_length):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence of features\n",
    "        X_seq = self.X[idx:idx + self.seq_length]\n",
    "        # Get target value (next value after sequence)\n",
    "        y_value = self.y[idx + self.seq_length]\n",
    "        \n",
    "        return torch.FloatTensor(X_seq), torch.FloatTensor([y_value])\n",
    "    \n",
    "\n",
    "\n",
    "class TCNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout=0.2):\n",
    "        super(TCNModel, self).__init__()\n",
    "        \n",
    "\n",
    "        self.tcn = TCN(\n",
    "            num_inputs=input_size,         \n",
    "            num_channels=[num_channels] * 8,  # (number of filters  in each convolutional layer)\n",
    "            kernel_size=kernel_size,        # (temporal receptive field)\n",
    "            dropout=dropout,                \n",
    "            causal=True,                    # Causal convolutions (dont look into future)\n",
    "            use_skip_connections=True       # Use skip connections for better gradient flow\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.linear = nn.Linear(num_channels, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch_size, seq_len, input_size]\n",
    "        # TCN expects shape: [batch_size, input_size, seq_len]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Apply TCN - output will be [batch_size, num_channels, seq_len]\n",
    "        y = self.tcn(x)\n",
    "        \n",
    "        # Get the last time step output and apply the linear layer\n",
    "        y = y[:, :, -1]\n",
    "        \n",
    "        return self.linear(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2fe347",
   "metadata": {},
   "source": [
    "# Helperfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d50bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(df, target_ride=None):\n",
    "    \"\"\"\n",
    "    Preprocess the data for a single ride\n",
    "    \"\"\"\n",
    "    # Drop time_bucket column as not needed\n",
    "    if 'time_bucket' in df.columns:\n",
    "        df = df.drop(columns=['time_bucket'])\n",
    "    \n",
    "    print(f\"Building model for ride: {target_ride}\")\n",
    "    \n",
    "\n",
    "    ride_col = f'ride_name_{target_ride}'\n",
    "    if ride_col in df.columns:\n",
    "        df = df[df[ride_col] == 1].copy()\n",
    "    \n",
    "    ride_cols = [col for col in df.columns if col.startswith('ride_name_')]\n",
    "    df = df.drop(columns=ride_cols)\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create features for the model\n",
    "    \"\"\"\n",
    "    # The features are everything except wait_time (target)\n",
    "    feature_cols = [col for col in df.columns if col != 'wait_time']\n",
    "    \n",
    "    return df, feature_cols\n",
    "\n",
    "def build_linear_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Build and train a linear regression model\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43011db",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7f6880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_wandb(config=None):\n",
    "    run = wandb.init(config=config)\n",
    "    config = wandb.config\n",
    "    \n",
    "    data_path = config.data_path\n",
    "    splits_output_dir = config.splits_output_dir\n",
    "    target_ride = config.target_ride\n",
    "    seq_length = config.seq_length\n",
    "    batch_size = config.batch_size\n",
    "    num_channels = config.num_channels\n",
    "    kernel_size = config.kernel_size\n",
    "    dropout = config.dropout\n",
    "    learning_rate = config.learning_rate\n",
    "    epochs = config.epochs\n",
    "    \n",
    "    # Add scheduler parameters\n",
    "    scheduler_type = config.get('scheduler_type', 'CosineAnnealingLR')  # Default to CosineAnnealingLR\n",
    "    t_max = config.get('t_max', epochs)  # Default to total epochs\n",
    "    eta_min = config.get('eta_min', 1e-6)  # Minimum learning rate\n",
    "    \n",
    "    df = pd.read_parquet(data_path)\n",
    "    \n",
    "    df = preprocess_data(df, target_ride)\n",
    "    \n",
    "    train_indices = pd.read_parquet(os.path.join(splits_output_dir, \"train_indices.parquet\"))\n",
    "    val_indices = pd.read_parquet(os.path.join(splits_output_dir, \"validation_indices.parquet\"))\n",
    "    test_indices = pd.read_parquet(os.path.join(splits_output_dir, \"test_indices.parquet\"))\n",
    "    \n",
    "    ride_name_normalized = target_ride.replace(' ', '_')\n",
    "    train_indices = train_indices[train_indices['ride_name'] == ride_name_normalized]['original_index'].values\n",
    "    val_indices = val_indices[val_indices['ride_name'] == ride_name_normalized]['original_index'].values\n",
    "    test_indices = test_indices[test_indices['ride_name'] == ride_name_normalized]['original_index'].values\n",
    "    \n",
    "    if len(train_indices) == 0 or len(val_indices) == 0 or len(test_indices) == 0:\n",
    "        raise ValueError(f\"No indices found for ride {target_ride}. Check ride name or indices files.\")\n",
    "    \n",
    "    print(f\"Found {len(train_indices)} train, {len(val_indices)} validation, and {len(test_indices)} test samples\")\n",
    "    \n",
    "    df, feature_cols = create_features(df)\n",
    "    \n",
    "    train_df = df.iloc[train_indices].copy()\n",
    "    val_df = df.iloc[val_indices].copy()\n",
    "    test_df = df.iloc[test_indices].copy()\n",
    "        \n",
    "    # Prepare features and target\n",
    "    X_train = train_df[feature_cols].values\n",
    "    y_train = train_df['wait_time'].values\n",
    "    X_val = val_df[feature_cols].values\n",
    "    y_val = val_df['wait_time'].values\n",
    "    X_test = test_df[feature_cols].values\n",
    "    y_test = test_df['wait_time'].values\n",
    "    \n",
    "    linear_model = build_linear_model(X_train, y_train)\n",
    "    \n",
    "    # Get predictions from linear model\n",
    "    y_train_pred_linear = linear_model.predict(X_train)\n",
    "    y_val_pred_linear = linear_model.predict(X_val)\n",
    "    y_test_pred_linear = linear_model.predict(X_test)\n",
    "    \n",
    "    # Calculate residuals (actual - predicted)\n",
    "    train_residuals = y_train - y_train_pred_linear\n",
    "    val_residuals = y_val - y_val_pred_linear\n",
    "    test_residuals = y_test - y_test_pred_linear\n",
    "    \n",
    "    train_dataset = TimeSeriesDataset(X_train, train_residuals, seq_length)\n",
    "    val_dataset = TimeSeriesDataset(X_val, val_residuals, seq_length)\n",
    "    test_dataset = TimeSeriesDataset(X_test, test_residuals, seq_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_size = X_train.shape[1]  # Number of features\n",
    "    output_size = 1  # Predicting a single value (residual)\n",
    "    \n",
    "    tcn_model = TCNModel(\n",
    "        input_size=input_size,\n",
    "        output_size=output_size,\n",
    "        num_channels=num_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(tcn_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Add the learning rate scheduler\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=t_max,\n",
    "        eta_min=eta_min\n",
    "    )\n",
    "    \n",
    "    tcn_model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    best_model = None\n",
    "    patience = config.patience\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        tcn_model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = tcn_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Step the scheduler after each epoch\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        tcn_model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = tcn_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"epoch\": epoch,\n",
    "            \"learning_rate\": current_lr  # Log the current learning rate\n",
    "        })\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, LR: {current_lr:.6f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = tcn_model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    if best_model is not None:\n",
    "        tcn_model.load_state_dict(best_model)\n",
    "    \n",
    "    # Model evaluation on test set\n",
    "    tcn_model.to(torch.device(\"cpu\"))\n",
    "    tcn_model.eval()\n",
    "    \n",
    "    # Get TCN predictions on test data\n",
    "    all_tcn_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(test_loader):\n",
    "            outputs = tcn_model(inputs)\n",
    "            all_tcn_preds.extend(outputs.numpy().flatten())\n",
    "    \n",
    "    # Get the corresponding test data (accounting for sequence length)\n",
    "    y_test_seq_linear = y_test_pred_linear[seq_length:][:len(all_tcn_preds)]\n",
    "    y_test_seq_actual = y_test[seq_length:][:len(all_tcn_preds)]\n",
    "    \n",
    "    test_eval_df = test_df.iloc[seq_length:].reset_index(drop=True).iloc[:len(all_tcn_preds)].copy()\n",
    "    test_eval_df['linear_pred'] = y_test_seq_linear\n",
    "    test_eval_df['tcn_pred'] = all_tcn_preds\n",
    "    test_eval_df['combined_pred'] = y_test_seq_linear + np.array(all_tcn_preds)\n",
    "    \n",
    "    # Filter out rows where closed = 1\n",
    "    if 'closed' in test_eval_df.columns:\n",
    "        print(f\"\\nExcluding {test_eval_df['closed'].sum()} data points where ride is closed from evaluation\")\n",
    "        open_ride_df = test_eval_df[test_eval_df['closed'] == 0]\n",
    "    else:\n",
    "        print(\"Warning: 'closed' column not found in the data. Evaluating on all test data.\")\n",
    "        open_ride_df = test_eval_df\n",
    "    \n",
    "    y_test_open_actual = open_ride_df['wait_time'].values\n",
    "    y_test_open_linear = open_ride_df['linear_pred'].values\n",
    "    y_test_open_combined = open_ride_df['combined_pred'].values\n",
    "\n",
    "    linear_mae = mean_absolute_error(y_test_open_actual, y_test_open_linear)\n",
    "    linear_mse = mean_squared_error(y_test_open_actual, y_test_open_linear)\n",
    "    linear_rmse = np.sqrt(linear_mse)\n",
    "    linear_r2 = r2_score(y_test_open_actual, y_test_open_linear)\n",
    "    \n",
    "    combined_mae = mean_absolute_error(y_test_open_actual, y_test_open_combined)\n",
    "    combined_mse = mean_squared_error(y_test_open_actual, y_test_open_combined)\n",
    "    combined_rmse = np.sqrt(combined_mse)\n",
    "    combined_r2 = r2_score(y_test_open_actual, y_test_open_combined)\n",
    "    \n",
    "    wandb.log({\n",
    "        \"linear_mae\": linear_mae,\n",
    "        \"combined_mae\": combined_mae,\n",
    "        \"combined_rmse\": combined_rmse,\n",
    "        \"combined_r2\": combined_r2,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    })\n",
    "    \n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    linear_model_filename = f\"{target_ride.replace(' ', '_')}_linear_model.pkl\"\n",
    "    tcn_model_filename = f\"{target_ride.replace(' ', '_')}_tcn_model.pt\"\n",
    "    \n",
    "    with open(f\"models/{linear_model_filename}\", \"wb\") as f:\n",
    "        pickle.dump(linear_model, f)\n",
    "    \n",
    "    torch.save(tcn_model.state_dict(), f\"models/{tcn_model_filename}\")\n",
    "\n",
    "    linear_artifact = wandb.Artifact(f\"linear_model_{wandb.run.id}\", type=\"model\")\n",
    "    linear_artifact.add_file(f\"models/{linear_model_filename}\")\n",
    "    wandb.log_artifact(linear_artifact)\n",
    "    \n",
    "    tcn_artifact = wandb.Artifact(f\"tcn_model_{wandb.run.id}\", type=\"model\")\n",
    "    tcn_artifact.add_file(f\"models/{tcn_model_filename}\")\n",
    "    wandb.log_artifact(tcn_artifact)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "def setup_wandb_sweep(project_name=\"queue-prediction\"):\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',  # Bayesian optimization\n",
    "        'metric': {\n",
    "            'name': 'combined_mae',  # Metric to optimize\n",
    "            'goal': 'minimize'  # We want to minimize RMSE\n",
    "        },\n",
    "        'parameters': {\n",
    "            'seq_length': {\n",
    "                'values': [24, 48, 96, 192, 384, 768] \n",
    "            },\n",
    "            'batch_size': {\n",
    "                'values': [128, 256, 512, 1048] \n",
    "            },\n",
    "            'num_channels': {\n",
    "                'values': [32, 64, 128, 256] \n",
    "            },\n",
    "            'kernel_size': {\n",
    "                'values': [2, 3, 5, 8] \n",
    "            },\n",
    "            'dropout': {\n",
    "                'values': [0.1, 0.2, 0.3]  \n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'values': [1e-3, 10**-3.5, 1e-4, 10**-4.5, 1e-5]  \n",
    "            },\n",
    "            'epochs': {\n",
    "                'value': 100 \n",
    "            },\n",
    "            'patience': {\n",
    "                'value': 10  \n",
    "            },\n",
    "            'data_path': {\n",
    "                'value': '../data/processed/ep/rides/poseidon.parquet' \n",
    "            },\n",
    "            'splits_output_dir': {\n",
    "                'value': '../data/processed/splits' \n",
    "            },\n",
    "            'target_ride': {\n",
    "                'value': 'poseidon' \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Initialize the sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "    return sweep_id\n",
    "\n",
    "def setup_wandb_sweep(project_name=\"queue-prediction\"):\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',  # Bayesian optimization\n",
    "        'metric': {\n",
    "            'name': 'combined_mae',  # Metric to optimize\n",
    "            'goal': 'minimize'  # We want to minimize RMSE\n",
    "        },\n",
    "        'parameters': {\n",
    "            'seq_length': {\n",
    "                'values': [24, 48, 96, 192, 384, 768] \n",
    "            },\n",
    "            'batch_size': {\n",
    "                'values': [128, 256, 512, 1048] \n",
    "            },\n",
    "            'num_channels': {\n",
    "                'values': [32, 64, 128, 256] \n",
    "            },\n",
    "            'kernel_size': {\n",
    "                'values': [2, 3, 5, 8] \n",
    "            },\n",
    "            'dropout': {\n",
    "                'values': [0.1, 0.2, 0.3]  \n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'values': [1e-3, 10**-3.5, 1e-4, 10**-4.5, 1e-5]  \n",
    "            },\n",
    "            'epochs': {\n",
    "                'value': 100 \n",
    "            },\n",
    "            'patience': {\n",
    "                'value': 10  \n",
    "            },\n",
    "            'scheduler_type': {\n",
    "                'value': 'CosineAnnealingLR'  \n",
    "            },\n",
    "            't_max': {\n",
    "                'values': [10, 25, 50, 100]  \n",
    "            },\n",
    "            'eta_min': {\n",
    "                'values': [0, 1e-7, 1e-6]  # learning rate will follow a cosine curve from the initial learning rate to eta_min over T_max epochs\n",
    "            },\n",
    "            'data_path': {\n",
    "                'value': '../data/processed/ep/rides/poseidon.parquet' \n",
    "            },\n",
    "            'splits_output_dir': {\n",
    "                'value': '../data/processed/splits' \n",
    "            },\n",
    "            'target_ride': {\n",
    "                'value': 'poseidon' \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "    return sweep_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaikotrede\u001b[0m (\u001b[33mmaikotrede-hochschule-luzern\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "project_name = \"queue-prediction-sweeps\"\n",
    "entity = \"maikotrede-hochschule-luzern\" \n",
    "wandb.login()\n",
    "sweep_id = setup_wandb_sweep(project_name)\n",
    "wandb.agent(sweep_id, train_with_wandb, count=1)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2aaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891ac0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspro2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
